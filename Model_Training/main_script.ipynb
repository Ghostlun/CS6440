{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mental Health Counsel Chatbot\n",
    "\n",
    "Kaggle Notebook: Mental Health Counsel Chatbot\n",
    "Description: Provides mental health counseling data, which we used to supplement information from the primary dataset and align topics for consistent categorization.\n",
    "Mental Health Synthetic Dataset\n",
    "\n",
    "Kaggle Dataset: Mental Health Synthetic Dataset\n",
    "Description: This primary dataset contains synthetic data on mental health symptoms, demographics, and treatment, forming the basis for model training and recommendation generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "m_BfAu_XPWub",
    "outputId": "693f18da-464b-47dc-8e40-b70491dae910"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['User ID', 'Age', 'Gender', 'Symptoms', 'Duration (weeks)',\n",
       "       'Previous Diagnosis', 'Therapy History', 'Medication',\n",
       "       'Diagnosis / Condition', 'Suggested Therapy', 'Self-care Advice',\n",
       "       'Urgency Level', 'Mood', 'Stress Level'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "mental_df = pd.read_csv(\"raw_data/mental_health.csv\")\n",
    "mental_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_data_path = \"cleaned_data\"\n",
    "raw_data_model_path = \"raw_data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['User ID', 'Age', 'Gender', 'Symptoms', 'Duration', 'Prev_Diagnosis',\n",
       "       'Therapy_History', 'Medication', 'Diagnosis', 'Suggested_Therapy',\n",
       "       'Self_Care_Advice', 'Urgency_Level', 'Mood', 'Stress_Level'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Renmae columns: easy to follow up.\n",
    "mental_df = mental_df.rename(columns= {\n",
    "    'Diagnosis / Condition' : 'Diagnosis',\n",
    "    'Self-care Advice': 'Self_Care_Advice',\n",
    "    'Therapy History' : 'Therapy_History',\n",
    "    'Stress Level' : 'Stress_Level',\n",
    "    'Urgency Level' : 'Urgency_Level',\n",
    "    'Suggested Therapy': 'Suggested_Therapy',\n",
    "    'Duration (weeks)': 'Duration',\n",
    "    'Previous Diagnosis': 'Prev_Diagnosis'\n",
    "})\n",
    "\n",
    "# Check renamed columns\n",
    "mental_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "1rl9DKaPRJ4-"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Symtoms unique values \n",
      " ['feeling anxious' 'excessive worry' 'trouble sleeping'\n",
      " 'loss of interest in activities' 'panic attacks' 'lack of concentration'\n",
      " 'feeling irritable' 'feeling sad' 'feeling overwhelmed']\n",
      "therapy_unique_values \n",
      " ['Support Groups' 'Cognitive Behavioral Therapy' 'Psychotherapy'\n",
      " 'Mindfulness-Based Therapy' 'No Therapy Needed']\n",
      "diagnosis_unique_values \n",
      " ['Panic Disorder' 'Depression' 'Anxiety' 'Burnout' 'Stress']\n"
     ]
    }
   ],
   "source": [
    "symptoms_unique_values = mental_df['Symptoms'].unique()\n",
    "therapy_unique_values = mental_df['Suggested_Therapy'].unique()\n",
    "diagnosis_unique_values = mental_df['Diagnosis'].unique()\n",
    "\n",
    "print(\"Symtoms unique values \\n\", symptoms_unique_values)\n",
    "print(\"therapy_unique_values \\n\", therapy_unique_values)\n",
    "print(\"diagnosis_unique_values \\n\", diagnosis_unique_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A label encoder is a technique that converts non-numerical data into numerical values, \n",
    "which is useful for machine learning and data analysis. <br>\n",
    "It's often used when working with categorical data, such as ordinal data, \n",
    "where there's a hierarchy among the values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import classification_report\n",
    "# Target supposed to be Target Supposed.\n",
    "#  th\n",
    "def build_self_test_self_care_advice():\n",
    "    le_diagnosis = LabelEncoder()\n",
    "    le_symtoms = LabelEncoder()\n",
    "    le_self_care = LabelEncoder()\n",
    "    le_therapy = LabelEncoder()\n",
    "    # mental_df['Duration'] is already Int style, we don't have to encode it\n",
    "    \n",
    "    mental_df['Diagnosis_encoded'] = le_diagnosis.fit_transform(mental_df['Diagnosis'])\n",
    "    mental_df['Symptoms_encoded'] = le_symtoms.fit_transform(mental_df['Symptoms'])\n",
    "    mental_df['Self_Care_Advice_encoded'] = le_self_care.fit_transform(mental_df['Self_Care_Advice'])\n",
    "    mental_df['Suggested_Therapy_encoded'] = le_therapy.fit_transform(mental_df['Suggested_Therapy'])\n",
    "\n",
    "    # Training Data\n",
    "    X = mental_df[['Diagnosis_encoded', 'Symptoms_encoded']]\n",
    "    y_self_care = mental_df['Self_Care_Advice_encoded']\n",
    "    y_therapy = mental_df['Suggested_Therapy_encoded']\n",
    "\n",
    "    X_train, X_test, y_self_care_train, y_self_care_test, y_therapy_train, y_therapy_test = train_test_split(X, y_self_care, y_therapy, test_size=0.2, random_state=42)\n",
    "    # Train models for Self Care Advice and Suggested Therapy\n",
    "    model_self_care = RandomForestClassifier()\n",
    "    model_therapy = RandomForestClassifier()\n",
    "\n",
    "    model_self_care.fit(X_train, y_self_care_train)\n",
    "    model_therapy.fit(X_train, y_therapy_train)\n",
    "\n",
    "    # Make predictions\n",
    "    self_care_pred = model_self_care.predict(X_test)\n",
    "    therapy_pred = model_therapy.predict(X_test)\n",
    "\n",
    "    # Display classification reports\n",
    "    print(\"Self Care Advice Classification Report:\")\n",
    "    self_care_report = classification_report(y_self_care_test, self_care_pred, target_names=le_self_care.classes_)\n",
    "    print(self_care_report)\n",
    "    print(\"\\nSuggested Therapy Classification Report:\")\n",
    "    self_therapy_report = classification_report(y_therapy_test, therapy_pred, target_names=le_therapy.classes_)\n",
    "    print(self_therapy_report)\n",
    "    return model_self_care , model_therapy, le_diagnosis, le_symtoms, le_self_care, le_therapy, self_care_report, self_therapy_report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initial Reports\n",
    "\n",
    "### Self Care Advice Classification Report\n",
    "\n",
    "| Self Care Advice       | Precision | Recall | F1-Score | Support |\n",
    "|------------------------|-----------|--------|----------|---------|\n",
    "| Breathing Exercises    | 0.20      | 0.17   | 0.18     | 166     |\n",
    "| Exercise               | 0.17      | 0.24   | 0.20     | 181     |\n",
    "| Journaling             | 0.32      | 0.14   | 0.20     | 191     |\n",
    "| Meditation             | 0.11      | 0.03   | 0.04     | 116     |\n",
    "| Take Breaks            | 0.17      | 0.33   | 0.23     | 164     |\n",
    "| Talk to a Friend       | 0.22      | 0.21   | 0.21     | 182     |\n",
    "| **Accuracy**           |           |        | 0.19     | 1000    |\n",
    "| **Macro Avg**          | 0.20      | 0.19   | 0.18     | 1000    |\n",
    "| **Weighted Avg**       | 0.21      | 0.19   | 0.18     | 1000    |\n",
    "\n",
    "### Suggested Therapy Classification Report\n",
    "\n",
    "| Suggested Therapy               | Precision | Recall | F1-Score | Support |\n",
    "|---------------------------------|-----------|--------|----------|---------|\n",
    "| Cognitive Behavioral Therapy    | 0.16      | 0.13   | 0.14     | 200     |\n",
    "| Mindfulness-Based Therapy       | 0.14      | 0.03   | 0.05     | 189     |\n",
    "| No Therapy Needed               | 0.17      | 0.09   | 0.11     | 187     |\n",
    "| Psychotherapy                   | 0.17      | 0.31   | 0.22     | 202     |\n",
    "| Support Groups                  | 0.24      | 0.34   | 0.28     | 222     |\n",
    "| **Accuracy**                    |           |        | 0.19     | 1000    |\n",
    "| **Macro Avg**                   | 0.17      | 0.18   | 0.16     | 1000    |\n",
    "| **Weighted Avg**                | 0.18      | 0.19   | 0.17     | 1000    |\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_self_care , model_therapy, le_diagnosis, le_symtoms, le_self_care, le_therapy, self_care_report, self_therapy_report = build_model()\n",
    "# Save model for purpose\n",
    "import joblib\n",
    "def save_model(model_self_care , model_therapy, le_diagnosis, le_symtoms, le_self_care, le_therapy ):\n",
    "    joblib.dump(model_self_care, \"model_self_care.pkl\")\n",
    "    joblib.dump(model_therapy, \"model_therapy.pkl\")\n",
    "    joblib.dump(le_diagnosis, \"le_diagnosis.pkl\")\n",
    "    joblib.dump(le_symtoms, \"le_symtoms.pkl\")\n",
    "    joblib.dump(le_self_care, \"le_self_care.pkl\")\n",
    "    joblib.dump(le_therapy, \"le_therapy.pkl\")\n",
    "\n",
    "# save_model(model_self_care , model_therapy, le_diagnosis, le_symtoms, le_self_care, le_therapy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Current accuracy 0.18 precison also,\n",
    "To imporve this models. <br>\n",
    "There are serveral ways to imporve ways.<br>\n",
    "Add more parameters (which contains demographic infomrationm which user can simply input them), also re mapping based Diagnosis.<br>\n",
    "I created three value includes 2 informaiton.<br>\n",
    "It wil help to organize better modeling. <br>\n",
    "Current features has <br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gender ['Other' 'Female' 'Non-binary' 'Male']\n",
      "Duration:  [29 37 47 35 22  8 31 20 21  9 38 43 30  2  3 34 36 19 33 23 46 49 48 41\n",
      " 11 16 10 45 13  4 39 12 51 24 17 32  1 14 44 15 26  5 40 27 42 18  6 50\n",
      " 28 25  7]\n",
      "Urgency Label ['Moderate' 'High' 'Low' 'Critical']\n",
      "Stress level [ 1  4  5  2  6  9 10  8  7  3]\n",
      "Prev_diagnosis ['OCD' 'None' 'PTSD' 'Bipolar Disorder' 'Anxiety' 'Depression']\n"
     ]
    }
   ],
   "source": [
    "print(\"Gender\",mental_df['Gender'].unique())\n",
    "print(\"Duration: \", mental_df['Duration'].unique())\n",
    "print(\"Urgency Label\", mental_df['Urgency_Level'].unique())\n",
    "print(\"Stress level\", mental_df['Stress_Level'].unique())\n",
    "print(\"Prev_diagnosis\", mental_df['Prev_Diagnosis'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Diagnosis Group Classification Report:\n",
      "                          precision    recall  f1-score   support\n",
      "\n",
      "       Anxiety Disorders       0.45      0.61      0.51       422\n",
      "          Mood Disorders       0.45      0.39      0.42       440\n",
      "Stress-Related Disorders       0.23      0.08      0.12       138\n",
      "\n",
      "                accuracy                           0.44      1000\n",
      "               macro avg       0.38      0.36      0.35      1000\n",
      "            weighted avg       0.42      0.44      0.42      1000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Diagnosis Group\n",
    "\n",
    "def group_diagnosis(row):\n",
    "    if row['Diagnosis'] in ['Panic Disorder', 'Anxiety']:\n",
    "        return 'Anxiety Disorders'\n",
    "    elif row['Diagnosis'] in ['Depression', 'Burnout']:\n",
    "        return 'Mood Disorders'\n",
    "    elif row['Diagnosis'] == 'Stress':\n",
    "        return 'Stress-Related Disorders'\n",
    "def group_prev_diagnosis(row):\n",
    "    if row['Prev_Diagnosis'] in ['Panic Disorder', 'Anxiety', 'OCD']:\n",
    "        return 'Anxiety Disorders'\n",
    "    elif row['Prev_Diagnosis'] in ['Depression', 'Bipolar Disorder']:\n",
    "        return 'Mood Disorders'\n",
    "    elif row['Prev_Diagnosis'] in ['Stress', 'PTSD']:\n",
    "        return 'Stress-Related Disorders'\n",
    "    else:\n",
    "        return ''\n",
    "\n",
    "def re_map_gender(row):\n",
    "    if row['Gender'] == 'Male':\n",
    "        return 1\n",
    "    elif row['Gender'] == 'Female':\n",
    "        return 2\n",
    "    else:\n",
    "        return 3\n",
    "def re_map_urgency_level(row):\n",
    "    if row[\"Urgency_Level\"] == \"Low\":\n",
    "        return 1\n",
    "    elif row[\"Urgency_Level\"] == \"Moderate\":\n",
    "        return 2\n",
    "    elif row[\"Urgency_Level\"] == \"High\":\n",
    "        return 3\n",
    "    else:\n",
    "        return 4\n",
    "    \n",
    "# Inital diagnosis model\n",
    "def improved_test_reports_diagnosis(mental_df):\n",
    "    # Diagnosis Group\n",
    "    mental_df[\"Diagnosis_Group\"] = mental_df.apply(group_diagnosis, axis=1)\n",
    "    mental_df['Prev_Diagnosis_Group'] = mental_df.apply(group_prev_diagnosis, axis=1)\n",
    "    mental_df[\"Re_Gender\"] = mental_df.apply(re_map_gender, axis=1)\n",
    "    mental_df[\"Urgency_Level\"] = mental_df.apply(re_map_urgency_level, axis=1)\n",
    "    \n",
    "    le_diagnosis_group = LabelEncoder()\n",
    "    le_prev_Diagnosis_group = LabelEncoder()\n",
    "    le_symtoms = LabelEncoder()\n",
    "\n",
    "    mental_df['Diagnosis_Group_encoded'] = le_diagnosis_group.fit_transform(mental_df['Diagnosis_Group'])\n",
    "    mental_df['Prev_Diagnosis_Group_encoded'] =  le_prev_Diagnosis_group.fit_transform(mental_df['Prev_Diagnosis'])\n",
    "    mental_df['Symptoms_encoded'] = le_symtoms.fit_transform(mental_df['Symptoms'])\n",
    "\n",
    "    # Training Data with duration\n",
    "    X = mental_df[['Age', 'Symptoms_encoded', \"Re_Gender\", \"Prev_Diagnosis_Group_encoded\", \"Duration\", \"Stress_Level\", \"Urgency_Level\"]] \n",
    "    y_diagnosis = mental_df['Diagnosis_Group_encoded']\n",
    "\n",
    "    X_train, X_test, y_diagnosis_train, y_diagnosis_test = train_test_split(X, y_diagnosis, test_size=0.2, random_state=42)\n",
    "    \n",
    "    # Train model for Diagnosis\n",
    "    model_diagnosis = RandomForestClassifier()\n",
    "    model_diagnosis.fit(X_train, y_diagnosis_train)\n",
    "\n",
    "    # Make predictions\n",
    "    diagnos_pred = model_diagnosis.predict(X_test)\n",
    "\n",
    "    # Display classification reports\n",
    "    print(\"Diagnosis Group Classification Report:\")\n",
    "    diagnosis_report = classification_report(y_diagnosis_test, diagnos_pred, target_names=le_diagnosis_group.classes_)\n",
    "    print(diagnosis_report)\n",
    "    \n",
    "    return model_diagnosis, le_diagnosis_group, le_prev_Diagnosis_group, le_symtoms, diagnosis_report\n",
    "\n",
    "model_diagnosis, le_diagnosis_group, le_prev_Diagnosis_group, le_symtoms, diagnosis_report = improved_test_reports_diagnosis(mental_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to trained_models/01_diagnosis_model/\n"
     ]
    }
   ],
   "source": [
    "import joblib\n",
    "import os\n",
    "\n",
    "# Download Model\n",
    "def createModel(model_diagnosis, le_diagnosis_group, le_prev_Diagnosis_group, le_symtoms):\n",
    "      # Save the model to the diagnosisModel folder\n",
    "    os.makedirs('trained_models/01_diagnosis_model', exist_ok=True)\n",
    "    model_path = 'trained_models/01_diagnosis_model/'\n",
    "    joblib.dump(model_diagnosis, f'{model_path}diagnosis_model.pkl')\n",
    "    joblib.dump(le_diagnosis_group, f'{model_path}le_diagnosis_group.pkl')\n",
    "    joblib.dump(le_prev_Diagnosis_group, f'{model_path}le_prev_Diagnosis_group.pkl')\n",
    "    joblib.dump(le_symtoms, f'{model_path}le_symptoms.pkl')\n",
    "    print(f\"Model saved to {model_path}\")\n",
    "    \n",
    "createModel(model_diagnosis, le_diagnosis_group, le_prev_Diagnosis_group, le_symtoms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Diagnosis Group Classification Report (SVM):\n",
      "                          precision    recall  f1-score   support\n",
      "\n",
      "       Anxiety Disorders       0.42      1.00      0.59       422\n",
      "          Mood Disorders       0.00      0.00      0.00       440\n",
      "Stress-Related Disorders       0.00      0.00      0.00       138\n",
      "\n",
      "                accuracy                           0.42      1000\n",
      "               macro avg       0.14      0.33      0.20      1000\n",
      "            weighted avg       0.18      0.42      0.25      1000\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ykim\\AppData\\Local\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\ykim\\AppData\\Local\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\ykim\\AppData\\Local\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(SVC(kernel='poly'),\n",
       " LabelEncoder(),\n",
       " '                          precision    recall  f1-score   support\\n\\n       Anxiety Disorders       0.42      1.00      0.59       422\\n          Mood Disorders       0.00      0.00      0.00       440\\nStress-Related Disorders       0.00      0.00      0.00       138\\n\\n                accuracy                           0.42      1000\\n               macro avg       0.14      0.33      0.20      1000\\n            weighted avg       0.18      0.42      0.25      1000\\n')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "def improved_test_reports_diagnosis_svm(mental_df):\n",
    "    # Same preprocessing steps\n",
    "    mental_df[\"Diagnosis_Group\"] = mental_df.apply(group_diagnosis, axis=1)\n",
    "    mental_df['Prev_Diagnosis_Group'] = mental_df.apply(group_prev_diagnosis, axis=1)\n",
    "    mental_df[\"Re_Gender\"] = mental_df.apply(re_map_gender, axis=1)\n",
    "    mental_df[\"Urgency_Level\"] = mental_df.apply(re_map_urgency_level, axis=1)\n",
    "\n",
    "    le_diagnosis_group = LabelEncoder()\n",
    "    le_prev_Diagnosis_group = LabelEncoder()\n",
    "    le_symtoms = LabelEncoder()\n",
    "\n",
    "    mental_df['Diagnosis_Group_encoded'] = le_diagnosis_group.fit_transform(mental_df['Diagnosis_Group'])\n",
    "    mental_df['Prev_Diagnosis_Group_encoded'] = le_prev_Diagnosis_group.fit_transform(mental_df['Prev_Diagnosis'])\n",
    "    mental_df['Symptoms_encoded'] = le_symtoms.fit_transform(mental_df['Symptoms'])\n",
    "\n",
    "    X = mental_df[['Age', 'Symptoms_encoded', \"Re_Gender\", \"Prev_Diagnosis_Group_encoded\", \"Duration\", \"Stress_Level\", \"Urgency_Level\"]] \n",
    "    y_diagnosis = mental_df['Diagnosis_Group_encoded']\n",
    "\n",
    "    X_train, X_test, y_diagnosis_train, y_diagnosis_test = train_test_split(X, y_diagnosis, test_size=0.2, random_state=42)\n",
    "\n",
    "    # Train model using Support Vector Machine\n",
    "    model_diagnosis = SVC(kernel='poly')  # You can also try 'rbf' or 'poly'\n",
    "    model_diagnosis.fit(X_train, y_diagnosis_train)\n",
    "\n",
    "    # Make predictions\n",
    "    diagnos_pred = model_diagnosis.predict(X_test)\n",
    "\n",
    "    # Display classification report\n",
    "    print(\"Diagnosis Group Classification Report (SVM):\")\n",
    "    diagnosis_report = classification_report(y_diagnosis_test, diagnos_pred, target_names=le_diagnosis_group.classes_)\n",
    "    print(diagnosis_report)\n",
    "\n",
    "    return model_diagnosis, le_diagnosis_group, diagnosis_report\n",
    "\n",
    "improved_test_reports_diagnosis_svm(mental_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\ykim\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Panic': ['panic', 'affright', 'scare', 'terror'], 'Disorder': ['disarray', 'disorderliness', 'upset', 'disorder', 'unhinge', 'perturb', 'cark', 'disquiet', 'trouble', 'distract'], 'Depression': ['Great_Depression', 'economic_crisis', 'depressive_disorder', 'low', 'imprint', 'impression', 'depression', 'slump', 'clinical_depression', 'Depression', 'natural_depression'], 'Burnout': [], 'Stress': ['try', 'accentuate', 'emphasis', 'emphasise', 'focus', 'emphasize', 'stress', 'punctuate', 'accent', 'tension', 'strain', 'tenseness']}\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import wordnet\n",
    "from itertools import chain\n",
    "\n",
    "# Based on questions and topics, Create chains then finds vlaues.\n",
    "def find_related_words():\n",
    "    nltk.download('wordnet')\n",
    "\n",
    "    words = {\n",
    "        'Panic': 'panic',\n",
    "        'Disorder': 'disorder',\n",
    "        'Depression': 'depression',\n",
    "        'Burnout': 'burnout',\n",
    "        'Stress': 'stress'\n",
    "    }\n",
    "\n",
    "    # Initialize a dictionary to store the results\n",
    "    related_words = {}\n",
    "\n",
    "    for key, word in words.items():\n",
    "        word_synsets = wordnet.synsets(word)\n",
    "        similar_words = set(chain(*[synset.lemma_names() for synset in word_synsets]))\n",
    "        related_words[key] = list(similar_words)\n",
    "\n",
    "    return related_words\n",
    "\n",
    "# Get the related words and print the dictionary\n",
    "similar_words_dict = find_related_words()\n",
    "print(similar_words_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Result for related words\n",
    "# {'Panic': ['panic', 'scare', 'terror', 'affright'], \n",
    "# 'Disorder': ['distract', 'upset', 'trouble', 'disorderliness', 'cark', 'disarray', 'disorder', 'perturb', 'disquiet', 'unhinge'], \n",
    "# 'Depression': ['impression', 'clinical_depression', 'slump', 'Great_Depression', 'Depression', 'low', 'depressive_disorder', 'natural_depression', 'imprint', 'economic_crisis', 'depression'], \n",
    "# 'Burnout': [], \n",
    "# 'Stress': ['strain', 'emphasis', 'emphasize', 'tension', 'punctuate', 'focus', 'accentuate', 'try', 'tenseness', 'emphasise', 'stress', 'accent']}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Counsel data training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Orignal length1482\n",
      "\n",
      "Word frequencies in the 'topics' column:\n",
      "Family :  119\n",
      "Conflict :  91\n",
      "Substance :  14\n",
      "AbuseAddiction :  9\n",
      "Behavioral :  49\n",
      "ChangeSocial :  3\n",
      "Relationships :  214\n",
      "Relationship :  45\n",
      "Dissolution :  68\n",
      "Anger :  32\n",
      "Management :  25\n",
      "Sleep :  10\n",
      "Improvement :  17\n",
      "Professional :  34\n",
      "EthicsLegal :  6\n",
      "& :  45\n",
      "Regulatory :  20\n",
      "Social :  41\n",
      "RelationshipsMarriage :  11\n",
      "MarriageIntimacy :  26\n",
      "Domestic :  9\n",
      "ViolenceAnger :  2\n",
      "ManagementFamily :  3\n",
      "Human :  50\n",
      "Sexuality :  33\n",
      "ManagementSleep :  2\n",
      "Military :  3\n",
      "Issues :  10\n",
      "RelationshipsDomestic :  3\n",
      "Violence :  10\n",
      "ViolenceRelationship :  1\n",
      "Marriage :  25\n",
      "Grief :  20\n",
      "and :  23\n",
      "Loss :  9\n",
      "ConflictChildren :  1\n",
      "Adolescents :  9\n",
      "MarriageRelationship :  4\n",
      "TraumaHuman :  1\n",
      "RelationshipsIntimacy :  42\n",
      "ManagementParenting :  1\n",
      "Intimacy :  17\n",
      "Workplace :  9\n",
      "SexualityMarriage :  3\n",
      "LGBTQ :  29\n",
      "SpiritualityFamily :  2\n",
      "Ethics :  23\n",
      "ViolenceRelationships :  1\n",
      "ConflictRelationships :  5\n",
      "Self-esteem :  29\n",
      "Self-esteemRelationships :  12\n",
      "Parenting :  35\n",
      "ConflictMarriage :  6\n",
      "ConflictSelf-esteem :  3\n",
      "ParentingRelationships :  1\n",
      "nan :  10\n",
      "Counseling :  53\n",
      "RelationshipsSelf-esteem :  9\n",
      "Eating :  7\n",
      "RelationshipsProfessional :  2\n",
      "ParentingSubstance :  1\n",
      "AbuseSpirituality :  1\n",
      "Self-esteemRelationship :  1\n",
      "ConflictAnger :  2\n",
      "ParentingAnger :  1\n",
      "MarriageFamily :  3\n",
      "ConflictProfessional :  2\n",
      "RelationshipsHuman :  4\n",
      "SexualityLGBTQ :  10\n",
      "RelationshipsParentingFamily :  1\n",
      "Legal :  16\n",
      "LGBTQIntimacy :  1\n",
      "ManagementRelationships :  3\n",
      "AbuseFamily :  1\n",
      "Self-esteemMarriageTraumaIntimacy :  3\n",
      "MarriageAddiction :  1\n",
      "RelationshipsLegal :  1\n",
      "SexualityRelationships :  4\n",
      "ConflictRelationshipsMarriage :  1\n",
      "MarriageAnger :  1\n",
      "RelationshipsFamily :  8\n",
      "Change :  56\n",
      "SexualitySocial :  1\n",
      "Self-esteemEating :  1\n",
      "Career :  2\n",
      "CounselingProfessional :  2\n",
      "MarriageGrief :  1\n",
      "Self-esteemSocial :  5\n",
      "AddictionSubstance :  11\n",
      "Abuse :  23\n",
      "Spirituality :  8\n",
      "RelationshipsSocial :  22\n",
      "SexualityAddiction :  2\n",
      "IntimacyRelationships :  11\n",
      "RelationshipsSelf-esteemHuman :  1\n",
      "Trauma :  28\n",
      "SexualityIntimacyMarriage :  9\n",
      "RelationshipsParenting :  1\n",
      "ConflictParenting :  16\n",
      "RegulatoryProfessional :  3\n",
      "ManagementDomestic :  2\n",
      "ParentingFamily :  6\n",
      "ConflictLegal :  6\n",
      "ViolenceLegal :  1\n",
      "IntimacyHuman :  7\n",
      "IntimacySocial :  1\n",
      "TraumaMilitary :  3\n",
      "ManagementRelationshipsSocial :  3\n",
      "TraumaFamily :  4\n",
      "TraumaSelf-esteemRelationship :  1\n",
      "RelationshipsRelationship :  2\n",
      "RelationshipsBehavioral :  6\n",
      "MarriageDomestic :  1\n",
      "ConflictParentingMarriage :  1\n",
      "MarriageIntimacyHuman :  9\n",
      "MarriageRelationshipsIntimacy :  9\n",
      "SexualityFamily :  1\n",
      "ConflictSpirituality :  1\n",
      "RelationshipsIntimacyHuman :  1\n",
      "ParentingRelationship :  9\n",
      "RelationshipsTrauma :  1\n",
      "AddictionMarriageIntimacy :  3\n",
      "ConflictTrauma :  1\n",
      "ConflictSocial :  1\n",
      "RelationshipsRelationshipsIntimacy :  1\n",
      "LGBTQRelationshipsIntimacy :  1\n",
      "MarriageSocial :  1\n",
      "ManagementSocial :  1\n",
      "RelationshipsRelationships :  1\n",
      "ChangeMarriage :  1\n",
      "ManagementBehavioral :  1\n",
      "ViolenceMarriage :  1\n",
      "LGBTQFamily :  4\n",
      "IntimacyMarriage :  1\n",
      "ConflictRelationshipsIntimacy :  1\n",
      "ConflictLGBTQ :  6\n",
      "SpiritualityRelationships :  2\n",
      "RelationshipsWorkplace :  5\n",
      "SexualityIntimacyRelationships :  14\n",
      "SexualityIntimacy :  8\n",
      "RegulatoryAddiction :  2\n",
      "RelationshipsSubstance :  8\n",
      "Self-esteemSleep :  1\n",
      "RelationshipsChildren :  1\n",
      "ChangeLGBTQ :  1\n",
      "LossFamily :  2\n",
      "Self-esteemBehavioral :  2\n",
      "RelationshipsRelationshipsAddiction :  1\n",
      "DiagnosisCounseling :  4\n",
      "Fundamentals :  87\n",
      "RelationshipsLGBTQ :  2\n",
      "Self-esteemLGBTQ :  1\n",
      "EthicsParentingLegal :  1\n",
      "TraumaRelationships :  8\n",
      "LGBTQHuman :  4\n",
      "IntimacyTrauma :  2\n",
      "ViolenceSleep :  2\n",
      "LossSubstance :  7\n",
      "AbuseTrauma :  7\n",
      "IntimacyRelationshipsHuman :  11\n",
      "RelationshipsCareer :  2\n",
      "ChangeRelationships :  1\n",
      "Addiction :  3\n",
      "ConflictDomestic :  1\n",
      "Alzheimer'sFamily :  2\n",
      "SexualityRelationshipsIntimacy :  2\n",
      "ChangeSleep :  1\n",
      "SpiritualitySocial :  6\n",
      "RelationshipsIntimacyLGBTQ :  5\n",
      "MarriageHuman :  2\n",
      "MarriageIntimacyAddictionBehavioral :  1\n",
      "LossRelationships :  2\n",
      "ConflictRelationship :  1\n",
      "IntimacyRelationshipsDomestic :  1\n",
      "AddictionProfessional :  2\n",
      "ConflictParentingRelationship :  1\n",
      "Diagnosis :  8\n",
      "RegulatoryParentingFamily :  5\n",
      "AbuseSocial :  4\n",
      "RelationshipsAddictionSubstance :  2\n",
      "AbuseSelf-esteem :  1\n",
      "Self-esteemSpirituality :  1\n",
      "SexualityTrauma :  1\n",
      "SexualityTraumaIntimacyRelationships :  1\n",
      "LossTrauma :  1\n",
      "Self-esteemSubstance :  1\n",
      "RelationshipsMarriageWorkplace :  3\n",
      "RelationshipsMilitary :  4\n",
      "RegulatorySubstance :  5\n",
      "ConflictRelationshipsParenting :  2\n",
      "ManagementRelationshipsSubstance :  1\n",
      "RelationshipsIntimacySpirituality :  5\n",
      "SexualityLGBTQIntimacy :  3\n",
      "ParentingChildren :  4\n",
      "Children :  3\n",
      "EthicsCounseling :  15\n",
      "ConflictRelationshipsRelationship :  1\n",
      "AdolescentsBehavioral :  1\n",
      "RelationshipsAddiction :  2\n",
      "ConflictAddictionSubstance :  1\n",
      "ParentingAddiction :  1\n",
      "ManagementSelf-esteemMarriageFamily :  1\n",
      "ConflictParentingChildren :  1\n",
      "total counts1482\n",
      "stress_count 54\n",
      "depression_count 196\n",
      "disorder_count 12\n",
      "anxiety_count 180\n",
      "burn_out_count 0\n",
      "Found selcted count 442\n"
     ]
    }
   ],
   "source": [
    "original_counsel_df = pd.read_csv(\"raw_data/counselchat-data.csv\")\n",
    "\n",
    "from collections import Counter\n",
    "# Ensure you've downloaded the WordNet corpus\n",
    "# Display the word frequencies\n",
    "def get_word_frequencies(counsel_df):\n",
    "    print(\"Orignal length\" + str(len(counsel_df)))\n",
    "    counsel_df = counsel_df[['questionText', 'topics','answerText']]\n",
    "    all_words = ' '.join(counsel_df['topics'].astype(str)).replace(',', '').split()\n",
    "    # Count the frequency of each word\n",
    "    word_count = Counter(all_words)\n",
    "    print(\"\\nWord frequencies in the 'topics' column:\")\n",
    "    found_selected_count = 0\n",
    "    stress_count = 0\n",
    "    depression_count = 0\n",
    "    disorder_count = 0\n",
    "    anxiety_count = 0\n",
    "    burn_out_count = 0\n",
    "    for word, count in word_count.items():\n",
    "        if word.__contains__(\"Stress\"):\n",
    "            stress_count +=count\n",
    "        elif word.__contains__(\"Depression\"):\n",
    "            depression_count += count\n",
    "        elif word.__contains__(\"Disorder\"):\n",
    "            disorder_count += count\n",
    "        elif word.__contains__(\"Anxiety\"):\n",
    "            anxiety_count += count\n",
    "        elif word.__contains__(\"Burnout\"):\n",
    "            burn_out_count += count\n",
    "        else:\n",
    "            print(f\"{word} :  {count}\")\n",
    "    found_selected_count = stress_count + depression_count + disorder_count + anxiety_count + burn_out_count\n",
    "    print(\"total counts\" + str(len(counsel_df)))\n",
    "    print(\"stress_count\", stress_count)\n",
    "    print(\"depression_count\", depression_count)\n",
    "    print(\"disorder_count\", disorder_count)\n",
    "    print(\"anxiety_count\", anxiety_count)\n",
    "    print(\"burn_out_count\", burn_out_count)\n",
    "    print(\"Found selcted count\", found_selected_count)\n",
    "\n",
    "get_word_frequencies(original_counsel_df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy: 0.9976133651551312\n",
      "Number of rows in test set: 1063\n"
     ]
    }
   ],
   "source": [
    "# https://my.clevelandclinic.org/health/diseases/22295-mental-health-disorders\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "counsel_df = original_counsel_df\n",
    "target_Keywords = {\n",
    "    'Anxiety Disorders': ['panic disorder', 'anxiety'],\n",
    "    'Mood Disorders': ['depression', 'burnout'],\n",
    "    'Stress-Related Disorders': ['stress', 'PTSD']\n",
    "}\n",
    "\n",
    "similar_words_dict = {\n",
    "    'panic': ['panic', 'scare', 'terror', 'affright'],\n",
    "    'disorder': ['distract', 'upset', 'trouble', 'disorderliness', 'cark', 'disarray', 'disorder', 'perturb', 'disquiet', 'unhinge'],\n",
    "    'depression': ['impression', 'clinical_depression', 'slump', 'Great_Depression', 'depression', 'low', 'depressive_disorder', 'natural_depression', 'imprint', 'economic_crisis', 'depression'],\n",
    "    'burnout': [],\n",
    "    'stress': ['strain', 'emphasis', 'emphasize', 'tension', 'punctuate', 'focus', 'accentuate', 'try', 'tenseness', 'emphasise', 'stress', 'accent']\n",
    "}\n",
    "\n",
    "def group_diagnosis(row):\n",
    "    all_words = str(row['topics']).lower().split()\n",
    "\n",
    "    for disorder, keywords in target_Keywords.items():\n",
    "        for keyword in keywords:\n",
    "            if any(keyword in word for word in all_words):\n",
    "                return disorder\n",
    "            if keyword in similar_words_dict:\n",
    "                # Check if any of the similar words match\n",
    "                if any(sim_word in all_words for sim_word in similar_words_dict[keyword]):\n",
    "                    return disorder\n",
    "    return None\n",
    "# Original 120.\n",
    "# print(similar_words_dict)\n",
    "# Adding new counsel_df based on items\n",
    "counsel_df[\"re_diagnosis\"] = counsel_df.apply(group_diagnosis, axis=1)\n",
    "# print(counsel_df['re_diagnosis'])\n",
    "# print(len(counsel_df['re_diagnosis']))\n",
    "\n",
    "# cd remapping diagnosis.\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Function to create and train the model\n",
    "# TfidfVectorizer\n",
    "# LogicRegression\n",
    "def create_diagnosis_model(train_data, target_column='re_diagnosis'):\n",
    "    # Extract text and target columns\n",
    "    X_train = train_data['topics']\n",
    "    y_train = train_data[target_column]\n",
    "\n",
    "    # TF-IDF vectorization\n",
    "    tfidf = TfidfVectorizer(max_features=5000)\n",
    "    X_train_tfidf = tfidf.fit_transform(X_train)\n",
    "\n",
    "    # Train a logistic regression model\n",
    "    model = LogisticRegression(max_iter=1000)\n",
    "    model.fit(X_train_tfidf, y_train)\n",
    "\n",
    "    # Evaluate model accuracy on the training set\n",
    "    X_train_pred = model.predict(X_train_tfidf)\n",
    "    accuracy = accuracy_score(y_train, X_train_pred)\n",
    "    print(f\"Training Accuracy: {accuracy}\")\n",
    "\n",
    "    return model, tfidf\n",
    "\n",
    "# Function to make predictions using the trained model\n",
    "def predict_missing_diagnoses(df, model, tfidf, target_column='re_diagnosis'):\n",
    "    # Extarct out df re_diagnosis is None\n",
    "    df_test = df[df[target_column].isna()]\n",
    "\n",
    "    X_test = df_test['topics']\n",
    "    print(f\"Number of rows in test set: {len(X_test)}\")\n",
    "    X_test = df_test['topics'].fillna('')  # Replace NaN values with an empty string\n",
    "    non_empty_mask = X_test.str.strip() != ''\n",
    "\n",
    "    # 한번더 걸러준다\n",
    "    df_test = df_test[non_empty_mask]\n",
    "    X_test =  X_test[non_empty_mask]\n",
    "\n",
    "    # \n",
    "    X_test_tfidf = tfidf.transform(X_test)\n",
    "\n",
    "    # Predict missing diagnoses\n",
    "    predictions = model.predict(X_test_tfidf)\n",
    "\n",
    "    # Assign predictions back to the DataFrame\n",
    "    df.loc[df[target_column].isna() & non_empty_mask, target_column] = predictions\n",
    "\n",
    "    return df\n",
    "\n",
    "# Example usage\n",
    "# Step 1: Create model\n",
    "# Training and prediction process\n",
    "df_train = counsel_df[counsel_df['re_diagnosis'].notna()]\n",
    "model, tfidf = create_diagnosis_model(df_train)\n",
    "remapped_consel_df = predict_missing_diagnoses(counsel_df, model, tfidf)\n",
    "\n",
    "\n",
    "\n",
    "remapped_consel_df.to_csv(\"cleaned_data/predicted_consel_data.csv\", index=False)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def extract_sentence(row):\n",
    "    soup = BeautifulSoup(row['answerText'], 'html.parser')\n",
    "    clean_text = soup.get_text() \n",
    "    return clean_text.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1482\n",
      "Index(['questionID', 'questionTitle', 'questionText', 'questionUrl', 'topics',\n",
      "       'therapistName', 'therapistUrl', 'answerText', 'upvotes',\n",
      "       're_diagnosis'],\n",
      "      dtype='object')\n",
      "                                        questionText  \\\n",
      "0  My wife and mother are having tense disagreeme...   \n",
      "1  I'm planning to have baby, so I have to quit s...   \n",
      "2  I have secrets in my mind, and I don't know wh...   \n",
      "3  I am extremely possessive in my relationships ...   \n",
      "4  I had a head injury a few years ago and my min...   \n",
      "\n",
      "                                   topics       re_diagnosis  \\\n",
      "0                         Family Conflict     Mood Disorders   \n",
      "1               Substance Abuse,Addiction     Mood Disorders   \n",
      "2                         Family Conflict     Mood Disorders   \n",
      "3  Behavioral Change,Social Relationships  Anxiety Disorders   \n",
      "4                                 Anxiety  Anxiety Disorders   \n",
      "\n",
      "                                   clean_answer_text  \n",
      "0  What you are describing is something psycholog...  \n",
      "1  Hi. Good for you in planning ahead to do what'...  \n",
      "2  It sounds like keeping the secrets has become ...  \n",
      "3  Hi there. It's great you are able to realize t...  \n",
      "4  You didn't say what or how many medications yo...  \n"
     ]
    }
   ],
   "source": [
    "print(len(remapped_consel_df))\n",
    "print(remapped_consel_df.columns)\n",
    "consel_df = pd.read_csv(\"cleaned_data/predicted_consel_data.csv\")\n",
    "consel_df[\"clean_answer_text\"] = consel_df.apply(extract_sentence, axis=1)\n",
    "consel_df = consel_df.dropna(subset=['questionText', 'topics', 're_diagnosis', 'clean_answer_text'])\n",
    "consel_df = consel_df[consel_df['questionText'].str.strip() != '']\n",
    "consel_df = consel_df[consel_df['topics'].str.strip() != '']\n",
    "consel_df = consel_df[consel_df['re_diagnosis'].str.strip() != '']\n",
    "consel_df = consel_df[consel_df['clean_answer_text'].str.strip() != '']\n",
    "\n",
    "consel_df = consel_df[['questionText', 'topics', 're_diagnosis', 'clean_answer_text']]\n",
    "# consel_df.to_csv(\"cleaned_data/cleaned_predicted_consel_data.csv\", index=False)\n",
    "print(consel_df.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inital Model created in\n",
    "\"cleaned_data/cleaned_predicted_consel_data.csv\". It includes cleaned_answer_text lists based parsed by BeautifulSoup.\n",
    "Also, it has re_diagnosis column for precited based on their question and topics as well.\n",
    "It will be used as inital thier dianogis belonged threee Mood Disorders, Anxiety Disorders  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inputs: \n",
      "\n",
      "Maximum token length: 667\n",
      "Average token length: 77.877640203933\n",
      "95th percentile token length: 190\n",
      "Total numbers of target_df 1373\n",
      "After drop target_df 1299\n",
      "Maximum token length after drop: 185\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1138 > 1024). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[18], line 54\u001b[0m\n\u001b[0;32m     51\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m target_df\n\u001b[0;32m     53\u001b[0m target_df \u001b[38;5;241m=\u001b[39m clean_inputs_as_well(target_df)\n\u001b[1;32m---> 54\u001b[0m target_df \u001b[38;5;241m=\u001b[39m clean_outputs_as_well(target_df)\n\u001b[0;32m     56\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFinal numbers of target df \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(target_df)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[18], line 33\u001b[0m, in \u001b[0;36mclean_outputs_as_well\u001b[1;34m(target_df)\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mclean_outputs_as_well\u001b[39m(target_df):\n\u001b[0;32m     32\u001b[0m     target_df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutputs_text\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m target_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mclean_answer_text\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m---> 33\u001b[0m     target_df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtoken_length\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m target_df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutputs_text\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m text: \u001b[38;5;28mlen\u001b[39m(tokenizer\u001b[38;5;241m.\u001b[39mencode(text, truncation\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)))\n\u001b[0;32m     35\u001b[0m     outputs_text \u001b[38;5;241m=\u001b[39m (target_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124moutputs_text\u001b[39m\u001b[38;5;124m'\u001b[39m])\u001b[38;5;241m.\u001b[39mtolist()\n\u001b[0;32m     36\u001b[0m     token_lengths \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mlen\u001b[39m(tokenizer\u001b[38;5;241m.\u001b[39mencode(text, truncation\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)) \u001b[38;5;28;01mfor\u001b[39;00m text \u001b[38;5;129;01min\u001b[39;00m outputs_text]\n",
      "File \u001b[1;32mc:\\Users\\ykim\\AppData\\Local\\anaconda3\\Lib\\site-packages\\pandas\\core\\series.py:4771\u001b[0m, in \u001b[0;36mSeries.apply\u001b[1;34m(self, func, convert_dtype, args, **kwargs)\u001b[0m\n\u001b[0;32m   4661\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply\u001b[39m(\n\u001b[0;32m   4662\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   4663\u001b[0m     func: AggFuncType,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   4666\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m   4667\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame \u001b[38;5;241m|\u001b[39m Series:\n\u001b[0;32m   4668\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   4669\u001b[0m \u001b[38;5;124;03m    Invoke function on values of Series.\u001b[39;00m\n\u001b[0;32m   4670\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   4769\u001b[0m \u001b[38;5;124;03m    dtype: float64\u001b[39;00m\n\u001b[0;32m   4770\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 4771\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m SeriesApply(\u001b[38;5;28mself\u001b[39m, func, convert_dtype, args, kwargs)\u001b[38;5;241m.\u001b[39mapply()\n",
      "File \u001b[1;32mc:\\Users\\ykim\\AppData\\Local\\anaconda3\\Lib\\site-packages\\pandas\\core\\apply.py:1123\u001b[0m, in \u001b[0;36mSeriesApply.apply\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1120\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_str()\n\u001b[0;32m   1122\u001b[0m \u001b[38;5;66;03m# self.f is Callable\u001b[39;00m\n\u001b[1;32m-> 1123\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_standard()\n",
      "File \u001b[1;32mc:\\Users\\ykim\\AppData\\Local\\anaconda3\\Lib\\site-packages\\pandas\\core\\apply.py:1174\u001b[0m, in \u001b[0;36mSeriesApply.apply_standard\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1172\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1173\u001b[0m         values \u001b[38;5;241m=\u001b[39m obj\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mobject\u001b[39m)\u001b[38;5;241m.\u001b[39m_values\n\u001b[1;32m-> 1174\u001b[0m         mapped \u001b[38;5;241m=\u001b[39m lib\u001b[38;5;241m.\u001b[39mmap_infer(\n\u001b[0;32m   1175\u001b[0m             values,\n\u001b[0;32m   1176\u001b[0m             f,\n\u001b[0;32m   1177\u001b[0m             convert\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconvert_dtype,\n\u001b[0;32m   1178\u001b[0m         )\n\u001b[0;32m   1180\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(mapped) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(mapped[\u001b[38;5;241m0\u001b[39m], ABCSeries):\n\u001b[0;32m   1181\u001b[0m     \u001b[38;5;66;03m# GH#43986 Need to do list(mapped) in order to get treated as nested\u001b[39;00m\n\u001b[0;32m   1182\u001b[0m     \u001b[38;5;66;03m#  See also GH#25959 regarding EA support\u001b[39;00m\n\u001b[0;32m   1183\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m obj\u001b[38;5;241m.\u001b[39m_constructor_expanddim(\u001b[38;5;28mlist\u001b[39m(mapped), index\u001b[38;5;241m=\u001b[39mobj\u001b[38;5;241m.\u001b[39mindex)\n",
      "File \u001b[1;32mc:\\Users\\ykim\\AppData\\Local\\anaconda3\\Lib\\site-packages\\pandas\\_libs\\lib.pyx:2924\u001b[0m, in \u001b[0;36mpandas._libs.lib.map_infer\u001b[1;34m()\u001b[0m\n",
      "Cell \u001b[1;32mIn[18], line 33\u001b[0m, in \u001b[0;36mclean_outputs_as_well.<locals>.<lambda>\u001b[1;34m(text)\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mclean_outputs_as_well\u001b[39m(target_df):\n\u001b[0;32m     32\u001b[0m     target_df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutputs_text\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m target_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mclean_answer_text\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m---> 33\u001b[0m     target_df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtoken_length\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m target_df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutputs_text\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m text: \u001b[38;5;28mlen\u001b[39m(tokenizer\u001b[38;5;241m.\u001b[39mencode(text, truncation\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)))\n\u001b[0;32m     35\u001b[0m     outputs_text \u001b[38;5;241m=\u001b[39m (target_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124moutputs_text\u001b[39m\u001b[38;5;124m'\u001b[39m])\u001b[38;5;241m.\u001b[39mtolist()\n\u001b[0;32m     36\u001b[0m     token_lengths \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mlen\u001b[39m(tokenizer\u001b[38;5;241m.\u001b[39mencode(text, truncation\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)) \u001b[38;5;28;01mfor\u001b[39;00m text \u001b[38;5;129;01min\u001b[39;00m outputs_text]\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\tokenization_utils_base.py:2788\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.encode\u001b[1;34m(self, text, text_pair, add_special_tokens, padding, truncation, max_length, stride, padding_side, return_tensors, **kwargs)\u001b[0m\n\u001b[0;32m   2750\u001b[0m \u001b[38;5;129m@add_end_docstrings\u001b[39m(\n\u001b[0;32m   2751\u001b[0m     ENCODE_KWARGS_DOCSTRING,\n\u001b[0;32m   2752\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2771\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m   2772\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[\u001b[38;5;28mint\u001b[39m]:\n\u001b[0;32m   2773\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   2774\u001b[0m \u001b[38;5;124;03m    Converts a string to a sequence of ids (integer), using the tokenizer and vocabulary.\u001b[39;00m\n\u001b[0;32m   2775\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2786\u001b[0m \u001b[38;5;124;03m            method).\u001b[39;00m\n\u001b[0;32m   2787\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 2788\u001b[0m     encoded_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencode_plus(\n\u001b[0;32m   2789\u001b[0m         text,\n\u001b[0;32m   2790\u001b[0m         text_pair\u001b[38;5;241m=\u001b[39mtext_pair,\n\u001b[0;32m   2791\u001b[0m         add_special_tokens\u001b[38;5;241m=\u001b[39madd_special_tokens,\n\u001b[0;32m   2792\u001b[0m         padding\u001b[38;5;241m=\u001b[39mpadding,\n\u001b[0;32m   2793\u001b[0m         truncation\u001b[38;5;241m=\u001b[39mtruncation,\n\u001b[0;32m   2794\u001b[0m         max_length\u001b[38;5;241m=\u001b[39mmax_length,\n\u001b[0;32m   2795\u001b[0m         stride\u001b[38;5;241m=\u001b[39mstride,\n\u001b[0;32m   2796\u001b[0m         padding_side\u001b[38;5;241m=\u001b[39mpadding_side,\n\u001b[0;32m   2797\u001b[0m         return_tensors\u001b[38;5;241m=\u001b[39mreturn_tensors,\n\u001b[0;32m   2798\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m   2799\u001b[0m     )\n\u001b[0;32m   2801\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m encoded_inputs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\tokenization_utils_base.py:3207\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.encode_plus\u001b[1;34m(self, text, text_pair, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, padding_side, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[0;32m   3197\u001b[0m \u001b[38;5;66;03m# Backward compatibility for 'truncation_strategy', 'pad_to_max_length'\u001b[39;00m\n\u001b[0;32m   3198\u001b[0m padding_strategy, truncation_strategy, max_length, kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_padding_truncation_strategies(\n\u001b[0;32m   3199\u001b[0m     padding\u001b[38;5;241m=\u001b[39mpadding,\n\u001b[0;32m   3200\u001b[0m     truncation\u001b[38;5;241m=\u001b[39mtruncation,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   3204\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m   3205\u001b[0m )\n\u001b[1;32m-> 3207\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_encode_plus(\n\u001b[0;32m   3208\u001b[0m     text\u001b[38;5;241m=\u001b[39mtext,\n\u001b[0;32m   3209\u001b[0m     text_pair\u001b[38;5;241m=\u001b[39mtext_pair,\n\u001b[0;32m   3210\u001b[0m     add_special_tokens\u001b[38;5;241m=\u001b[39madd_special_tokens,\n\u001b[0;32m   3211\u001b[0m     padding_strategy\u001b[38;5;241m=\u001b[39mpadding_strategy,\n\u001b[0;32m   3212\u001b[0m     truncation_strategy\u001b[38;5;241m=\u001b[39mtruncation_strategy,\n\u001b[0;32m   3213\u001b[0m     max_length\u001b[38;5;241m=\u001b[39mmax_length,\n\u001b[0;32m   3214\u001b[0m     stride\u001b[38;5;241m=\u001b[39mstride,\n\u001b[0;32m   3215\u001b[0m     is_split_into_words\u001b[38;5;241m=\u001b[39mis_split_into_words,\n\u001b[0;32m   3216\u001b[0m     pad_to_multiple_of\u001b[38;5;241m=\u001b[39mpad_to_multiple_of,\n\u001b[0;32m   3217\u001b[0m     padding_side\u001b[38;5;241m=\u001b[39mpadding_side,\n\u001b[0;32m   3218\u001b[0m     return_tensors\u001b[38;5;241m=\u001b[39mreturn_tensors,\n\u001b[0;32m   3219\u001b[0m     return_token_type_ids\u001b[38;5;241m=\u001b[39mreturn_token_type_ids,\n\u001b[0;32m   3220\u001b[0m     return_attention_mask\u001b[38;5;241m=\u001b[39mreturn_attention_mask,\n\u001b[0;32m   3221\u001b[0m     return_overflowing_tokens\u001b[38;5;241m=\u001b[39mreturn_overflowing_tokens,\n\u001b[0;32m   3222\u001b[0m     return_special_tokens_mask\u001b[38;5;241m=\u001b[39mreturn_special_tokens_mask,\n\u001b[0;32m   3223\u001b[0m     return_offsets_mapping\u001b[38;5;241m=\u001b[39mreturn_offsets_mapping,\n\u001b[0;32m   3224\u001b[0m     return_length\u001b[38;5;241m=\u001b[39mreturn_length,\n\u001b[0;32m   3225\u001b[0m     verbose\u001b[38;5;241m=\u001b[39mverbose,\n\u001b[0;32m   3226\u001b[0m     split_special_tokens\u001b[38;5;241m=\u001b[39mkwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msplit_special_tokens\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msplit_special_tokens),\n\u001b[0;32m   3227\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m   3228\u001b[0m )\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\tokenization_utils.py:801\u001b[0m, in \u001b[0;36mPreTrainedTokenizer._encode_plus\u001b[1;34m(self, text, text_pair, add_special_tokens, padding_strategy, truncation_strategy, max_length, stride, is_split_into_words, pad_to_multiple_of, padding_side, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[0;32m    792\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m return_offsets_mapping:\n\u001b[0;32m    793\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\n\u001b[0;32m    794\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreturn_offset_mapping is not available when using Python tokenizers. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    795\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTo use this feature, change your tokenizer to one deriving from \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    798\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://github.com/huggingface/transformers/pull/2674\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    799\u001b[0m     )\n\u001b[1;32m--> 801\u001b[0m first_ids \u001b[38;5;241m=\u001b[39m get_input_ids(text)\n\u001b[0;32m    802\u001b[0m second_ids \u001b[38;5;241m=\u001b[39m get_input_ids(text_pair) \u001b[38;5;28;01mif\u001b[39;00m text_pair \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    804\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprepare_for_model(\n\u001b[0;32m    805\u001b[0m     first_ids,\n\u001b[0;32m    806\u001b[0m     pair_ids\u001b[38;5;241m=\u001b[39msecond_ids,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    821\u001b[0m     verbose\u001b[38;5;241m=\u001b[39mverbose,\n\u001b[0;32m    822\u001b[0m )\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\tokenization_utils.py:768\u001b[0m, in \u001b[0;36mPreTrainedTokenizer._encode_plus.<locals>.get_input_ids\u001b[1;34m(text)\u001b[0m\n\u001b[0;32m    766\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_input_ids\u001b[39m(text):\n\u001b[0;32m    767\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(text, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m--> 768\u001b[0m         tokens \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokenize(text, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    769\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconvert_tokens_to_ids(tokens)\n\u001b[0;32m    770\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(text, (\u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mtuple\u001b[39m)) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(text) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(text[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;28mstr\u001b[39m):\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\tokenization_utils.py:698\u001b[0m, in \u001b[0;36mPreTrainedTokenizer.tokenize\u001b[1;34m(self, text, **kwargs)\u001b[0m\n\u001b[0;32m    696\u001b[0m         tokenized_text\u001b[38;5;241m.\u001b[39mappend(token)\n\u001b[0;32m    697\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 698\u001b[0m         tokenized_text\u001b[38;5;241m.\u001b[39mextend(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tokenize(token))\n\u001b[0;32m    699\u001b[0m \u001b[38;5;66;03m# [\"This\", \" is\", \" something\", \"<special_token_1>\", \"else\"]\u001b[39;00m\n\u001b[0;32m    700\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m tokenized_text\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\models\\gpt2\\tokenization_gpt2.py:277\u001b[0m, in \u001b[0;36mGPT2Tokenizer._tokenize\u001b[1;34m(self, text)\u001b[0m\n\u001b[0;32m    275\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Tokenize a string.\"\"\"\u001b[39;00m\n\u001b[0;32m    276\u001b[0m bpe_tokens \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m--> 277\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m re\u001b[38;5;241m.\u001b[39mfindall(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpat, text):\n\u001b[0;32m    278\u001b[0m     token \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\n\u001b[0;32m    279\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbyte_encoder[b] \u001b[38;5;28;01mfor\u001b[39;00m b \u001b[38;5;129;01min\u001b[39;00m token\u001b[38;5;241m.\u001b[39mencode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    280\u001b[0m     )  \u001b[38;5;66;03m# Maps all our bytes to unicode strings, avoiding control tokens of the BPE (spaces in our case)\u001b[39;00m\n\u001b[0;32m    281\u001b[0m     bpe_tokens\u001b[38;5;241m.\u001b[39mextend(bpe_token \u001b[38;5;28;01mfor\u001b[39;00m bpe_token \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbpe(token)\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m))\n",
      "File \u001b[1;32mc:\\Users\\ykim\\AppData\\Local\\anaconda3\\Lib\\site-packages\\regex\\regex.py:338\u001b[0m, in \u001b[0;36mfindall\u001b[1;34m(pattern, string, flags, pos, endpos, overlapped, concurrent, timeout, ignore_unused, **kwargs)\u001b[0m\n\u001b[0;32m    333\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Return a list of all matches in the string. The matches may be overlapped\u001b[39;00m\n\u001b[0;32m    334\u001b[0m \u001b[38;5;124;03mif overlapped is True. If one or more groups are present in the pattern,\u001b[39;00m\n\u001b[0;32m    335\u001b[0m \u001b[38;5;124;03mreturn a list of groups; this will be a list of tuples if the pattern has\u001b[39;00m\n\u001b[0;32m    336\u001b[0m \u001b[38;5;124;03mmore than one group. Empty matches are included in the result.\"\"\"\u001b[39;00m\n\u001b[0;32m    337\u001b[0m pat \u001b[38;5;241m=\u001b[39m _compile(pattern, flags, ignore_unused, kwargs, \u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m--> 338\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m pat\u001b[38;5;241m.\u001b[39mfindall(string, pos, endpos, overlapped, concurrent, timeout)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# We only use this varible only.\n",
    "import numpy as np\n",
    "target_df = pd.read_csv(\"cleaned_data/cleaned_predicted_consel_data.csv\")\n",
    "from transformers import GPT2Tokenizer\n",
    "\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "\n",
    "# Tokenize the inputs and get their lengths\n",
    "def clean_inputs_as_well(target_df):\n",
    "    target_df[\"inputs_text\"] = \"Question: \" + target_df[\"questionText\"] + \"Diagnosis: \" + target_df[\"re_diagnosis\"]\n",
    "    target_df[\"token_length\"] = target_df[\"inputs_text\"].apply(lambda text: len(tokenizer.encode(text, truncation=False)))\n",
    "\n",
    "    input_texts = (\"Question: \" + target_df[\"questionText\"] + \"Diagnosis: \" + target_df[\"re_diagnosis\"]).tolist()\n",
    "    token_lengths = [len(tokenizer.encode(text, truncation=False)) for text in input_texts]\n",
    "\n",
    "    # Calculate statistics\n",
    "    max_length = max(token_lengths)\n",
    "    avg_length = sum(token_lengths) / len(token_lengths)\n",
    "    percentile_95 = int(np.percentile(token_lengths, 95))  # 95th percentile length\n",
    "    print(\"Inputs: \\n\")\n",
    "    print(f\"Maximum token length: {max_length}\")\n",
    "    print(f\"Average token length: {avg_length}\")\n",
    "    print(f\"95th percentile token length: {percentile_95}\")\n",
    "    print(f\"Total numbers of target_df {len(target_df)}\")\n",
    "    target_df = target_df[target_df[\"token_length\"] < 190]\n",
    "    print(f\"After drop target_df {len(target_df)}\")\n",
    "    max_length_after_drop = target_df[\"token_length\"].max()\n",
    "    print(f\"Maximum token length after drop: {max_length_after_drop}\")\n",
    "    return target_df\n",
    "\n",
    "def clean_outputs_as_well(target_df):\n",
    "    target_df[\"outputs_text\"] = target_df['clean_answer_text']\n",
    "    target_df[\"token_length\"] = target_df[\"outputs_text\"].apply(lambda text: len(tokenizer.encode(text, truncation=False)))\n",
    "\n",
    "    outputs_text = (target_df['outputs_text']).tolist()\n",
    "    token_lengths = [len(tokenizer.encode(text, truncation=False)) for text in outputs_text]\n",
    "\n",
    "    # Calculate statistics\n",
    "    max_length = max(token_lengths)\n",
    "    avg_length = sum(token_lengths) / len(token_lengths)\n",
    "    percentile_95 = int(np.percentile(token_lengths, 95))  # 95th percentile length\n",
    "    print(\"Outputs line as well \\n\")\n",
    "    print(f\"Maximum token length: {max_length}\")\n",
    "    print(f\"Average token length: {avg_length}\")\n",
    "    print(f\"95th percentile token length: {percentile_95}\")\n",
    "    print(f\"Total numbers of target_df {len(target_df)}\")\n",
    "    target_df = target_df[target_df[\"token_length\"] < percentile_95]\n",
    "    print(f\"After drop target_df {len(target_df)}\")\n",
    "    max_length_after_drop = target_df[\"token_length\"].max()\n",
    "    print(f\"Maximum token length after drop: {max_length_after_drop}\")\n",
    "    return target_df\n",
    "\n",
    "target_df = clean_inputs_as_well(target_df)\n",
    "target_df = clean_outputs_as_well(target_df)\n",
    "\n",
    "print(f\"Final numbers of target df {len(target_df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['questionText', 'topics', 're_diagnosis', 'clean_answer_text',\n",
       "       'inputs_text', 'token_length', 'outputs_text'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer, AdamW\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "def train_and_save_model(train_data_path, save_directory, model_name=\"gpt2\", max_length_inputs=185, max_length_outputs=558, batch_size=2, epochs=3, learning_rate=5e-5):\n",
    "    \"\"\"\n",
    "    Train a GPT-2 model using a given dataset and save the model and tokenizer.\n",
    "\n",
    "    Args:\n",
    "        train_data_path (str): Path to the training data file (CSV format).\n",
    "        save_directory (str): Directory to save the trained model and tokenizer.\n",
    "        model_name (str): Name of the pre-trained model to use. Defaults to 'gpt2'.\n",
    "        max_length_inputs (int): Maximum length for input texts. Defaults to 185.\n",
    "        max_length_outputs (int): Maximum length for target texts. Defaults to 558.\n",
    "        batch_size (int): Batch size for training. Defaults to 2.\n",
    "        epochs (int): Number of training epochs. Defaults to 3.\n",
    "        learning_rate (float): Learning rate for the optimizer. Defaults to 5e-5.\n",
    "    \"\"\"\n",
    "\n",
    "    # Load and preprocess data\n",
    "    train_df = pd.read_csv(train_data_path)\n",
    "    train_df[\"input_text\"] = \"Question: \" + train_df[\"questionText\"] + \" Diagnosis: \" + train_df[\"re_diagnosis\"]\n",
    "    train_df[\"target_text\"] = train_df[\"clean_answer_text\"]\n",
    "\n",
    "    # Initialize the model and tokenizer\n",
    "    tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "    model = GPT2LMHeadModel.from_pretrained(model_name)\n",
    "\n",
    "    # Set padding token for GPT-2\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    model.config.pad_token_id = tokenizer.pad_token_id\n",
    "\n",
    "    # Create a custom dataset class\n",
    "    class ChatBotDataSet(Dataset):\n",
    "        def __init__(self, tokenizer, input_texts, target_texts):\n",
    "            self.tokenizer = tokenizer\n",
    "            self.input_texts = input_texts\n",
    "            self.target_texts = target_texts\n",
    "\n",
    "        def __len__(self):\n",
    "            return len(self.input_texts)\n",
    "\n",
    "        def __getitem__(self, index):\n",
    "            # Encode input and target texts with truncation and padding\n",
    "            input_encodings = self.tokenizer(\n",
    "                self.input_texts[index],\n",
    "                max_length=max_length_inputs,\n",
    "                padding=\"max_length\",\n",
    "                truncation=True,\n",
    "                return_tensors=\"pt\"\n",
    "            )\n",
    "            target_encodings = self.tokenizer(\n",
    "                self.target_texts[index],\n",
    "                max_length=max_length_outputs,\n",
    "                padding=\"max_length\",\n",
    "                truncation=True,\n",
    "                return_tensors=\"pt\"\n",
    "            )\n",
    "\n",
    "            return {\n",
    "                \"input_ids\": input_encodings[\"input_ids\"].squeeze(),\n",
    "                \"attention_mask\": input_encodings[\"attention_mask\"].squeeze(),\n",
    "                \"labels\": target_encodings[\"input_ids\"].squeeze(),\n",
    "            }\n",
    "\n",
    "    # Prepare dataset and dataloader\n",
    "    input_texts = train_df[\"input_text\"].tolist()\n",
    "    target_texts = train_df[\"target_text\"].tolist()\n",
    "    dataset = ChatBotDataSet(tokenizer, input_texts, target_texts)\n",
    "    train_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    # Model training setup\n",
    "    optimizer = AdamW(model.parameters(), lr=learning_rate)\n",
    "    model.train()\n",
    "\n",
    "    # # Training loop\n",
    "    # for epoch in range(epochs):\n",
    "    #     for batch in train_loader:\n",
    "    #         optimizer.zero_grad()\n",
    "    #         outputs = model(\n",
    "    #             input_ids=batch[\"input_ids\"],\n",
    "    #             attention_mask=batch[\"attention_mask\"],\n",
    "    #             labels=batch[\"labels\"]\n",
    "    #         )\n",
    "    #         loss = outputs.loss\n",
    "    #         loss.backward()\n",
    "    #         optimizer.step()\n",
    "    #         print(f\"Epoch: {epoch}, Loss: {loss.item()}\")\n",
    "\n",
    "    # # Save the model and tokenizer\n",
    "    # model.save_pretrained(save_directory)\n",
    "    # tokenizer.save_pretrained(save_directory)\n",
    "    # print(f\"Model and tokenizer saved in '{save_directory}'\")\n",
    "\n",
    "# Example usage\n",
    "cleaned_data_path = \"cleaned_data\"\n",
    "train_data_path = f\"{cleaned_data_path}/cleaned_predicted_consel_data.csv\"\n",
    "save_directory = \"trained_models/01_consel_data_with_max_length_model\"\n",
    "train_and_save_model(train_data_path, save_directory)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Issue Found.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: Question: I failed my exam. I think I am going to always fail. Diagnosis: Mood Disorders\n",
      "Generated Response: Question: I failed my exam. I think I am going to always fail. Diagnosis: Mood Disorders, OCD, or any mental disorder\n",
      "\n",
      "Posted by: Anonymous on October 24, 2016 at 3:11 PM\n",
      "\n",
      "\n",
      "\"I was just told I had a mood disorder. I did not get a diagnosis until the end of the day. My doctor said I should be able to pass, so I did.\"\n",
      "\n",
      "\n",
      "I was just told I had a mood disorder. I did not get\n",
      "\n",
      "\n",
      "Input: Question: I feel overwhelmed at work and can't concentrate. Diagnosis: Anxiety Disorders\n",
      "Generated Response: Question: I feel overwhelmed at work and can't concentrate. Diagnosis: Anxiety Disorders\n",
      "\n",
      "I've been in the news lately. I've been on a list of people who have been diagnosed with anxiety disorders, and I've been told that they're going through the same process. But the truth is, I've never had a diagnosis.\n",
      "\n",
      "I've never had a diagnosis, because I've been diagnosed with depression. I've been diagnosed with anxiety disorders. I've never had a diagnosis\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "import logging\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "def generate_responses(input_texts, model_name, trained_models_path):\n",
    "    \"\"\"\n",
    "    Generate responses from the model given a list of input texts.\n",
    "\n",
    "    Args:\n",
    "        input_texts (list): A list of input strings to generate responses for.\n",
    "        model_name (str): Name or path of the model to be used.\n",
    "        trained_models_path (str): Path to the trained models directory.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary with input texts as keys and generated responses as values.\n",
    "    \"\"\"\n",
    "    # Load the tokenizer and the model\n",
    "    model_full_path = f\"{trained_models_path}/{model_name}\"\n",
    "    tokenizer = GPT2Tokenizer.from_pretrained(model_full_path)\n",
    "    model = GPT2LMHeadModel.from_pretrained(model_full_path)\n",
    "\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    model.config.pad_token_id = tokenizer.pad_token_id\n",
    "\n",
    "    responses = {}\n",
    "\n",
    "    for input_text in input_texts:\n",
    "        # Tokenize and encode the input\n",
    "        inputs = tokenizer.encode(input_text, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "        attention_mask = torch.ones_like(inputs)\n",
    "\n",
    "        # Generate the response\n",
    "        outputs = model.generate(\n",
    "            inputs,\n",
    "            max_length=100,\n",
    "            num_return_sequences=1,\n",
    "            attention_mask=attention_mask,\n",
    "            temperature=0.7,      \n",
    "            top_k=50,            \n",
    "            top_p=0.9,             \n",
    "            do_sample=True         \n",
    "        )\n",
    "        response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "        # Store the response\n",
    "        responses[input_text] = response\n",
    "\n",
    "        # Log the input and response\n",
    "        logging.info(f\"Model used: {model_name}\")\n",
    "        logging.info(f\"Model outputs Information: {outputs}\")\n",
    "        logging.info(f\"Input Question: {input_text}\")\n",
    "        logging.info(f\"Generated Response: {response}\")\n",
    "\n",
    "    return responses\n",
    "\n",
    "input_texts = [\n",
    "        \"Question: I failed my exam. I think I am going to always fail. Diagnosis: Mood Disorders\",\n",
    "        \"Question: I feel overwhelmed at work and can't concentrate. Diagnosis: Anxiety Disorders\"\n",
    "    ]\n",
    "model_name = \"01_consel_data_with_max_length_model\"\n",
    "trained_models_path = \"trained_models\"\n",
    "\n",
    "responses = generate_responses(input_texts, model_name, trained_models_path)\n",
    "for question, response in responses.items():\n",
    "    print(f\"Input: {question}\\nGenerated Response: {response}\\n\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Loss: 11.38691520690918\n",
      "Epoch: 0, Loss: 4.974128723144531\n",
      "Epoch: 0, Loss: 4.618574619293213\n",
      "Epoch: 0, Loss: 4.21665096282959\n",
      "Epoch: 0, Loss: 2.757869243621826\n",
      "Epoch: 0, Loss: 5.848262786865234\n",
      "Epoch: 0, Loss: 2.3834915161132812\n",
      "Epoch: 0, Loss: 6.6004638671875\n",
      "Epoch: 0, Loss: 3.3225464820861816\n",
      "Epoch: 0, Loss: 3.9655656814575195\n",
      "Epoch: 0, Loss: 4.671762943267822\n",
      "Epoch: 0, Loss: 4.587943077087402\n",
      "Epoch: 0, Loss: 1.6163400411605835\n",
      "Epoch: 0, Loss: 1.412158489227295\n",
      "Epoch: 0, Loss: 1.9969096183776855\n",
      "Epoch: 0, Loss: 2.111706495285034\n",
      "Epoch: 0, Loss: 3.1563289165496826\n",
      "Epoch: 0, Loss: 7.76963996887207\n",
      "Epoch: 0, Loss: 1.371148705482483\n",
      "Epoch: 0, Loss: 2.7987377643585205\n",
      "Epoch: 0, Loss: 2.839463710784912\n",
      "Epoch: 0, Loss: 1.2391597032546997\n",
      "Epoch: 0, Loss: 3.4987270832061768\n",
      "Epoch: 0, Loss: 4.951460361480713\n",
      "Epoch: 0, Loss: 3.265469789505005\n",
      "Epoch: 0, Loss: 4.276489734649658\n",
      "Epoch: 0, Loss: 2.3980774879455566\n",
      "Epoch: 0, Loss: 3.2678492069244385\n",
      "Epoch: 0, Loss: 3.5446760654449463\n",
      "Epoch: 0, Loss: 2.8787789344787598\n",
      "Epoch: 0, Loss: 4.898336887359619\n",
      "Epoch: 0, Loss: 1.918283224105835\n",
      "Epoch: 0, Loss: 3.153282403945923\n",
      "Epoch: 0, Loss: 2.6886329650878906\n",
      "Epoch: 0, Loss: 4.22497034072876\n",
      "Epoch: 0, Loss: 4.094274044036865\n",
      "Epoch: 0, Loss: 3.4881436824798584\n",
      "Epoch: 0, Loss: 2.805335760116577\n",
      "Epoch: 0, Loss: 5.599975109100342\n",
      "Epoch: 0, Loss: 2.027456283569336\n",
      "Epoch: 0, Loss: 2.596487283706665\n",
      "Epoch: 0, Loss: 8.10047721862793\n",
      "Epoch: 0, Loss: 1.6666897535324097\n",
      "Epoch: 0, Loss: 2.128728151321411\n",
      "Epoch: 0, Loss: 3.1948580741882324\n",
      "Epoch: 0, Loss: 3.613814353942871\n",
      "Epoch: 0, Loss: 1.6990753412246704\n",
      "Epoch: 0, Loss: 5.455094814300537\n",
      "Epoch: 0, Loss: 1.4069498777389526\n",
      "Epoch: 0, Loss: 6.46556282043457\n",
      "Epoch: 0, Loss: 3.7727108001708984\n",
      "Epoch: 0, Loss: 3.220496416091919\n",
      "Epoch: 0, Loss: 1.3417783975601196\n",
      "Epoch: 0, Loss: 4.70972204208374\n",
      "Epoch: 0, Loss: 3.4464237689971924\n",
      "Epoch: 0, Loss: 2.1022884845733643\n",
      "Epoch: 0, Loss: 2.166860818862915\n",
      "Epoch: 0, Loss: 2.607487678527832\n",
      "Epoch: 0, Loss: 4.793612480163574\n",
      "Epoch: 0, Loss: 3.289670944213867\n",
      "Epoch: 0, Loss: 6.1488518714904785\n",
      "Epoch: 0, Loss: 2.333394765853882\n",
      "Epoch: 0, Loss: 2.580080032348633\n",
      "Epoch: 0, Loss: 1.708834171295166\n",
      "Epoch: 0, Loss: 3.381235122680664\n",
      "Epoch: 0, Loss: 5.601075172424316\n",
      "Epoch: 0, Loss: 3.569243907928467\n",
      "Epoch: 0, Loss: 3.676767587661743\n",
      "Epoch: 0, Loss: 6.0408477783203125\n",
      "Epoch: 0, Loss: 3.264951229095459\n",
      "Epoch: 0, Loss: 1.5974512100219727\n",
      "Epoch: 0, Loss: 5.130012512207031\n",
      "Epoch: 0, Loss: 2.8666348457336426\n",
      "Epoch: 0, Loss: 1.9731831550598145\n",
      "Epoch: 0, Loss: 4.154303073883057\n",
      "Epoch: 0, Loss: 2.162522792816162\n",
      "Epoch: 0, Loss: 3.3128890991210938\n",
      "Epoch: 0, Loss: 2.5795860290527344\n",
      "Epoch: 0, Loss: 5.568445682525635\n",
      "Epoch: 0, Loss: 3.0987906455993652\n",
      "Epoch: 0, Loss: 5.9189629554748535\n",
      "Epoch: 0, Loss: 2.1619534492492676\n",
      "Epoch: 0, Loss: 5.294386386871338\n",
      "Epoch: 0, Loss: 2.571317195892334\n",
      "Epoch: 0, Loss: 2.174976348876953\n",
      "Epoch: 0, Loss: 1.7407476902008057\n",
      "Epoch: 0, Loss: 1.232635498046875\n",
      "Epoch: 0, Loss: 3.037984848022461\n",
      "Epoch: 0, Loss: 2.2823688983917236\n",
      "Epoch: 0, Loss: 3.037994861602783\n",
      "Epoch: 0, Loss: 1.9201868772506714\n",
      "Epoch: 0, Loss: 4.428308486938477\n",
      "Epoch: 0, Loss: 1.8449152708053589\n",
      "Epoch: 0, Loss: 2.9164164066314697\n",
      "Epoch: 0, Loss: 5.323266506195068\n",
      "Epoch: 0, Loss: 1.460810661315918\n",
      "Epoch: 0, Loss: 2.185825824737549\n",
      "Epoch: 0, Loss: 2.8175952434539795\n",
      "Epoch: 0, Loss: 1.701737642288208\n",
      "Epoch: 0, Loss: 1.1421436071395874\n",
      "Epoch: 0, Loss: 1.5942801237106323\n",
      "Epoch: 0, Loss: 3.103158473968506\n",
      "Epoch: 0, Loss: 2.224838972091675\n",
      "Epoch: 0, Loss: 5.092677116394043\n",
      "Epoch: 0, Loss: 3.377387285232544\n",
      "Epoch: 0, Loss: 3.0929458141326904\n",
      "Epoch: 0, Loss: 4.13081693649292\n",
      "Epoch: 0, Loss: 3.5823581218719482\n",
      "Epoch: 0, Loss: 4.954768657684326\n",
      "Epoch: 0, Loss: 2.3042712211608887\n",
      "Epoch: 0, Loss: 4.4276299476623535\n",
      "Epoch: 0, Loss: 2.4514846801757812\n",
      "Epoch: 0, Loss: 3.4195120334625244\n",
      "Epoch: 0, Loss: 3.4591267108917236\n",
      "Epoch: 0, Loss: 2.4846839904785156\n",
      "Epoch: 0, Loss: 4.292463302612305\n",
      "Epoch: 0, Loss: 2.1121785640716553\n",
      "Epoch: 0, Loss: 2.575254440307617\n",
      "Epoch: 0, Loss: 2.32757830619812\n",
      "Epoch: 0, Loss: 2.2053093910217285\n",
      "Epoch: 0, Loss: 2.075737953186035\n",
      "Epoch: 0, Loss: 3.4963810443878174\n",
      "Epoch: 0, Loss: 4.402470111846924\n",
      "Epoch: 0, Loss: 1.7729910612106323\n",
      "Epoch: 0, Loss: 2.323137044906616\n",
      "Epoch: 0, Loss: 6.347203254699707\n",
      "Epoch: 0, Loss: 3.3297626972198486\n",
      "Epoch: 0, Loss: 3.0889720916748047\n",
      "Epoch: 0, Loss: 5.501869201660156\n",
      "Epoch: 0, Loss: 2.6851680278778076\n",
      "Epoch: 0, Loss: 3.253892421722412\n",
      "Epoch: 0, Loss: 1.907225251197815\n",
      "Epoch: 0, Loss: 5.139837265014648\n",
      "Epoch: 0, Loss: 2.7750396728515625\n",
      "Epoch: 0, Loss: 3.2313618659973145\n",
      "Epoch: 0, Loss: 1.6782962083816528\n",
      "Epoch: 0, Loss: 2.39107608795166\n",
      "Epoch: 0, Loss: 1.2869364023208618\n",
      "Epoch: 0, Loss: 1.7871904373168945\n",
      "Epoch: 0, Loss: 3.3279330730438232\n",
      "Epoch: 0, Loss: 7.02400016784668\n",
      "Epoch: 0, Loss: 4.252503871917725\n",
      "Epoch: 0, Loss: 4.612652778625488\n",
      "Epoch: 0, Loss: 2.6207823753356934\n",
      "Epoch: 0, Loss: 5.83310079574585\n",
      "Epoch: 0, Loss: 3.001235008239746\n",
      "Epoch: 0, Loss: 2.6868410110473633\n",
      "Epoch: 0, Loss: 2.594271183013916\n",
      "Epoch: 0, Loss: 4.46572208404541\n",
      "Epoch: 0, Loss: 2.9631381034851074\n",
      "Epoch: 0, Loss: 3.9990234375\n",
      "Epoch: 0, Loss: 2.7448043823242188\n",
      "Epoch: 0, Loss: 4.438078880310059\n",
      "Epoch: 0, Loss: 3.533156156539917\n",
      "Epoch: 0, Loss: 2.082642078399658\n",
      "Epoch: 0, Loss: 4.644501209259033\n",
      "Epoch: 0, Loss: 1.5697598457336426\n",
      "Epoch: 0, Loss: 1.9698389768600464\n",
      "Epoch: 0, Loss: 4.380471229553223\n",
      "Epoch: 0, Loss: 2.0269408226013184\n",
      "Epoch: 0, Loss: 1.5787943601608276\n",
      "Epoch: 0, Loss: 2.7829744815826416\n",
      "Epoch: 0, Loss: 1.728825330734253\n",
      "Epoch: 0, Loss: 3.0942676067352295\n",
      "Epoch: 0, Loss: 1.6279921531677246\n",
      "Epoch: 0, Loss: 1.5687077045440674\n",
      "Epoch: 0, Loss: 4.984218120574951\n",
      "Epoch: 0, Loss: 1.2923800945281982\n",
      "Epoch: 0, Loss: 1.9750744104385376\n",
      "Epoch: 0, Loss: 1.8216110467910767\n",
      "Epoch: 0, Loss: 3.28725004196167\n",
      "Epoch: 0, Loss: 1.6303186416625977\n",
      "Epoch: 0, Loss: 1.0384752750396729\n",
      "Epoch: 0, Loss: 4.44689416885376\n",
      "Epoch: 0, Loss: 2.671959400177002\n",
      "Epoch: 0, Loss: 1.943742275238037\n",
      "Epoch: 0, Loss: 2.803459405899048\n",
      "Epoch: 0, Loss: 1.6196037530899048\n",
      "Epoch: 0, Loss: 2.854942798614502\n",
      "Epoch: 0, Loss: 4.724833011627197\n",
      "Epoch: 0, Loss: 5.0916032791137695\n",
      "Epoch: 0, Loss: 2.4020886421203613\n",
      "Epoch: 0, Loss: 2.8626909255981445\n",
      "Epoch: 0, Loss: 5.190805912017822\n",
      "Epoch: 0, Loss: 2.389200210571289\n",
      "Epoch: 0, Loss: 4.276505947113037\n",
      "Epoch: 0, Loss: 2.2900073528289795\n",
      "Epoch: 0, Loss: 3.118288040161133\n",
      "Epoch: 0, Loss: 4.532423973083496\n",
      "Epoch: 0, Loss: 3.2386252880096436\n",
      "Epoch: 0, Loss: 4.454174995422363\n",
      "Epoch: 0, Loss: 1.8692846298217773\n",
      "Epoch: 0, Loss: 2.539235830307007\n",
      "Epoch: 0, Loss: 3.114091634750366\n",
      "Epoch: 0, Loss: 2.716604232788086\n",
      "Epoch: 0, Loss: 2.5952212810516357\n",
      "Epoch: 0, Loss: 2.433138132095337\n",
      "Epoch: 0, Loss: 1.705086588859558\n",
      "Epoch: 0, Loss: 3.1897239685058594\n",
      "Epoch: 0, Loss: 2.8135266304016113\n",
      "Epoch: 0, Loss: 2.577880620956421\n",
      "Epoch: 0, Loss: 4.408375263214111\n",
      "Epoch: 0, Loss: 2.1172268390655518\n",
      "Epoch: 0, Loss: 2.8752782344818115\n",
      "Epoch: 0, Loss: 1.761409044265747\n",
      "Epoch: 0, Loss: 2.228177070617676\n",
      "Epoch: 0, Loss: 1.2953424453735352\n",
      "Epoch: 0, Loss: 2.316028356552124\n",
      "Epoch: 0, Loss: 1.4120018482208252\n",
      "Epoch: 0, Loss: 2.443490505218506\n",
      "Epoch: 0, Loss: 2.034288167953491\n",
      "Epoch: 0, Loss: 2.9555225372314453\n",
      "Epoch: 0, Loss: 3.38862681388855\n",
      "Epoch: 0, Loss: 1.9740312099456787\n",
      "Epoch: 0, Loss: 2.046431541442871\n",
      "Epoch: 0, Loss: 2.4542033672332764\n",
      "Epoch: 0, Loss: 2.550462007522583\n",
      "Epoch: 0, Loss: 2.098737955093384\n",
      "Epoch: 0, Loss: 2.172156810760498\n",
      "Epoch: 0, Loss: 2.288745403289795\n",
      "Epoch: 0, Loss: 3.945919990539551\n",
      "Epoch: 0, Loss: 2.4662046432495117\n",
      "Epoch: 0, Loss: 1.8270751237869263\n",
      "Epoch: 0, Loss: 2.5489237308502197\n",
      "Epoch: 0, Loss: 3.765859603881836\n",
      "Epoch: 0, Loss: 3.1891000270843506\n",
      "Epoch: 0, Loss: 2.583233594894409\n",
      "Epoch: 0, Loss: 1.4403992891311646\n",
      "Epoch: 0, Loss: 2.694110155105591\n",
      "Epoch: 0, Loss: 5.035140514373779\n",
      "Epoch: 0, Loss: 3.529536485671997\n",
      "Epoch: 0, Loss: 4.624975681304932\n",
      "Epoch: 0, Loss: 2.4421021938323975\n",
      "Epoch: 0, Loss: 4.477652549743652\n",
      "Epoch: 0, Loss: 1.9763201475143433\n",
      "Epoch: 0, Loss: 2.006082773208618\n",
      "Epoch: 0, Loss: 2.984204053878784\n",
      "Epoch: 0, Loss: 4.207739353179932\n",
      "Epoch: 0, Loss: 1.8523155450820923\n",
      "Epoch: 0, Loss: 3.5794789791107178\n",
      "Epoch: 0, Loss: 2.0208380222320557\n",
      "Epoch: 0, Loss: 2.496852397918701\n",
      "Epoch: 0, Loss: 2.149376630783081\n",
      "Epoch: 0, Loss: 3.881514549255371\n",
      "Epoch: 0, Loss: 1.0453161001205444\n",
      "Epoch: 0, Loss: 2.8469536304473877\n",
      "Epoch: 0, Loss: 5.365947723388672\n",
      "Epoch: 0, Loss: 5.658223628997803\n",
      "Epoch: 0, Loss: 1.5709502696990967\n",
      "Epoch: 0, Loss: 2.6502623558044434\n",
      "Epoch: 0, Loss: 2.4471521377563477\n",
      "Epoch: 0, Loss: 6.003345966339111\n",
      "Epoch: 0, Loss: 5.014708995819092\n",
      "Epoch: 0, Loss: 5.279180526733398\n",
      "Epoch: 0, Loss: 2.009478807449341\n",
      "Epoch: 0, Loss: 2.21443510055542\n",
      "Epoch: 0, Loss: 3.6782310009002686\n",
      "Epoch: 0, Loss: 3.5936784744262695\n",
      "Epoch: 0, Loss: 5.724625587463379\n",
      "Epoch: 0, Loss: 3.1100828647613525\n",
      "Epoch: 0, Loss: 2.7343997955322266\n",
      "Epoch: 0, Loss: 3.845850944519043\n",
      "Epoch: 0, Loss: 3.277026414871216\n",
      "Epoch: 0, Loss: 3.079411029815674\n",
      "Epoch: 0, Loss: 1.9839329719543457\n",
      "Epoch: 0, Loss: 3.387481451034546\n",
      "Epoch: 0, Loss: 4.133701324462891\n",
      "Epoch: 0, Loss: 4.428666591644287\n",
      "Epoch: 0, Loss: 2.885242223739624\n",
      "Epoch: 0, Loss: 3.571375608444214\n",
      "Epoch: 0, Loss: 4.806298732757568\n",
      "Epoch: 0, Loss: 3.0840742588043213\n",
      "Epoch: 0, Loss: 2.6302490234375\n",
      "Epoch: 0, Loss: 2.0075225830078125\n",
      "Epoch: 0, Loss: 2.826117515563965\n",
      "Epoch: 0, Loss: 3.272426128387451\n",
      "Epoch: 0, Loss: 1.5779129266738892\n",
      "Epoch: 0, Loss: 2.1771442890167236\n",
      "Epoch: 0, Loss: 1.859709620475769\n",
      "Epoch: 0, Loss: 2.2585928440093994\n",
      "Epoch: 0, Loss: 4.689823150634766\n",
      "Epoch: 0, Loss: 3.545529365539551\n",
      "Epoch: 0, Loss: 1.273797631263733\n",
      "Epoch: 0, Loss: 3.7955994606018066\n",
      "Epoch: 0, Loss: 1.6590033769607544\n",
      "Epoch: 0, Loss: 4.905358791351318\n",
      "Epoch: 0, Loss: 0.9085105061531067\n",
      "Epoch: 0, Loss: 2.3874268531799316\n",
      "Epoch: 0, Loss: 3.8150370121002197\n",
      "Epoch: 0, Loss: 2.4842300415039062\n",
      "Epoch: 0, Loss: 1.65004301071167\n",
      "Epoch: 0, Loss: 1.3198212385177612\n",
      "Epoch: 0, Loss: 1.5991798639297485\n",
      "Epoch: 0, Loss: 2.3013792037963867\n",
      "Epoch: 0, Loss: 2.7638967037200928\n",
      "Epoch: 0, Loss: 3.1859185695648193\n",
      "Epoch: 0, Loss: 2.1753759384155273\n",
      "Epoch: 0, Loss: 5.025176525115967\n",
      "Epoch: 0, Loss: 5.828789710998535\n",
      "Epoch: 0, Loss: 2.440887451171875\n",
      "Epoch: 0, Loss: 5.076016902923584\n",
      "Epoch: 0, Loss: 2.7732484340667725\n",
      "Epoch: 0, Loss: 3.0918052196502686\n",
      "Epoch: 0, Loss: 1.838767647743225\n",
      "Epoch: 0, Loss: 4.831697940826416\n",
      "Epoch: 0, Loss: 2.732678174972534\n",
      "Epoch: 0, Loss: 1.5378832817077637\n",
      "Epoch: 0, Loss: 6.631051540374756\n",
      "Epoch: 0, Loss: 3.215113639831543\n",
      "Epoch: 0, Loss: 4.985929489135742\n",
      "Epoch: 0, Loss: 2.1241836547851562\n",
      "Epoch: 0, Loss: 1.884192943572998\n",
      "Epoch: 0, Loss: 5.22990608215332\n",
      "Epoch: 0, Loss: 1.931589961051941\n",
      "Epoch: 0, Loss: 2.1874818801879883\n",
      "Epoch: 0, Loss: 2.5139853954315186\n",
      "Epoch: 0, Loss: 2.645603895187378\n",
      "Epoch: 0, Loss: 3.6755337715148926\n",
      "Epoch: 0, Loss: 3.707296371459961\n",
      "Epoch: 0, Loss: 1.7217237949371338\n",
      "Epoch: 0, Loss: 1.5014214515686035\n",
      "Epoch: 0, Loss: 2.4512124061584473\n",
      "Epoch: 0, Loss: 3.0975451469421387\n",
      "Epoch: 0, Loss: 2.3234384059906006\n",
      "Epoch: 0, Loss: 2.340818166732788\n",
      "Epoch: 0, Loss: 2.608185052871704\n",
      "Epoch: 0, Loss: 2.2053301334381104\n",
      "Epoch: 0, Loss: 4.692936897277832\n",
      "Epoch: 0, Loss: 2.1433632373809814\n",
      "Epoch: 0, Loss: 2.5514278411865234\n",
      "Epoch: 0, Loss: 2.446929931640625\n",
      "Epoch: 0, Loss: 2.417794942855835\n",
      "Epoch: 0, Loss: 1.824444055557251\n",
      "Epoch: 0, Loss: 5.712540626525879\n",
      "Epoch: 0, Loss: 2.1412672996520996\n",
      "Epoch: 0, Loss: 3.6857738494873047\n",
      "Epoch: 0, Loss: 4.099630832672119\n",
      "Epoch: 0, Loss: 1.954448938369751\n",
      "Epoch: 0, Loss: 4.391436576843262\n",
      "Epoch: 0, Loss: 3.42122220993042\n",
      "Epoch: 0, Loss: 2.0259287357330322\n",
      "Epoch: 0, Loss: 1.9622023105621338\n",
      "Epoch: 0, Loss: 4.136872291564941\n",
      "Epoch: 0, Loss: 4.117564678192139\n",
      "Epoch: 0, Loss: 1.3159737586975098\n",
      "Epoch: 0, Loss: 3.5909218788146973\n",
      "Epoch: 0, Loss: 2.2710695266723633\n",
      "Epoch: 0, Loss: 1.7577123641967773\n",
      "Epoch: 0, Loss: 1.9595087766647339\n",
      "Epoch: 0, Loss: 4.7237958908081055\n",
      "Epoch: 0, Loss: 1.9784327745437622\n",
      "Epoch: 0, Loss: 1.3757561445236206\n",
      "Epoch: 0, Loss: 7.015578746795654\n",
      "Epoch: 0, Loss: 2.4569201469421387\n",
      "Epoch: 0, Loss: 4.717419624328613\n",
      "Epoch: 0, Loss: 2.834460973739624\n",
      "Epoch: 0, Loss: 1.8617397546768188\n",
      "Epoch: 0, Loss: 2.636892557144165\n",
      "Epoch: 0, Loss: 5.686904430389404\n",
      "Epoch: 0, Loss: 1.8225690126419067\n",
      "Epoch: 0, Loss: 3.1571648120880127\n",
      "Epoch: 0, Loss: 5.728426933288574\n",
      "Epoch: 0, Loss: 2.0230824947357178\n",
      "Epoch: 0, Loss: 1.6537885665893555\n",
      "Epoch: 0, Loss: 4.07574987411499\n",
      "Epoch: 0, Loss: 5.129542350769043\n",
      "Epoch: 0, Loss: 1.3482791185379028\n",
      "Epoch: 0, Loss: 5.164264678955078\n",
      "Epoch: 0, Loss: 4.106949329376221\n",
      "Epoch: 0, Loss: 2.7476396560668945\n",
      "Epoch: 0, Loss: 1.3999369144439697\n",
      "Epoch: 0, Loss: 2.9907867908477783\n",
      "Epoch: 0, Loss: 4.316381931304932\n",
      "Epoch: 0, Loss: 2.329021692276001\n",
      "Epoch: 0, Loss: 2.8042683601379395\n",
      "Epoch: 0, Loss: 2.8030872344970703\n",
      "Epoch: 0, Loss: 4.874509811401367\n",
      "Epoch: 0, Loss: 3.450838804244995\n",
      "Epoch: 0, Loss: 1.8061236143112183\n",
      "Epoch: 0, Loss: 5.101468563079834\n",
      "Epoch: 0, Loss: 1.9899102449417114\n",
      "Epoch: 0, Loss: 3.605825185775757\n",
      "Epoch: 0, Loss: 2.2486090660095215\n",
      "Epoch: 0, Loss: 5.245321750640869\n",
      "Epoch: 0, Loss: 3.4347641468048096\n",
      "Epoch: 0, Loss: 3.2049813270568848\n",
      "Epoch: 0, Loss: 4.557042598724365\n",
      "Epoch: 0, Loss: 3.842517852783203\n",
      "Epoch: 0, Loss: 2.710134506225586\n",
      "Epoch: 0, Loss: 1.9110771417617798\n",
      "Epoch: 0, Loss: 3.900545358657837\n",
      "Epoch: 0, Loss: 1.3337445259094238\n",
      "Epoch: 0, Loss: 3.648515462875366\n",
      "Epoch: 0, Loss: 5.9671502113342285\n",
      "Epoch: 0, Loss: 4.748511791229248\n",
      "Epoch: 0, Loss: 1.7104905843734741\n",
      "Epoch: 0, Loss: 3.269071340560913\n",
      "Epoch: 0, Loss: 3.219383716583252\n",
      "Epoch: 0, Loss: 2.6644976139068604\n",
      "Epoch: 0, Loss: 2.189448356628418\n",
      "Epoch: 0, Loss: 2.3173327445983887\n",
      "Epoch: 0, Loss: 4.605502605438232\n",
      "Epoch: 0, Loss: 5.380607604980469\n",
      "Epoch: 0, Loss: 2.7232463359832764\n",
      "Epoch: 0, Loss: 2.512221336364746\n",
      "Epoch: 0, Loss: 3.9623897075653076\n",
      "Epoch: 0, Loss: 2.5174520015716553\n",
      "Epoch: 0, Loss: 3.8219122886657715\n",
      "Epoch: 0, Loss: 1.572460651397705\n",
      "Epoch: 0, Loss: 2.5347399711608887\n",
      "Epoch: 0, Loss: 4.896617889404297\n",
      "Epoch: 0, Loss: 3.185063123703003\n",
      "Epoch: 0, Loss: 2.6087212562561035\n",
      "Epoch: 0, Loss: 2.757760763168335\n",
      "Epoch: 0, Loss: 1.9009474515914917\n",
      "Epoch: 0, Loss: 2.825361490249634\n",
      "Epoch: 0, Loss: 3.418673276901245\n",
      "Epoch: 0, Loss: 3.124284029006958\n",
      "Epoch: 0, Loss: 5.418910503387451\n",
      "Epoch: 0, Loss: 2.254988431930542\n",
      "Epoch: 0, Loss: 4.988734245300293\n",
      "Epoch: 0, Loss: 1.6443161964416504\n",
      "Epoch: 0, Loss: 2.452310800552368\n",
      "Epoch: 0, Loss: 1.6762853860855103\n",
      "Epoch: 0, Loss: 2.2503559589385986\n",
      "Epoch: 0, Loss: 3.584848403930664\n",
      "Epoch: 0, Loss: 2.132279396057129\n",
      "Epoch: 0, Loss: 3.0266640186309814\n",
      "Epoch: 0, Loss: 5.730803489685059\n",
      "Epoch: 0, Loss: 1.204099178314209\n",
      "Epoch: 0, Loss: 1.7263095378875732\n",
      "Epoch: 0, Loss: 3.2101964950561523\n",
      "Epoch: 0, Loss: 2.234628677368164\n",
      "Epoch: 0, Loss: 2.4231109619140625\n",
      "Epoch: 0, Loss: 2.021559000015259\n",
      "Epoch: 0, Loss: 3.8472399711608887\n",
      "Epoch: 0, Loss: 4.093402862548828\n",
      "Epoch: 0, Loss: 1.8062186241149902\n",
      "Epoch: 0, Loss: 2.3145272731781006\n",
      "Epoch: 0, Loss: 1.5012595653533936\n",
      "Epoch: 0, Loss: 3.1689341068267822\n",
      "Epoch: 0, Loss: 2.526216745376587\n",
      "Epoch: 0, Loss: 4.16918420791626\n",
      "Epoch: 0, Loss: 2.1752097606658936\n",
      "Epoch: 0, Loss: 7.617367267608643\n",
      "Epoch: 0, Loss: 2.294422149658203\n",
      "Epoch: 0, Loss: 2.40938401222229\n",
      "Epoch: 0, Loss: 5.902098178863525\n",
      "Epoch: 0, Loss: 2.5261025428771973\n",
      "Epoch: 0, Loss: 2.4797568321228027\n",
      "Epoch: 0, Loss: 2.988891124725342\n",
      "Epoch: 0, Loss: 2.9037363529205322\n",
      "Epoch: 0, Loss: 1.7400929927825928\n",
      "Epoch: 0, Loss: 2.4101316928863525\n",
      "Epoch: 0, Loss: 3.189486265182495\n",
      "Epoch: 0, Loss: 2.480278491973877\n",
      "Epoch: 0, Loss: 5.219602584838867\n",
      "Epoch: 0, Loss: 4.2271881103515625\n",
      "Epoch: 0, Loss: 1.7305105924606323\n",
      "Epoch: 0, Loss: 3.418163299560547\n",
      "Epoch: 0, Loss: 3.216400384902954\n",
      "Epoch: 0, Loss: 2.2751851081848145\n",
      "Epoch: 0, Loss: 2.188328266143799\n",
      "Epoch: 0, Loss: 4.848487854003906\n",
      "Epoch: 0, Loss: 2.399594783782959\n",
      "Epoch: 0, Loss: 3.3185417652130127\n",
      "Epoch: 0, Loss: 5.825714111328125\n",
      "Epoch: 0, Loss: 2.214787483215332\n",
      "Epoch: 0, Loss: 3.0184333324432373\n",
      "Epoch: 0, Loss: 4.882937431335449\n",
      "Epoch: 0, Loss: 1.822334885597229\n",
      "Epoch: 0, Loss: 5.874338626861572\n",
      "Epoch: 0, Loss: 3.742893695831299\n",
      "Epoch: 0, Loss: 3.552896022796631\n",
      "Epoch: 0, Loss: 1.7618077993392944\n",
      "Epoch: 0, Loss: 2.7957801818847656\n",
      "Epoch: 0, Loss: 1.895745873451233\n",
      "Epoch: 0, Loss: 4.110987663269043\n",
      "Epoch: 0, Loss: 3.7358314990997314\n",
      "Epoch: 0, Loss: 2.4487016201019287\n",
      "Epoch: 0, Loss: 1.6893041133880615\n",
      "Epoch: 0, Loss: 5.2309441566467285\n",
      "Epoch: 0, Loss: 2.7501022815704346\n",
      "Epoch: 0, Loss: 2.744697093963623\n",
      "Epoch: 0, Loss: 3.9957377910614014\n",
      "Epoch: 0, Loss: 1.4310811758041382\n",
      "Epoch: 0, Loss: 2.3510701656341553\n",
      "Epoch: 0, Loss: 1.4246881008148193\n",
      "Epoch: 0, Loss: 2.586353063583374\n",
      "Epoch: 0, Loss: 5.234050273895264\n",
      "Epoch: 0, Loss: 5.4795427322387695\n",
      "Epoch: 0, Loss: 4.974839687347412\n",
      "Epoch: 0, Loss: 2.2736656665802\n",
      "Epoch: 0, Loss: 4.4117350578308105\n",
      "Epoch: 0, Loss: 3.5635337829589844\n",
      "Epoch: 0, Loss: 2.712390184402466\n",
      "Epoch: 0, Loss: 1.9815974235534668\n",
      "Epoch: 0, Loss: 3.302319049835205\n",
      "Epoch: 0, Loss: 5.370070457458496\n",
      "Epoch: 0, Loss: 2.519076108932495\n",
      "Epoch: 0, Loss: 2.3260622024536133\n",
      "Epoch: 0, Loss: 4.321196556091309\n",
      "Epoch: 0, Loss: 2.2079224586486816\n",
      "Epoch: 0, Loss: 1.7826244831085205\n",
      "Epoch: 0, Loss: 2.212399959564209\n",
      "Epoch: 0, Loss: 3.0996227264404297\n",
      "Epoch: 0, Loss: 1.3700242042541504\n",
      "Epoch: 0, Loss: 4.543703079223633\n",
      "Epoch: 0, Loss: 2.117091178894043\n",
      "Epoch: 0, Loss: 2.2475719451904297\n",
      "Epoch: 0, Loss: 8.246285438537598\n",
      "Epoch: 0, Loss: 2.704537868499756\n",
      "Epoch: 0, Loss: 2.936889410018921\n",
      "Epoch: 0, Loss: 1.9949020147323608\n",
      "Epoch: 0, Loss: 5.756402492523193\n",
      "Epoch: 0, Loss: 3.888672351837158\n",
      "Epoch: 0, Loss: 1.4509165287017822\n",
      "Epoch: 0, Loss: 4.5946879386901855\n",
      "Epoch: 0, Loss: 5.50622034072876\n",
      "Epoch: 0, Loss: 2.4616053104400635\n",
      "Epoch: 0, Loss: 4.665215492248535\n",
      "Epoch: 0, Loss: 3.7401251792907715\n",
      "Epoch: 0, Loss: 4.1707682609558105\n",
      "Epoch: 0, Loss: 2.870741605758667\n",
      "Epoch: 0, Loss: 2.548168897628784\n",
      "Epoch: 0, Loss: 4.480575084686279\n",
      "Epoch: 0, Loss: 3.9117789268493652\n",
      "Epoch: 0, Loss: 2.2600924968719482\n",
      "Epoch: 0, Loss: 3.0028865337371826\n",
      "Epoch: 0, Loss: 3.074347972869873\n",
      "Epoch: 0, Loss: 1.678217887878418\n",
      "Epoch: 0, Loss: 4.36310338973999\n",
      "Epoch: 0, Loss: 3.073606252670288\n",
      "Epoch: 0, Loss: 1.5916038751602173\n",
      "Epoch: 0, Loss: 3.489403486251831\n",
      "Epoch: 0, Loss: 2.390043020248413\n",
      "Epoch: 0, Loss: 5.497940540313721\n",
      "Epoch: 0, Loss: 4.278129577636719\n",
      "Epoch: 0, Loss: 2.7240638732910156\n",
      "Epoch: 0, Loss: 1.3550879955291748\n",
      "Epoch: 0, Loss: 4.668873310089111\n",
      "Epoch: 0, Loss: 1.8113301992416382\n",
      "Epoch: 0, Loss: 3.621384382247925\n",
      "Epoch: 0, Loss: 5.935070037841797\n",
      "Epoch: 0, Loss: 2.183156728744507\n",
      "Epoch: 0, Loss: 2.740851640701294\n",
      "Epoch: 0, Loss: 2.0922608375549316\n",
      "Epoch: 0, Loss: 1.9054980278015137\n",
      "Epoch: 0, Loss: 3.704953670501709\n",
      "Epoch: 0, Loss: 2.7785801887512207\n",
      "Epoch: 0, Loss: 2.764709711074829\n",
      "Epoch: 0, Loss: 4.03277063369751\n",
      "Epoch: 0, Loss: 1.6101624965667725\n",
      "Epoch: 0, Loss: 4.953871250152588\n",
      "Epoch: 0, Loss: 3.3870129585266113\n",
      "Epoch: 0, Loss: 4.637475967407227\n",
      "Epoch: 0, Loss: 1.788509726524353\n",
      "Epoch: 0, Loss: 1.4989455938339233\n",
      "Epoch: 0, Loss: 2.726527214050293\n",
      "Epoch: 0, Loss: 4.0435662269592285\n",
      "Epoch: 0, Loss: 3.366492986679077\n",
      "Epoch: 0, Loss: 2.6755433082580566\n",
      "Epoch: 0, Loss: 3.235409736633301\n",
      "Epoch: 0, Loss: 2.1217806339263916\n",
      "Epoch: 0, Loss: 2.8475067615509033\n",
      "Epoch: 0, Loss: 3.4936959743499756\n",
      "Epoch: 0, Loss: 2.2794432640075684\n",
      "Epoch: 0, Loss: 1.1048610210418701\n",
      "Epoch: 0, Loss: 1.6269813776016235\n",
      "Epoch: 0, Loss: 3.0524754524230957\n",
      "Epoch: 0, Loss: 1.2716641426086426\n",
      "Epoch: 0, Loss: 3.8128585815429688\n",
      "Epoch: 0, Loss: 1.2969372272491455\n",
      "Epoch: 0, Loss: 5.06847620010376\n",
      "Epoch: 0, Loss: 2.803701400756836\n",
      "Epoch: 0, Loss: 2.044761896133423\n",
      "Epoch: 0, Loss: 2.5410895347595215\n",
      "Epoch: 0, Loss: 3.4560904502868652\n",
      "Epoch: 0, Loss: 2.2392184734344482\n",
      "Epoch: 0, Loss: 2.884448766708374\n",
      "Epoch: 0, Loss: 1.6082797050476074\n",
      "Epoch: 0, Loss: 6.5711164474487305\n",
      "Epoch: 0, Loss: 2.5243892669677734\n",
      "Epoch: 0, Loss: 5.934091567993164\n",
      "Epoch: 0, Loss: 1.776297688484192\n",
      "Epoch: 0, Loss: 1.7927615642547607\n",
      "Epoch: 0, Loss: 4.488792419433594\n",
      "Epoch: 0, Loss: 2.0139570236206055\n",
      "Epoch: 0, Loss: 1.6999567747116089\n",
      "Epoch: 0, Loss: 2.5274593830108643\n",
      "Epoch: 0, Loss: 5.357120513916016\n",
      "Epoch: 0, Loss: 1.9221938848495483\n",
      "Epoch: 0, Loss: 4.460483074188232\n",
      "Epoch: 0, Loss: 1.584221601486206\n",
      "Epoch: 0, Loss: 2.3884263038635254\n",
      "Epoch: 0, Loss: 2.972668170928955\n",
      "Epoch: 0, Loss: 5.307248592376709\n",
      "Epoch: 0, Loss: 1.6588014364242554\n",
      "Epoch: 0, Loss: 5.240579128265381\n",
      "Epoch: 0, Loss: 6.532794952392578\n",
      "Epoch: 0, Loss: 3.634929656982422\n",
      "Epoch: 0, Loss: 1.500078797340393\n",
      "Epoch: 0, Loss: 3.7338528633117676\n",
      "Epoch: 0, Loss: 2.426964282989502\n",
      "Epoch: 0, Loss: 3.954611301422119\n",
      "Epoch: 0, Loss: 1.9232577085494995\n",
      "Epoch: 0, Loss: 1.9870223999023438\n",
      "Epoch: 0, Loss: 2.2057080268859863\n",
      "Epoch: 0, Loss: 2.170470714569092\n",
      "Epoch: 0, Loss: 1.8504998683929443\n",
      "Epoch: 0, Loss: 2.902378559112549\n",
      "Epoch: 0, Loss: 1.2885582447052002\n",
      "Epoch: 0, Loss: 4.410739421844482\n",
      "Epoch: 0, Loss: 4.018535614013672\n",
      "Epoch: 0, Loss: 6.2979302406311035\n",
      "Epoch: 0, Loss: 1.3688137531280518\n",
      "Epoch: 0, Loss: 3.6446139812469482\n",
      "Epoch: 0, Loss: 3.534594774246216\n",
      "Epoch: 0, Loss: 3.583153247833252\n",
      "Epoch: 0, Loss: 1.627010703086853\n",
      "Epoch: 0, Loss: 1.8764355182647705\n",
      "Epoch: 0, Loss: 4.118007183074951\n",
      "Epoch: 0, Loss: 2.0549802780151367\n",
      "Epoch: 0, Loss: 2.0693938732147217\n",
      "Epoch: 0, Loss: 5.025518894195557\n",
      "Epoch: 0, Loss: 1.9427961111068726\n",
      "Epoch: 0, Loss: 2.501107692718506\n",
      "Epoch: 0, Loss: 2.373014450073242\n",
      "Epoch: 0, Loss: 4.620998859405518\n",
      "Epoch: 0, Loss: 5.2732744216918945\n",
      "Epoch: 0, Loss: 2.4079675674438477\n",
      "Epoch: 0, Loss: 5.32142448425293\n",
      "Epoch: 0, Loss: 4.7045464515686035\n",
      "Epoch: 0, Loss: 2.164954423904419\n",
      "Epoch: 0, Loss: 1.2823506593704224\n",
      "Epoch: 0, Loss: 2.3408846855163574\n",
      "Epoch: 0, Loss: 5.604003429412842\n",
      "Epoch: 0, Loss: 5.706087589263916\n",
      "Epoch: 0, Loss: 3.0795953273773193\n",
      "Epoch: 0, Loss: 4.38659143447876\n",
      "Epoch: 0, Loss: 4.9729228019714355\n",
      "Epoch: 0, Loss: 2.6609623432159424\n",
      "Epoch: 0, Loss: 2.954911947250366\n",
      "Epoch: 0, Loss: 2.552701234817505\n",
      "Epoch: 0, Loss: 2.2878947257995605\n",
      "Epoch: 0, Loss: 2.922753095626831\n",
      "Epoch: 0, Loss: 2.9387481212615967\n",
      "Epoch: 0, Loss: 1.3228063583374023\n",
      "Epoch: 0, Loss: 2.222017526626587\n",
      "Epoch: 0, Loss: 1.8964197635650635\n",
      "Epoch: 0, Loss: 1.8186964988708496\n",
      "Epoch: 0, Loss: 2.9754693508148193\n",
      "Epoch: 0, Loss: 6.540445327758789\n",
      "Epoch: 0, Loss: 1.4272747039794922\n",
      "Epoch: 0, Loss: 3.0257487297058105\n",
      "Epoch: 0, Loss: 5.170413017272949\n",
      "Epoch: 0, Loss: 1.866816520690918\n",
      "Epoch: 0, Loss: 6.261292934417725\n",
      "Epoch: 0, Loss: 2.516042947769165\n",
      "Epoch: 0, Loss: 3.613283634185791\n",
      "Epoch: 0, Loss: 5.395967483520508\n",
      "Epoch: 0, Loss: 2.5521931648254395\n",
      "Epoch: 0, Loss: 2.7625231742858887\n",
      "Epoch: 0, Loss: 2.4148056507110596\n",
      "Epoch: 0, Loss: 2.4067270755767822\n",
      "Epoch: 0, Loss: 2.3496477603912354\n",
      "Epoch: 0, Loss: 2.152465343475342\n",
      "Epoch: 0, Loss: 3.3698318004608154\n",
      "Epoch: 0, Loss: 3.6876423358917236\n",
      "Epoch: 0, Loss: 2.0411150455474854\n",
      "Epoch: 0, Loss: 2.3274216651916504\n",
      "Epoch: 0, Loss: 1.949320673942566\n",
      "Epoch: 0, Loss: 5.2658586502075195\n",
      "Epoch: 0, Loss: 2.66125750541687\n",
      "Epoch: 0, Loss: 3.2196576595306396\n",
      "Epoch: 0, Loss: 3.3020570278167725\n",
      "Epoch: 0, Loss: 2.6499922275543213\n",
      "Epoch: 0, Loss: 3.6811695098876953\n",
      "Epoch: 0, Loss: 5.920779228210449\n",
      "Epoch: 0, Loss: 4.386548042297363\n",
      "Epoch: 0, Loss: 5.459133625030518\n",
      "Epoch: 0, Loss: 3.300276756286621\n",
      "Epoch: 0, Loss: 1.6239681243896484\n",
      "Epoch: 0, Loss: 1.834942102432251\n",
      "Epoch: 0, Loss: 4.069413185119629\n",
      "Epoch: 0, Loss: 1.815319538116455\n",
      "Epoch: 0, Loss: 4.200070381164551\n",
      "Epoch: 1, Loss: 2.4702069759368896\n",
      "Epoch: 1, Loss: 5.349599838256836\n",
      "Epoch: 1, Loss: 4.884719371795654\n",
      "Epoch: 1, Loss: 4.366069316864014\n",
      "Epoch: 1, Loss: 3.756951332092285\n",
      "Epoch: 1, Loss: 4.2515869140625\n",
      "Epoch: 1, Loss: 5.700973033905029\n",
      "Epoch: 1, Loss: 2.9305081367492676\n",
      "Epoch: 1, Loss: 3.996290445327759\n",
      "Epoch: 1, Loss: 1.6391100883483887\n",
      "Epoch: 1, Loss: 2.6492342948913574\n",
      "Epoch: 1, Loss: 2.5253312587738037\n",
      "Epoch: 1, Loss: 4.642066955566406\n",
      "Epoch: 1, Loss: 2.644510507583618\n",
      "Epoch: 1, Loss: 4.100238800048828\n",
      "Epoch: 1, Loss: 5.131783962249756\n",
      "Epoch: 1, Loss: 2.7334682941436768\n",
      "Epoch: 1, Loss: 1.9620431661605835\n",
      "Epoch: 1, Loss: 4.088626384735107\n",
      "Epoch: 1, Loss: 4.170440196990967\n",
      "Epoch: 1, Loss: 4.526982307434082\n",
      "Epoch: 1, Loss: 2.968179941177368\n",
      "Epoch: 1, Loss: 1.5253040790557861\n",
      "Epoch: 1, Loss: 4.319604873657227\n",
      "Epoch: 1, Loss: 1.8599061965942383\n",
      "Epoch: 1, Loss: 4.439990520477295\n",
      "Epoch: 1, Loss: 2.227302312850952\n",
      "Epoch: 1, Loss: 1.6013975143432617\n",
      "Epoch: 1, Loss: 1.9483195543289185\n",
      "Epoch: 1, Loss: 2.02948260307312\n",
      "Epoch: 1, Loss: 7.421647071838379\n",
      "Epoch: 1, Loss: 3.00346040725708\n",
      "Epoch: 1, Loss: 2.269286632537842\n",
      "Epoch: 1, Loss: 2.4454476833343506\n",
      "Epoch: 1, Loss: 2.1261370182037354\n",
      "Epoch: 1, Loss: 1.961406946182251\n",
      "Epoch: 1, Loss: 2.6003990173339844\n",
      "Epoch: 1, Loss: 3.1095049381256104\n",
      "Epoch: 1, Loss: 3.308520555496216\n",
      "Epoch: 1, Loss: 5.5378499031066895\n",
      "Epoch: 1, Loss: 4.212746620178223\n",
      "Epoch: 1, Loss: 2.8163962364196777\n",
      "Epoch: 1, Loss: 2.373466968536377\n",
      "Epoch: 1, Loss: 4.448857307434082\n",
      "Epoch: 1, Loss: 7.521084785461426\n",
      "Epoch: 1, Loss: 2.058608055114746\n",
      "Epoch: 1, Loss: 2.0221192836761475\n",
      "Epoch: 1, Loss: 2.2574527263641357\n",
      "Epoch: 1, Loss: 2.5835440158843994\n",
      "Epoch: 1, Loss: 4.1166791915893555\n",
      "Epoch: 1, Loss: 2.0419068336486816\n",
      "Epoch: 1, Loss: 2.9019153118133545\n",
      "Epoch: 1, Loss: 4.081858158111572\n",
      "Epoch: 1, Loss: 2.477745771408081\n",
      "Epoch: 1, Loss: 1.6847491264343262\n",
      "Epoch: 1, Loss: 2.6305363178253174\n",
      "Epoch: 1, Loss: 1.1961897611618042\n",
      "Epoch: 1, Loss: 2.629828691482544\n",
      "Epoch: 1, Loss: 1.2249499559402466\n",
      "Epoch: 1, Loss: 4.883744239807129\n",
      "Epoch: 1, Loss: 6.061227798461914\n",
      "Epoch: 1, Loss: 6.427180290222168\n",
      "Epoch: 1, Loss: 4.664976596832275\n",
      "Epoch: 1, Loss: 3.849018096923828\n",
      "Epoch: 1, Loss: 2.702038526535034\n",
      "Epoch: 1, Loss: 3.01373028755188\n",
      "Epoch: 1, Loss: 6.316699504852295\n",
      "Epoch: 1, Loss: 2.2728958129882812\n",
      "Epoch: 1, Loss: 1.68048095703125\n",
      "Epoch: 1, Loss: 4.852946758270264\n",
      "Epoch: 1, Loss: 2.9551291465759277\n",
      "Epoch: 1, Loss: 2.3859496116638184\n",
      "Epoch: 1, Loss: 3.1975131034851074\n",
      "Epoch: 1, Loss: 2.34963321685791\n",
      "Epoch: 1, Loss: 5.82773494720459\n",
      "Epoch: 1, Loss: 1.8622393608093262\n",
      "Epoch: 1, Loss: 3.109921455383301\n",
      "Epoch: 1, Loss: 3.9420886039733887\n",
      "Epoch: 1, Loss: 2.4099340438842773\n",
      "Epoch: 1, Loss: 1.782598853111267\n",
      "Epoch: 1, Loss: 4.585022449493408\n",
      "Epoch: 1, Loss: 1.8608205318450928\n",
      "Epoch: 1, Loss: 2.6373450756073\n",
      "Epoch: 1, Loss: 6.364116191864014\n",
      "Epoch: 1, Loss: 2.661801338195801\n",
      "Epoch: 1, Loss: 2.636711359024048\n",
      "Epoch: 1, Loss: 5.4463791847229\n",
      "Epoch: 1, Loss: 3.5619235038757324\n",
      "Epoch: 1, Loss: 1.1998612880706787\n",
      "Epoch: 1, Loss: 2.2113752365112305\n",
      "Epoch: 1, Loss: 3.3651576042175293\n",
      "Epoch: 1, Loss: 1.882023811340332\n",
      "Epoch: 1, Loss: 3.436824083328247\n",
      "Epoch: 1, Loss: 1.293276071548462\n",
      "Epoch: 1, Loss: 1.6894313097000122\n",
      "Epoch: 1, Loss: 1.2087395191192627\n",
      "Epoch: 1, Loss: 3.9020113945007324\n",
      "Epoch: 1, Loss: 2.3212528228759766\n",
      "Epoch: 1, Loss: 1.8117064237594604\n",
      "Epoch: 1, Loss: 4.942610740661621\n",
      "Epoch: 1, Loss: 1.9659579992294312\n",
      "Epoch: 1, Loss: 5.014430522918701\n",
      "Epoch: 1, Loss: 1.10879647731781\n",
      "Epoch: 1, Loss: 1.4600377082824707\n",
      "Epoch: 1, Loss: 2.213792324066162\n",
      "Epoch: 1, Loss: 3.9016003608703613\n",
      "Epoch: 1, Loss: 1.3516566753387451\n",
      "Epoch: 1, Loss: 2.502948522567749\n",
      "Epoch: 1, Loss: 3.272189140319824\n",
      "Epoch: 1, Loss: 4.116373062133789\n",
      "Epoch: 1, Loss: 6.221548557281494\n",
      "Epoch: 1, Loss: 1.901489496231079\n",
      "Epoch: 1, Loss: 3.436894416809082\n",
      "Epoch: 1, Loss: 4.363221168518066\n",
      "Epoch: 1, Loss: 5.178824424743652\n",
      "Epoch: 1, Loss: 2.8901727199554443\n",
      "Epoch: 1, Loss: 4.869515419006348\n",
      "Epoch: 1, Loss: 4.547181606292725\n",
      "Epoch: 1, Loss: 1.8530302047729492\n",
      "Epoch: 1, Loss: 3.712977409362793\n",
      "Epoch: 1, Loss: 1.9507887363433838\n",
      "Epoch: 1, Loss: 2.4460558891296387\n",
      "Epoch: 1, Loss: 2.545118808746338\n",
      "Epoch: 1, Loss: 2.3495631217956543\n",
      "Epoch: 1, Loss: 2.7757012844085693\n",
      "Epoch: 1, Loss: 6.384799003601074\n",
      "Epoch: 1, Loss: 1.9466735124588013\n",
      "Epoch: 1, Loss: 3.2091307640075684\n",
      "Epoch: 1, Loss: 2.1056113243103027\n",
      "Epoch: 1, Loss: 1.4820319414138794\n",
      "Epoch: 1, Loss: 2.784597873687744\n",
      "Epoch: 1, Loss: 2.6315231323242188\n",
      "Epoch: 1, Loss: 2.9484496116638184\n",
      "Epoch: 1, Loss: 3.1663990020751953\n",
      "Epoch: 1, Loss: 2.696978807449341\n",
      "Epoch: 1, Loss: 2.4713923931121826\n",
      "Epoch: 1, Loss: 1.170009732246399\n",
      "Epoch: 1, Loss: 3.1161718368530273\n",
      "Epoch: 1, Loss: 1.556513786315918\n",
      "Epoch: 1, Loss: 2.2046220302581787\n",
      "Epoch: 1, Loss: 2.3639414310455322\n",
      "Epoch: 1, Loss: 2.4904026985168457\n",
      "Epoch: 1, Loss: 3.6552510261535645\n",
      "Epoch: 1, Loss: 4.367193698883057\n",
      "Epoch: 1, Loss: 1.1981440782546997\n",
      "Epoch: 1, Loss: 3.0411038398742676\n",
      "Epoch: 1, Loss: 1.9765526056289673\n",
      "Epoch: 1, Loss: 5.085457801818848\n",
      "Epoch: 1, Loss: 5.888411045074463\n",
      "Epoch: 1, Loss: 4.9381914138793945\n",
      "Epoch: 1, Loss: 3.7910358905792236\n",
      "Epoch: 1, Loss: 1.8056105375289917\n",
      "Epoch: 1, Loss: 5.786714553833008\n",
      "Epoch: 1, Loss: 2.7868597507476807\n",
      "Epoch: 1, Loss: 2.130176305770874\n",
      "Epoch: 1, Loss: 2.1347992420196533\n",
      "Epoch: 1, Loss: 4.596834659576416\n",
      "Epoch: 1, Loss: 2.704782247543335\n",
      "Epoch: 1, Loss: 3.4479761123657227\n",
      "Epoch: 1, Loss: 1.648901104927063\n",
      "Epoch: 1, Loss: 1.9618401527404785\n",
      "Epoch: 1, Loss: 3.8481345176696777\n",
      "Epoch: 1, Loss: 2.5780584812164307\n",
      "Epoch: 1, Loss: 1.9831169843673706\n",
      "Epoch: 1, Loss: 2.6680641174316406\n",
      "Epoch: 1, Loss: 2.248598337173462\n",
      "Epoch: 1, Loss: 1.9301536083221436\n",
      "Epoch: 1, Loss: 6.010284900665283\n",
      "Epoch: 1, Loss: 1.372488260269165\n",
      "Epoch: 1, Loss: 2.9100165367126465\n",
      "Epoch: 1, Loss: 2.3499510288238525\n",
      "Epoch: 1, Loss: 1.8855270147323608\n",
      "Epoch: 1, Loss: 3.209942102432251\n",
      "Epoch: 1, Loss: 2.938110113143921\n",
      "Epoch: 1, Loss: 2.1611883640289307\n",
      "Epoch: 1, Loss: 1.6172163486480713\n",
      "Epoch: 1, Loss: 2.7022387981414795\n",
      "Epoch: 1, Loss: 2.2740018367767334\n",
      "Epoch: 1, Loss: 3.6475253105163574\n",
      "Epoch: 1, Loss: 1.739176869392395\n",
      "Epoch: 1, Loss: 2.677111864089966\n",
      "Epoch: 1, Loss: 2.434441566467285\n",
      "Epoch: 1, Loss: 1.8820842504501343\n",
      "Epoch: 1, Loss: 2.8258984088897705\n",
      "Epoch: 1, Loss: 1.620711326599121\n",
      "Epoch: 1, Loss: 1.592686414718628\n",
      "Epoch: 1, Loss: 3.408482551574707\n",
      "Epoch: 1, Loss: 1.8095680475234985\n",
      "Epoch: 1, Loss: 1.6909929513931274\n",
      "Epoch: 1, Loss: 2.275946855545044\n",
      "Epoch: 1, Loss: 2.007855176925659\n",
      "Epoch: 1, Loss: 3.260193347930908\n",
      "Epoch: 1, Loss: 1.691314697265625\n",
      "Epoch: 1, Loss: 6.138272285461426\n",
      "Epoch: 1, Loss: 2.9148881435394287\n",
      "Epoch: 1, Loss: 4.491873741149902\n",
      "Epoch: 1, Loss: 4.925178527832031\n",
      "Epoch: 1, Loss: 3.2908477783203125\n",
      "Epoch: 1, Loss: 2.1351137161254883\n",
      "Epoch: 1, Loss: 5.965448379516602\n",
      "Epoch: 1, Loss: 3.0064990520477295\n",
      "Epoch: 1, Loss: 4.8502631187438965\n",
      "Epoch: 1, Loss: 3.364464044570923\n",
      "Epoch: 1, Loss: 1.5736585855484009\n",
      "Epoch: 1, Loss: 3.1486926078796387\n",
      "Epoch: 1, Loss: 2.554304599761963\n",
      "Epoch: 1, Loss: 2.294174909591675\n",
      "Epoch: 1, Loss: 2.6702098846435547\n",
      "Epoch: 1, Loss: 2.4894421100616455\n",
      "Epoch: 1, Loss: 1.9677002429962158\n",
      "Epoch: 1, Loss: 2.0694973468780518\n",
      "Epoch: 1, Loss: 1.6222176551818848\n",
      "Epoch: 1, Loss: 4.8835530281066895\n",
      "Epoch: 1, Loss: 2.14235520362854\n",
      "Epoch: 1, Loss: 2.4678661823272705\n",
      "Epoch: 1, Loss: 2.06903076171875\n",
      "Epoch: 1, Loss: 2.3886783123016357\n",
      "Epoch: 1, Loss: 1.2725037336349487\n",
      "Epoch: 1, Loss: 3.97411847114563\n",
      "Epoch: 1, Loss: 4.402564525604248\n",
      "Epoch: 1, Loss: 3.2482995986938477\n",
      "Epoch: 1, Loss: 6.092767715454102\n",
      "Epoch: 1, Loss: 3.4169676303863525\n",
      "Epoch: 1, Loss: 5.034658908843994\n",
      "Epoch: 1, Loss: 3.256401538848877\n",
      "Epoch: 1, Loss: 2.189931631088257\n",
      "Epoch: 1, Loss: 2.213953971862793\n",
      "Epoch: 1, Loss: 2.36942720413208\n",
      "Epoch: 1, Loss: 2.6884961128234863\n",
      "Epoch: 1, Loss: 3.2708256244659424\n",
      "Epoch: 1, Loss: 5.499090671539307\n",
      "Epoch: 1, Loss: 2.5435080528259277\n",
      "Epoch: 1, Loss: 4.05246639251709\n",
      "Epoch: 1, Loss: 2.6716887950897217\n",
      "Epoch: 1, Loss: 1.4468655586242676\n",
      "Epoch: 1, Loss: 5.113815784454346\n",
      "Epoch: 1, Loss: 4.773104667663574\n",
      "Epoch: 1, Loss: 1.9923704862594604\n",
      "Epoch: 1, Loss: 2.2633566856384277\n",
      "Epoch: 1, Loss: 2.0108015537261963\n",
      "Epoch: 1, Loss: 3.987921714782715\n",
      "Epoch: 1, Loss: 4.557899475097656\n",
      "Epoch: 1, Loss: 2.1375203132629395\n",
      "Epoch: 1, Loss: 1.6980265378952026\n",
      "Epoch: 1, Loss: 3.272730588912964\n",
      "Epoch: 1, Loss: 2.9808971881866455\n",
      "Epoch: 1, Loss: 3.198932647705078\n",
      "Epoch: 1, Loss: 1.4288047552108765\n",
      "Epoch: 1, Loss: 2.5425610542297363\n",
      "Epoch: 1, Loss: 3.190136194229126\n",
      "Epoch: 1, Loss: 2.0856943130493164\n",
      "Epoch: 1, Loss: 1.8731715679168701\n",
      "Epoch: 1, Loss: 2.504713535308838\n",
      "Epoch: 1, Loss: 1.8692140579223633\n",
      "Epoch: 1, Loss: 1.3271772861480713\n",
      "Epoch: 1, Loss: 4.820254802703857\n",
      "Epoch: 1, Loss: 1.1094402074813843\n",
      "Epoch: 1, Loss: 2.6253280639648438\n",
      "Epoch: 1, Loss: 5.56463098526001\n",
      "Epoch: 1, Loss: 1.2413862943649292\n",
      "Epoch: 1, Loss: 2.658621311187744\n",
      "Epoch: 1, Loss: 3.7857706546783447\n",
      "Epoch: 1, Loss: 1.4268438816070557\n",
      "Epoch: 1, Loss: 8.127521514892578\n",
      "Epoch: 1, Loss: 2.100281238555908\n",
      "Epoch: 1, Loss: 6.581634044647217\n",
      "Epoch: 1, Loss: 3.1316850185394287\n",
      "Epoch: 1, Loss: 4.187780857086182\n",
      "Epoch: 1, Loss: 5.053724765777588\n",
      "Epoch: 1, Loss: 2.378645658493042\n",
      "Epoch: 1, Loss: 2.108022451400757\n",
      "Epoch: 1, Loss: 3.1893727779388428\n",
      "Epoch: 1, Loss: 4.781483173370361\n",
      "Epoch: 1, Loss: 3.6179678440093994\n",
      "Epoch: 1, Loss: 2.6420156955718994\n",
      "Epoch: 1, Loss: 4.9243245124816895\n",
      "Epoch: 1, Loss: 2.222370147705078\n",
      "Epoch: 1, Loss: 3.552762746810913\n",
      "Epoch: 1, Loss: 3.1761512756347656\n",
      "Epoch: 1, Loss: 2.489211082458496\n",
      "Epoch: 1, Loss: 3.9709174633026123\n",
      "Epoch: 1, Loss: 2.674374580383301\n",
      "Epoch: 1, Loss: 2.1687304973602295\n",
      "Epoch: 1, Loss: 5.134385108947754\n",
      "Epoch: 1, Loss: 1.9158966541290283\n",
      "Epoch: 1, Loss: 5.618895530700684\n",
      "Epoch: 1, Loss: 2.310350179672241\n",
      "Epoch: 1, Loss: 1.80372154712677\n",
      "Epoch: 1, Loss: 2.6350948810577393\n",
      "Epoch: 1, Loss: 2.175692319869995\n",
      "Epoch: 1, Loss: 1.2146589756011963\n",
      "Epoch: 1, Loss: 4.4983720779418945\n",
      "Epoch: 1, Loss: 3.6223418712615967\n",
      "Epoch: 1, Loss: 4.828336715698242\n",
      "Epoch: 1, Loss: 1.0563693046569824\n",
      "Epoch: 1, Loss: 1.3625826835632324\n",
      "Epoch: 1, Loss: 2.480247974395752\n",
      "Epoch: 1, Loss: 2.7677483558654785\n",
      "Epoch: 1, Loss: 3.6039843559265137\n",
      "Epoch: 1, Loss: 3.6671454906463623\n",
      "Epoch: 1, Loss: 2.36297869682312\n",
      "Epoch: 1, Loss: 4.338528156280518\n",
      "Epoch: 1, Loss: 3.3330800533294678\n",
      "Epoch: 1, Loss: 2.9755337238311768\n",
      "Epoch: 1, Loss: 2.5129289627075195\n",
      "Epoch: 1, Loss: 2.321706771850586\n",
      "Epoch: 1, Loss: 1.9179490804672241\n",
      "Epoch: 1, Loss: 3.1010653972625732\n",
      "Epoch: 1, Loss: 1.3283358812332153\n",
      "Epoch: 1, Loss: 2.503187894821167\n",
      "Epoch: 1, Loss: 2.2948474884033203\n",
      "Epoch: 1, Loss: 3.500429391860962\n",
      "Epoch: 1, Loss: 2.331371307373047\n",
      "Epoch: 1, Loss: 5.602731704711914\n",
      "Epoch: 1, Loss: 1.9910731315612793\n",
      "Epoch: 1, Loss: 2.546184539794922\n",
      "Epoch: 1, Loss: 2.3006396293640137\n",
      "Epoch: 1, Loss: 2.159360885620117\n",
      "Epoch: 1, Loss: 1.7212369441986084\n",
      "Epoch: 1, Loss: 1.4035626649856567\n",
      "Epoch: 1, Loss: 1.893776297569275\n",
      "Epoch: 1, Loss: 1.5777584314346313\n",
      "Epoch: 1, Loss: 2.280763626098633\n",
      "Epoch: 1, Loss: 2.816920518875122\n",
      "Epoch: 1, Loss: 3.8401081562042236\n",
      "Epoch: 1, Loss: 3.9884934425354004\n",
      "Epoch: 1, Loss: 2.328321933746338\n",
      "Epoch: 1, Loss: 3.491549015045166\n",
      "Epoch: 1, Loss: 3.891432046890259\n",
      "Epoch: 1, Loss: 2.4787235260009766\n",
      "Epoch: 1, Loss: 1.7814925909042358\n",
      "Epoch: 1, Loss: 2.8829078674316406\n",
      "Epoch: 1, Loss: 4.728230953216553\n",
      "Epoch: 1, Loss: 2.3384971618652344\n",
      "Epoch: 1, Loss: 2.493490219116211\n",
      "Epoch: 1, Loss: 1.5196577310562134\n",
      "Epoch: 1, Loss: 2.3866679668426514\n",
      "Epoch: 1, Loss: 2.618530035018921\n",
      "Epoch: 1, Loss: 1.512161374092102\n",
      "Epoch: 1, Loss: 1.959784984588623\n",
      "Epoch: 1, Loss: 2.5884766578674316\n",
      "Epoch: 1, Loss: 1.4169244766235352\n",
      "Epoch: 1, Loss: 8.413681983947754\n",
      "Epoch: 1, Loss: 1.7372456789016724\n",
      "Epoch: 1, Loss: 3.369264841079712\n",
      "Epoch: 1, Loss: 2.2572202682495117\n",
      "Epoch: 1, Loss: 2.7792460918426514\n",
      "Epoch: 1, Loss: 2.4909613132476807\n",
      "Epoch: 1, Loss: 4.3510966300964355\n",
      "Epoch: 1, Loss: 4.922553539276123\n",
      "Epoch: 1, Loss: 2.1035654544830322\n",
      "Epoch: 1, Loss: 1.266471266746521\n",
      "Epoch: 1, Loss: 1.8744312524795532\n",
      "Epoch: 1, Loss: 2.068683624267578\n",
      "Epoch: 1, Loss: 3.1307570934295654\n",
      "Epoch: 1, Loss: 5.511196613311768\n",
      "Epoch: 1, Loss: 2.3552746772766113\n",
      "Epoch: 1, Loss: 1.618717908859253\n",
      "Epoch: 1, Loss: 4.0080037117004395\n",
      "Epoch: 1, Loss: 2.0644876956939697\n",
      "Epoch: 1, Loss: 3.125833749771118\n",
      "Epoch: 1, Loss: 4.4630351066589355\n",
      "Epoch: 1, Loss: 5.121720790863037\n",
      "Epoch: 1, Loss: 1.565789818763733\n",
      "Epoch: 1, Loss: 2.459430456161499\n",
      "Epoch: 1, Loss: 3.6655538082122803\n",
      "Epoch: 1, Loss: 2.083559989929199\n",
      "Epoch: 1, Loss: 2.4237310886383057\n",
      "Epoch: 1, Loss: 4.101325988769531\n",
      "Epoch: 1, Loss: 2.775357246398926\n",
      "Epoch: 1, Loss: 5.762200832366943\n",
      "Epoch: 1, Loss: 1.4326138496398926\n",
      "Epoch: 1, Loss: 4.73072624206543\n",
      "Epoch: 1, Loss: 2.9785873889923096\n",
      "Epoch: 1, Loss: 1.9983129501342773\n",
      "Epoch: 1, Loss: 1.802506923675537\n",
      "Epoch: 1, Loss: 4.0911102294921875\n",
      "Epoch: 1, Loss: 2.0136609077453613\n",
      "Epoch: 1, Loss: 3.164909601211548\n",
      "Epoch: 1, Loss: 4.38587760925293\n",
      "Epoch: 1, Loss: 4.071960926055908\n",
      "Epoch: 1, Loss: 2.5625193119049072\n",
      "Epoch: 1, Loss: 1.8535239696502686\n",
      "Epoch: 1, Loss: 1.7019771337509155\n",
      "Epoch: 1, Loss: 3.1829898357391357\n",
      "Epoch: 1, Loss: 4.91799783706665\n",
      "Epoch: 1, Loss: 1.6557879447937012\n",
      "Epoch: 1, Loss: 3.222717523574829\n",
      "Epoch: 1, Loss: 1.7516086101531982\n",
      "Epoch: 1, Loss: 3.7463672161102295\n",
      "Epoch: 1, Loss: 2.3267974853515625\n",
      "Epoch: 1, Loss: 4.241560935974121\n",
      "Epoch: 1, Loss: 3.16680645942688\n",
      "Epoch: 1, Loss: 1.4263641834259033\n",
      "Epoch: 1, Loss: 1.7388396263122559\n",
      "Epoch: 1, Loss: 1.4230659008026123\n",
      "Epoch: 1, Loss: 1.6031228303909302\n",
      "Epoch: 1, Loss: 2.6155178546905518\n",
      "Epoch: 1, Loss: 4.621245861053467\n",
      "Epoch: 1, Loss: 5.4697771072387695\n",
      "Epoch: 1, Loss: 2.4900712966918945\n",
      "Epoch: 1, Loss: 2.8543076515197754\n",
      "Epoch: 1, Loss: 3.32932186126709\n",
      "Epoch: 1, Loss: 2.4632205963134766\n",
      "Epoch: 1, Loss: 2.350959062576294\n",
      "Epoch: 1, Loss: 1.7698616981506348\n",
      "Epoch: 1, Loss: 2.6101715564727783\n",
      "Epoch: 1, Loss: 2.3354642391204834\n",
      "Epoch: 1, Loss: 3.4062557220458984\n",
      "Epoch: 1, Loss: 5.207520008087158\n",
      "Epoch: 1, Loss: 6.740660667419434\n",
      "Epoch: 1, Loss: 3.7955727577209473\n",
      "Epoch: 1, Loss: 2.626208782196045\n",
      "Epoch: 1, Loss: 1.8406579494476318\n",
      "Epoch: 1, Loss: 1.7271504402160645\n",
      "Epoch: 1, Loss: 2.988283157348633\n",
      "Epoch: 1, Loss: 2.896498680114746\n",
      "Epoch: 1, Loss: 3.6793267726898193\n",
      "Epoch: 1, Loss: 3.2333388328552246\n",
      "Epoch: 1, Loss: 1.9656140804290771\n",
      "Epoch: 1, Loss: 4.177809715270996\n",
      "Epoch: 1, Loss: 4.494997978210449\n",
      "Epoch: 1, Loss: 3.659559965133667\n",
      "Epoch: 1, Loss: 1.7983291149139404\n",
      "Epoch: 1, Loss: 3.9876911640167236\n",
      "Epoch: 1, Loss: 2.491352081298828\n",
      "Epoch: 1, Loss: 1.8796088695526123\n",
      "Epoch: 1, Loss: 2.368114948272705\n",
      "Epoch: 1, Loss: 1.5451496839523315\n",
      "Epoch: 1, Loss: 1.4192819595336914\n",
      "Epoch: 1, Loss: 2.1484248638153076\n",
      "Epoch: 1, Loss: 2.413626194000244\n",
      "Epoch: 1, Loss: 1.4611293077468872\n",
      "Epoch: 1, Loss: 5.4453887939453125\n",
      "Epoch: 1, Loss: 4.824461460113525\n",
      "Epoch: 1, Loss: 1.3908052444458008\n",
      "Epoch: 1, Loss: 2.3531415462493896\n",
      "Epoch: 1, Loss: 3.9551548957824707\n",
      "Epoch: 1, Loss: 3.3924150466918945\n",
      "Epoch: 1, Loss: 3.543433427810669\n",
      "Epoch: 1, Loss: 4.626418590545654\n",
      "Epoch: 1, Loss: 1.6798679828643799\n",
      "Epoch: 1, Loss: 3.1881487369537354\n",
      "Epoch: 1, Loss: 3.735999345779419\n",
      "Epoch: 1, Loss: 1.7761167287826538\n",
      "Epoch: 1, Loss: 3.371335506439209\n",
      "Epoch: 1, Loss: 3.52130126953125\n",
      "Epoch: 1, Loss: 4.41547155380249\n",
      "Epoch: 1, Loss: 1.8066326379776\n",
      "Epoch: 1, Loss: 2.4958348274230957\n",
      "Epoch: 1, Loss: 2.467782974243164\n",
      "Epoch: 1, Loss: 2.3856375217437744\n",
      "Epoch: 1, Loss: 4.739928722381592\n",
      "Epoch: 1, Loss: 2.5169246196746826\n",
      "Epoch: 1, Loss: 2.2653212547302246\n",
      "Epoch: 1, Loss: 2.453474998474121\n",
      "Epoch: 1, Loss: 4.062339782714844\n",
      "Epoch: 1, Loss: 2.1673929691314697\n",
      "Epoch: 1, Loss: 3.0238170623779297\n",
      "Epoch: 1, Loss: 3.2018396854400635\n",
      "Epoch: 1, Loss: 2.604602813720703\n",
      "Epoch: 1, Loss: 2.0532665252685547\n",
      "Epoch: 1, Loss: 2.2497339248657227\n",
      "Epoch: 1, Loss: 4.236048698425293\n",
      "Epoch: 1, Loss: 5.12320613861084\n",
      "Epoch: 1, Loss: 5.069457054138184\n",
      "Epoch: 1, Loss: 3.554237127304077\n",
      "Epoch: 1, Loss: 1.7281324863433838\n",
      "Epoch: 1, Loss: 1.8851877450942993\n",
      "Epoch: 1, Loss: 2.2151753902435303\n",
      "Epoch: 1, Loss: 2.5339763164520264\n",
      "Epoch: 1, Loss: 1.3599448204040527\n",
      "Epoch: 1, Loss: 2.323539972305298\n",
      "Epoch: 1, Loss: 2.4058454036712646\n",
      "Epoch: 1, Loss: 1.9129612445831299\n",
      "Epoch: 1, Loss: 1.375119924545288\n",
      "Epoch: 1, Loss: 1.7329210042953491\n",
      "Epoch: 1, Loss: 4.92133903503418\n",
      "Epoch: 1, Loss: 7.736087799072266\n",
      "Epoch: 1, Loss: 4.175149917602539\n",
      "Epoch: 1, Loss: 1.2848674058914185\n",
      "Epoch: 1, Loss: 4.889190196990967\n",
      "Epoch: 1, Loss: 2.3481552600860596\n",
      "Epoch: 1, Loss: 2.128984212875366\n",
      "Epoch: 1, Loss: 4.439512729644775\n",
      "Epoch: 1, Loss: 2.712139368057251\n",
      "Epoch: 1, Loss: 2.0921614170074463\n",
      "Epoch: 1, Loss: 4.796608924865723\n",
      "Epoch: 1, Loss: 3.715531826019287\n",
      "Epoch: 1, Loss: 2.727505683898926\n",
      "Epoch: 1, Loss: 3.671586751937866\n",
      "Epoch: 1, Loss: 1.7165648937225342\n",
      "Epoch: 1, Loss: 5.760412216186523\n",
      "Epoch: 1, Loss: 2.898669719696045\n",
      "Epoch: 1, Loss: 2.637836456298828\n",
      "Epoch: 1, Loss: 1.601701021194458\n",
      "Epoch: 1, Loss: 5.002289295196533\n",
      "Epoch: 1, Loss: 3.5529561042785645\n",
      "Epoch: 1, Loss: 2.364889621734619\n",
      "Epoch: 1, Loss: 5.707154273986816\n",
      "Epoch: 1, Loss: 2.8451356887817383\n",
      "Epoch: 1, Loss: 2.8821377754211426\n",
      "Epoch: 1, Loss: 3.055367946624756\n",
      "Epoch: 1, Loss: 1.6832925081253052\n",
      "Epoch: 1, Loss: 3.377004384994507\n",
      "Epoch: 1, Loss: 3.7902069091796875\n",
      "Epoch: 1, Loss: 2.8536629676818848\n",
      "Epoch: 1, Loss: 3.1426141262054443\n",
      "Epoch: 1, Loss: 3.575716972351074\n",
      "Epoch: 1, Loss: 4.279880523681641\n",
      "Epoch: 1, Loss: 4.5503950119018555\n",
      "Epoch: 1, Loss: 1.8898024559020996\n",
      "Epoch: 1, Loss: 5.251199722290039\n",
      "Epoch: 1, Loss: 2.598301649093628\n",
      "Epoch: 1, Loss: 4.515984535217285\n",
      "Epoch: 1, Loss: 2.8174080848693848\n",
      "Epoch: 1, Loss: 1.5260037183761597\n",
      "Epoch: 1, Loss: 2.213280439376831\n",
      "Epoch: 1, Loss: 2.3550000190734863\n",
      "Epoch: 1, Loss: 2.2454116344451904\n",
      "Epoch: 1, Loss: 3.081501007080078\n",
      "Epoch: 1, Loss: 1.863243579864502\n",
      "Epoch: 1, Loss: 3.2705676555633545\n",
      "Epoch: 1, Loss: 2.5755128860473633\n",
      "Epoch: 1, Loss: 2.7682905197143555\n",
      "Epoch: 1, Loss: 2.035895347595215\n",
      "Epoch: 1, Loss: 2.212620973587036\n",
      "Epoch: 1, Loss: 4.40854549407959\n",
      "Epoch: 1, Loss: 1.993323802947998\n",
      "Epoch: 1, Loss: 3.864663600921631\n",
      "Epoch: 1, Loss: 6.343794345855713\n",
      "Epoch: 1, Loss: 1.3283203840255737\n",
      "Epoch: 1, Loss: 4.93787145614624\n",
      "Epoch: 1, Loss: 1.2469673156738281\n",
      "Epoch: 1, Loss: 1.8280688524246216\n",
      "Epoch: 1, Loss: 4.777132987976074\n",
      "Epoch: 1, Loss: 2.299194097518921\n",
      "Epoch: 1, Loss: 7.164146900177002\n",
      "Epoch: 1, Loss: 1.8708370923995972\n",
      "Epoch: 1, Loss: 2.6916093826293945\n",
      "Epoch: 1, Loss: 1.139383316040039\n",
      "Epoch: 1, Loss: 5.2940144538879395\n",
      "Epoch: 1, Loss: 1.9246622323989868\n",
      "Epoch: 1, Loss: 4.5066118240356445\n",
      "Epoch: 1, Loss: 2.912097215652466\n",
      "Epoch: 1, Loss: 2.3365845680236816\n",
      "Epoch: 1, Loss: 4.537522315979004\n",
      "Epoch: 1, Loss: 1.407187819480896\n",
      "Epoch: 1, Loss: 2.357372283935547\n",
      "Epoch: 1, Loss: 4.693628311157227\n",
      "Epoch: 1, Loss: 5.126101493835449\n",
      "Epoch: 1, Loss: 2.6047487258911133\n",
      "Epoch: 1, Loss: 5.387199401855469\n",
      "Epoch: 1, Loss: 2.135287284851074\n",
      "Epoch: 1, Loss: 2.5019285678863525\n",
      "Epoch: 1, Loss: 2.8064794540405273\n",
      "Epoch: 1, Loss: 4.531527042388916\n",
      "Epoch: 1, Loss: 1.509803295135498\n",
      "Epoch: 1, Loss: 5.513210773468018\n",
      "Epoch: 1, Loss: 3.1197805404663086\n",
      "Epoch: 1, Loss: 1.5493292808532715\n",
      "Epoch: 1, Loss: 4.566486835479736\n",
      "Epoch: 1, Loss: 4.972057342529297\n",
      "Epoch: 1, Loss: 3.532139301300049\n",
      "Epoch: 1, Loss: 1.7268152236938477\n",
      "Epoch: 1, Loss: 2.437501907348633\n",
      "Epoch: 1, Loss: 2.1025924682617188\n",
      "Epoch: 1, Loss: 2.2357311248779297\n",
      "Epoch: 1, Loss: 2.391350269317627\n",
      "Epoch: 1, Loss: 2.9969077110290527\n",
      "Epoch: 1, Loss: 4.873516082763672\n",
      "Epoch: 1, Loss: 2.722539186477661\n",
      "Epoch: 1, Loss: 1.0594855546951294\n",
      "Epoch: 1, Loss: 1.545159935951233\n",
      "Epoch: 1, Loss: 3.093273878097534\n",
      "Epoch: 1, Loss: 5.807577610015869\n",
      "Epoch: 1, Loss: 1.5786375999450684\n",
      "Epoch: 1, Loss: 3.7599244117736816\n",
      "Epoch: 1, Loss: 1.3443762063980103\n",
      "Epoch: 1, Loss: 3.8915576934814453\n",
      "Epoch: 1, Loss: 5.193195819854736\n",
      "Epoch: 1, Loss: 2.3269808292388916\n",
      "Epoch: 1, Loss: 1.7420891523361206\n",
      "Epoch: 1, Loss: 1.7344974279403687\n",
      "Epoch: 1, Loss: 2.1854546070098877\n",
      "Epoch: 1, Loss: 1.322213888168335\n",
      "Epoch: 1, Loss: 1.775961995124817\n",
      "Epoch: 1, Loss: 1.5242249965667725\n",
      "Epoch: 1, Loss: 2.0057027339935303\n",
      "Epoch: 1, Loss: 2.638782501220703\n",
      "Epoch: 1, Loss: 3.1025443077087402\n",
      "Epoch: 1, Loss: 1.7256685495376587\n",
      "Epoch: 1, Loss: 2.8645195960998535\n",
      "Epoch: 1, Loss: 3.661015272140503\n",
      "Epoch: 1, Loss: 5.2410454750061035\n",
      "Epoch: 1, Loss: 2.956477403640747\n",
      "Epoch: 1, Loss: 2.6958632469177246\n",
      "Epoch: 1, Loss: 1.4340993165969849\n",
      "Epoch: 1, Loss: 4.241559028625488\n",
      "Epoch: 1, Loss: 3.844773530960083\n",
      "Epoch: 1, Loss: 2.522976875305176\n",
      "Epoch: 1, Loss: 1.1987422704696655\n",
      "Epoch: 1, Loss: 1.3675258159637451\n",
      "Epoch: 1, Loss: 1.8293852806091309\n",
      "Epoch: 1, Loss: 1.8894439935684204\n",
      "Epoch: 1, Loss: 2.6247098445892334\n",
      "Epoch: 1, Loss: 2.0821573734283447\n",
      "Epoch: 1, Loss: 1.58078932762146\n",
      "Epoch: 1, Loss: 2.154181480407715\n",
      "Epoch: 1, Loss: 1.632196068763733\n",
      "Epoch: 1, Loss: 4.943607807159424\n",
      "Epoch: 1, Loss: 7.759664058685303\n",
      "Epoch: 1, Loss: 2.615499258041382\n",
      "Epoch: 1, Loss: 1.6670143604278564\n",
      "Epoch: 1, Loss: 1.3927980661392212\n",
      "Epoch: 1, Loss: 1.026158094406128\n",
      "Epoch: 1, Loss: 2.5708320140838623\n",
      "Epoch: 1, Loss: 1.2421667575836182\n",
      "Epoch: 1, Loss: 6.204161643981934\n",
      "Epoch: 1, Loss: 2.4979441165924072\n",
      "Epoch: 1, Loss: 2.656446695327759\n",
      "Epoch: 1, Loss: 2.798976182937622\n",
      "Epoch: 1, Loss: 1.9884250164031982\n",
      "Epoch: 1, Loss: 2.16361927986145\n",
      "Epoch: 1, Loss: 3.3585433959960938\n",
      "Epoch: 1, Loss: 3.2631616592407227\n",
      "Epoch: 1, Loss: 1.372653603553772\n",
      "Epoch: 1, Loss: 3.4356672763824463\n",
      "Epoch: 1, Loss: 2.7439217567443848\n",
      "Epoch: 1, Loss: 1.46973717212677\n",
      "Epoch: 1, Loss: 2.682027816772461\n",
      "Epoch: 1, Loss: 4.1059346199035645\n",
      "Epoch: 1, Loss: 1.806627631187439\n",
      "Epoch: 1, Loss: 3.1363377571105957\n",
      "Epoch: 1, Loss: 2.0353338718414307\n",
      "Epoch: 1, Loss: 3.6830594539642334\n",
      "Epoch: 1, Loss: 2.464571714401245\n",
      "Epoch: 1, Loss: 1.5848631858825684\n",
      "Epoch: 1, Loss: 6.127293586730957\n",
      "Epoch: 1, Loss: 1.4817430973052979\n",
      "Epoch: 1, Loss: 1.2537412643432617\n",
      "Epoch: 1, Loss: 2.534058094024658\n",
      "Epoch: 1, Loss: 1.566827416419983\n",
      "Epoch: 1, Loss: 2.9075329303741455\n",
      "Epoch: 1, Loss: 3.1925551891326904\n",
      "Epoch: 1, Loss: 3.894455909729004\n",
      "Epoch: 1, Loss: 1.7781453132629395\n",
      "Epoch: 1, Loss: 2.4971537590026855\n",
      "Epoch: 1, Loss: 4.103992462158203\n",
      "Epoch: 1, Loss: 2.536201238632202\n",
      "Epoch: 1, Loss: 2.5949106216430664\n",
      "Epoch: 1, Loss: 4.591106414794922\n",
      "Epoch: 1, Loss: 3.0065486431121826\n",
      "Epoch: 1, Loss: 6.703011989593506\n",
      "Epoch: 1, Loss: 2.134579658508301\n",
      "Epoch: 1, Loss: 1.7775665521621704\n",
      "Epoch: 1, Loss: 2.8773365020751953\n",
      "Epoch: 1, Loss: 2.3512237071990967\n",
      "Epoch: 1, Loss: 1.404704213142395\n",
      "Epoch: 1, Loss: 2.3401060104370117\n",
      "Epoch: 1, Loss: 4.374639987945557\n",
      "Epoch: 1, Loss: 2.2951643466949463\n",
      "Epoch: 1, Loss: 3.7333481311798096\n",
      "Epoch: 1, Loss: 5.404734134674072\n",
      "Epoch: 1, Loss: 3.0816099643707275\n",
      "Epoch: 1, Loss: 1.175647497177124\n",
      "Epoch: 1, Loss: 4.7968831062316895\n",
      "Epoch: 1, Loss: 5.160275459289551\n",
      "Epoch: 1, Loss: 5.097880840301514\n",
      "Epoch: 1, Loss: 1.9185187816619873\n",
      "Epoch: 1, Loss: 1.9876612424850464\n",
      "Epoch: 1, Loss: 4.9045796394348145\n",
      "Epoch: 1, Loss: 3.3215372562408447\n",
      "Epoch: 1, Loss: 2.094362258911133\n",
      "Epoch: 1, Loss: 4.254560947418213\n",
      "Epoch: 1, Loss: 1.6294795274734497\n",
      "Epoch: 1, Loss: 2.0830624103546143\n",
      "Epoch: 1, Loss: 1.844640851020813\n",
      "Epoch: 1, Loss: 3.043524980545044\n",
      "Epoch: 1, Loss: 1.7846134901046753\n",
      "Epoch: 1, Loss: 5.34268856048584\n",
      "Epoch: 1, Loss: 2.6127989292144775\n",
      "Epoch: 1, Loss: 2.472844362258911\n",
      "Epoch: 1, Loss: 2.8259050846099854\n",
      "Epoch: 1, Loss: 4.069178104400635\n",
      "Epoch: 1, Loss: 2.1635563373565674\n",
      "Epoch: 1, Loss: 5.390063762664795\n",
      "Epoch: 2, Loss: 8.005233764648438\n",
      "Epoch: 2, Loss: 2.010674238204956\n",
      "Epoch: 2, Loss: 2.1317849159240723\n",
      "Epoch: 2, Loss: 2.320910692214966\n",
      "Epoch: 2, Loss: 1.937603235244751\n",
      "Epoch: 2, Loss: 1.7795443534851074\n",
      "Epoch: 2, Loss: 2.9257168769836426\n",
      "Epoch: 2, Loss: 1.8327537775039673\n",
      "Epoch: 2, Loss: 3.6767289638519287\n",
      "Epoch: 2, Loss: 4.645353317260742\n",
      "Epoch: 2, Loss: 3.9596493244171143\n",
      "Epoch: 2, Loss: 2.1944377422332764\n",
      "Epoch: 2, Loss: 2.1791505813598633\n",
      "Epoch: 2, Loss: 3.286963939666748\n",
      "Epoch: 2, Loss: 2.0101842880249023\n",
      "Epoch: 2, Loss: 2.6228811740875244\n",
      "Epoch: 2, Loss: 4.154613018035889\n",
      "Epoch: 2, Loss: 1.8819953203201294\n",
      "Epoch: 2, Loss: 5.17867374420166\n",
      "Epoch: 2, Loss: 2.9768781661987305\n",
      "Epoch: 2, Loss: 1.7161502838134766\n",
      "Epoch: 2, Loss: 2.534677505493164\n",
      "Epoch: 2, Loss: 2.1454403400421143\n",
      "Epoch: 2, Loss: 3.784099578857422\n",
      "Epoch: 2, Loss: 1.4315533638000488\n",
      "Epoch: 2, Loss: 1.962379813194275\n",
      "Epoch: 2, Loss: 2.4446029663085938\n",
      "Epoch: 2, Loss: 4.784811496734619\n",
      "Epoch: 2, Loss: 2.3443188667297363\n",
      "Epoch: 2, Loss: 2.062908887863159\n",
      "Epoch: 2, Loss: 5.010612487792969\n",
      "Epoch: 2, Loss: 1.6565388441085815\n",
      "Epoch: 2, Loss: 2.116180181503296\n",
      "Epoch: 2, Loss: 2.226486921310425\n",
      "Epoch: 2, Loss: 2.273305892944336\n",
      "Epoch: 2, Loss: 2.0384979248046875\n",
      "Epoch: 2, Loss: 2.326883316040039\n",
      "Epoch: 2, Loss: 1.5587481260299683\n",
      "Epoch: 2, Loss: 1.7641594409942627\n",
      "Epoch: 2, Loss: 2.703028917312622\n",
      "Epoch: 2, Loss: 6.0692830085754395\n",
      "Epoch: 2, Loss: 1.725871205329895\n",
      "Epoch: 2, Loss: 3.7089426517486572\n",
      "Epoch: 2, Loss: 3.477064371109009\n",
      "Epoch: 2, Loss: 5.315649509429932\n",
      "Epoch: 2, Loss: 3.0971620082855225\n",
      "Epoch: 2, Loss: 2.0275161266326904\n",
      "Epoch: 2, Loss: 4.995085716247559\n",
      "Epoch: 2, Loss: 2.136086940765381\n",
      "Epoch: 2, Loss: 2.9289278984069824\n",
      "Epoch: 2, Loss: 4.682341575622559\n",
      "Epoch: 2, Loss: 3.621091604232788\n",
      "Epoch: 2, Loss: 5.2764892578125\n",
      "Epoch: 2, Loss: 1.9913920164108276\n",
      "Epoch: 2, Loss: 2.1884584426879883\n",
      "Epoch: 2, Loss: 2.264946460723877\n",
      "Epoch: 2, Loss: 2.405794382095337\n",
      "Epoch: 2, Loss: 2.2004053592681885\n",
      "Epoch: 2, Loss: 1.8724451065063477\n",
      "Epoch: 2, Loss: 2.7364511489868164\n",
      "Epoch: 2, Loss: 1.888954758644104\n",
      "Epoch: 2, Loss: 1.5804126262664795\n",
      "Epoch: 2, Loss: 1.9394360780715942\n",
      "Epoch: 2, Loss: 1.259710669517517\n",
      "Epoch: 2, Loss: 2.3489584922790527\n",
      "Epoch: 2, Loss: 2.5064749717712402\n",
      "Epoch: 2, Loss: 3.5590803623199463\n",
      "Epoch: 2, Loss: 2.744852066040039\n",
      "Epoch: 2, Loss: 1.6622062921524048\n",
      "Epoch: 2, Loss: 4.5647382736206055\n",
      "Epoch: 2, Loss: 3.059277296066284\n",
      "Epoch: 2, Loss: 5.071394920349121\n",
      "Epoch: 2, Loss: 3.0330824851989746\n",
      "Epoch: 2, Loss: 2.3517675399780273\n",
      "Epoch: 2, Loss: 2.471437692642212\n",
      "Epoch: 2, Loss: 5.2313151359558105\n",
      "Epoch: 2, Loss: 4.504977226257324\n",
      "Epoch: 2, Loss: 1.3205100297927856\n",
      "Epoch: 2, Loss: 4.939044952392578\n",
      "Epoch: 2, Loss: 3.3405747413635254\n",
      "Epoch: 2, Loss: 2.305082321166992\n",
      "Epoch: 2, Loss: 2.6598880290985107\n",
      "Epoch: 2, Loss: 2.3190808296203613\n",
      "Epoch: 2, Loss: 3.4447200298309326\n",
      "Epoch: 2, Loss: 3.5597290992736816\n",
      "Epoch: 2, Loss: 2.951310634613037\n",
      "Epoch: 2, Loss: 2.323497772216797\n",
      "Epoch: 2, Loss: 3.825498580932617\n",
      "Epoch: 2, Loss: 5.014676570892334\n",
      "Epoch: 2, Loss: 1.8064922094345093\n",
      "Epoch: 2, Loss: 4.802793502807617\n",
      "Epoch: 2, Loss: 3.4652059078216553\n",
      "Epoch: 2, Loss: 2.4173336029052734\n",
      "Epoch: 2, Loss: 4.358922481536865\n",
      "Epoch: 2, Loss: 3.347550630569458\n",
      "Epoch: 2, Loss: 2.653756618499756\n",
      "Epoch: 2, Loss: 1.2238879203796387\n",
      "Epoch: 2, Loss: 3.0582590103149414\n",
      "Epoch: 2, Loss: 4.284338474273682\n",
      "Epoch: 2, Loss: 1.5420657396316528\n",
      "Epoch: 2, Loss: 4.501481056213379\n",
      "Epoch: 2, Loss: 2.4371633529663086\n",
      "Epoch: 2, Loss: 2.3000800609588623\n",
      "Epoch: 2, Loss: 1.7927354574203491\n",
      "Epoch: 2, Loss: 2.7488348484039307\n",
      "Epoch: 2, Loss: 5.739478588104248\n",
      "Epoch: 2, Loss: 2.0126113891601562\n",
      "Epoch: 2, Loss: 4.537757396697998\n",
      "Epoch: 2, Loss: 5.087026596069336\n",
      "Epoch: 2, Loss: 3.6033356189727783\n",
      "Epoch: 2, Loss: 1.8531476259231567\n",
      "Epoch: 2, Loss: 3.320810317993164\n",
      "Epoch: 2, Loss: 6.670057773590088\n",
      "Epoch: 2, Loss: 4.31901741027832\n",
      "Epoch: 2, Loss: 1.9008362293243408\n",
      "Epoch: 2, Loss: 1.6477389335632324\n",
      "Epoch: 2, Loss: 3.597358226776123\n",
      "Epoch: 2, Loss: 5.56658411026001\n",
      "Epoch: 2, Loss: 1.6390732526779175\n",
      "Epoch: 2, Loss: 1.9115902185440063\n",
      "Epoch: 2, Loss: 5.140975475311279\n",
      "Epoch: 2, Loss: 2.360527992248535\n",
      "Epoch: 2, Loss: 4.996500492095947\n",
      "Epoch: 2, Loss: 2.1769983768463135\n",
      "Epoch: 2, Loss: 2.989736318588257\n",
      "Epoch: 2, Loss: 1.4679816961288452\n",
      "Epoch: 2, Loss: 3.011171579360962\n",
      "Epoch: 2, Loss: 2.0811731815338135\n",
      "Epoch: 2, Loss: 2.2008087635040283\n",
      "Epoch: 2, Loss: 1.9662145376205444\n",
      "Epoch: 2, Loss: 3.1401751041412354\n",
      "Epoch: 2, Loss: 2.038729667663574\n",
      "Epoch: 2, Loss: 0.9408038854598999\n",
      "Epoch: 2, Loss: 1.5189446210861206\n",
      "Epoch: 2, Loss: 3.189120054244995\n",
      "Epoch: 2, Loss: 5.784589767456055\n",
      "Epoch: 2, Loss: 1.2615503072738647\n",
      "Epoch: 2, Loss: 2.414402961730957\n",
      "Epoch: 2, Loss: 5.528097152709961\n",
      "Epoch: 2, Loss: 2.2907214164733887\n",
      "Epoch: 2, Loss: 1.9245342016220093\n",
      "Epoch: 2, Loss: 1.7638672590255737\n",
      "Epoch: 2, Loss: 5.757224082946777\n",
      "Epoch: 2, Loss: 1.179095983505249\n",
      "Epoch: 2, Loss: 1.1450039148330688\n",
      "Epoch: 2, Loss: 1.1958690881729126\n",
      "Epoch: 2, Loss: 2.115009307861328\n",
      "Epoch: 2, Loss: 1.0151429176330566\n",
      "Epoch: 2, Loss: 1.5079708099365234\n",
      "Epoch: 2, Loss: 2.5957772731781006\n",
      "Epoch: 2, Loss: 3.663182258605957\n",
      "Epoch: 2, Loss: 3.4276280403137207\n",
      "Epoch: 2, Loss: 1.6150603294372559\n",
      "Epoch: 2, Loss: 1.52354896068573\n",
      "Epoch: 2, Loss: 1.9960955381393433\n",
      "Epoch: 2, Loss: 1.9296122789382935\n",
      "Epoch: 2, Loss: 3.2203903198242188\n",
      "Epoch: 2, Loss: 2.0690455436706543\n",
      "Epoch: 2, Loss: 2.626206159591675\n",
      "Epoch: 2, Loss: 6.277908802032471\n",
      "Epoch: 2, Loss: 1.3546247482299805\n",
      "Epoch: 2, Loss: 4.097936153411865\n",
      "Epoch: 2, Loss: 4.853647232055664\n",
      "Epoch: 2, Loss: 1.5802181959152222\n",
      "Epoch: 2, Loss: 7.017104625701904\n",
      "Epoch: 2, Loss: 4.403710842132568\n",
      "Epoch: 2, Loss: 3.581007242202759\n",
      "Epoch: 2, Loss: 1.6094621419906616\n",
      "Epoch: 2, Loss: 4.838425636291504\n",
      "Epoch: 2, Loss: 2.5986857414245605\n",
      "Epoch: 2, Loss: 2.3578827381134033\n",
      "Epoch: 2, Loss: 4.346756458282471\n",
      "Epoch: 2, Loss: 4.982115745544434\n",
      "Epoch: 2, Loss: 2.3374478816986084\n",
      "Epoch: 2, Loss: 1.9027024507522583\n",
      "Epoch: 2, Loss: 3.576338052749634\n",
      "Epoch: 2, Loss: 1.474874496459961\n",
      "Epoch: 2, Loss: 2.5481691360473633\n",
      "Epoch: 2, Loss: 1.853885531425476\n",
      "Epoch: 2, Loss: 1.6517012119293213\n",
      "Epoch: 2, Loss: 4.395662784576416\n",
      "Epoch: 2, Loss: 3.8171894550323486\n",
      "Epoch: 2, Loss: 4.348950386047363\n",
      "Epoch: 2, Loss: 2.469024181365967\n",
      "Epoch: 2, Loss: 5.650718688964844\n",
      "Epoch: 2, Loss: 5.032616138458252\n",
      "Epoch: 2, Loss: 2.2748513221740723\n",
      "Epoch: 2, Loss: 3.066077947616577\n",
      "Epoch: 2, Loss: 4.493621349334717\n",
      "Epoch: 2, Loss: 1.9699790477752686\n",
      "Epoch: 2, Loss: 1.8284363746643066\n",
      "Epoch: 2, Loss: 1.830582857131958\n",
      "Epoch: 2, Loss: 3.1740047931671143\n",
      "Epoch: 2, Loss: 1.3661935329437256\n",
      "Epoch: 2, Loss: 3.570444107055664\n",
      "Epoch: 2, Loss: 3.5983803272247314\n",
      "Epoch: 2, Loss: 2.483503818511963\n",
      "Epoch: 2, Loss: 5.101159572601318\n",
      "Epoch: 2, Loss: 2.855665683746338\n",
      "Epoch: 2, Loss: 1.6242969036102295\n",
      "Epoch: 2, Loss: 1.9800089597702026\n",
      "Epoch: 2, Loss: 4.6725640296936035\n",
      "Epoch: 2, Loss: 1.6944082975387573\n",
      "Epoch: 2, Loss: 4.375910758972168\n",
      "Epoch: 2, Loss: 1.8219218254089355\n",
      "Epoch: 2, Loss: 2.2629756927490234\n",
      "Epoch: 2, Loss: 2.5152204036712646\n",
      "Epoch: 2, Loss: 5.544346809387207\n",
      "Epoch: 2, Loss: 4.898958683013916\n",
      "Epoch: 2, Loss: 2.8422234058380127\n",
      "Epoch: 2, Loss: 1.4571290016174316\n",
      "Epoch: 2, Loss: 4.301691055297852\n",
      "Epoch: 2, Loss: 2.222222328186035\n",
      "Epoch: 2, Loss: 2.9777045249938965\n",
      "Epoch: 2, Loss: 4.379467487335205\n",
      "Epoch: 2, Loss: 1.6369131803512573\n",
      "Epoch: 2, Loss: 3.7851693630218506\n",
      "Epoch: 2, Loss: 5.1211018562316895\n",
      "Epoch: 2, Loss: 3.6041018962860107\n",
      "Epoch: 2, Loss: 2.9834699630737305\n",
      "Epoch: 2, Loss: 1.3505182266235352\n",
      "Epoch: 2, Loss: 3.8340566158294678\n",
      "Epoch: 2, Loss: 1.5818238258361816\n",
      "Epoch: 2, Loss: 4.531922817230225\n",
      "Epoch: 2, Loss: 3.6153550148010254\n",
      "Epoch: 2, Loss: 1.2943098545074463\n",
      "Epoch: 2, Loss: 7.208332538604736\n",
      "Epoch: 2, Loss: 5.402210712432861\n",
      "Epoch: 2, Loss: 2.546013593673706\n",
      "Epoch: 2, Loss: 2.0119264125823975\n",
      "Epoch: 2, Loss: 2.267688035964966\n",
      "Epoch: 2, Loss: 1.5798699855804443\n",
      "Epoch: 2, Loss: 2.1438310146331787\n",
      "Epoch: 2, Loss: 1.4235358238220215\n",
      "Epoch: 2, Loss: 1.6360567808151245\n",
      "Epoch: 2, Loss: 4.806329250335693\n",
      "Epoch: 2, Loss: 2.1043124198913574\n",
      "Epoch: 2, Loss: 4.099043846130371\n",
      "Epoch: 2, Loss: 0.9410804510116577\n",
      "Epoch: 2, Loss: 5.6233696937561035\n",
      "Epoch: 2, Loss: 1.8576087951660156\n",
      "Epoch: 2, Loss: 3.8142309188842773\n",
      "Epoch: 2, Loss: 1.8031302690505981\n",
      "Epoch: 2, Loss: 3.205059289932251\n",
      "Epoch: 2, Loss: 1.7769436836242676\n",
      "Epoch: 2, Loss: 1.3713637590408325\n",
      "Epoch: 2, Loss: 0.9670535922050476\n",
      "Epoch: 2, Loss: 2.8008763790130615\n",
      "Epoch: 2, Loss: 2.9183578491210938\n",
      "Epoch: 2, Loss: 2.712078809738159\n",
      "Epoch: 2, Loss: 2.295929431915283\n",
      "Epoch: 2, Loss: 2.917320728302002\n",
      "Epoch: 2, Loss: 1.7457486391067505\n",
      "Epoch: 2, Loss: 2.63781476020813\n",
      "Epoch: 2, Loss: 3.863187074661255\n",
      "Epoch: 2, Loss: 4.144169807434082\n",
      "Epoch: 2, Loss: 4.075240612030029\n",
      "Epoch: 2, Loss: 1.7209874391555786\n",
      "Epoch: 2, Loss: 2.01924729347229\n",
      "Epoch: 2, Loss: 1.8216692209243774\n",
      "Epoch: 2, Loss: 2.2543106079101562\n",
      "Epoch: 2, Loss: 3.4401023387908936\n",
      "Epoch: 2, Loss: 1.5826128721237183\n",
      "Epoch: 2, Loss: 3.4187254905700684\n",
      "Epoch: 2, Loss: 2.5601656436920166\n",
      "Epoch: 2, Loss: 2.293354034423828\n",
      "Epoch: 2, Loss: 4.433696269989014\n",
      "Epoch: 2, Loss: 4.018931865692139\n",
      "Epoch: 2, Loss: 2.411406993865967\n",
      "Epoch: 2, Loss: 2.430664300918579\n",
      "Epoch: 2, Loss: 2.761198043823242\n",
      "Epoch: 2, Loss: 3.340587854385376\n",
      "Epoch: 2, Loss: 1.6575508117675781\n",
      "Epoch: 2, Loss: 3.189824342727661\n",
      "Epoch: 2, Loss: 2.9200847148895264\n",
      "Epoch: 2, Loss: 3.4237899780273438\n",
      "Epoch: 2, Loss: 1.6257445812225342\n",
      "Epoch: 2, Loss: 3.2839090824127197\n",
      "Epoch: 2, Loss: 2.282576560974121\n",
      "Epoch: 2, Loss: 2.353489875793457\n",
      "Epoch: 2, Loss: 4.1407880783081055\n",
      "Epoch: 2, Loss: 2.6147420406341553\n",
      "Epoch: 2, Loss: 3.524524688720703\n",
      "Epoch: 2, Loss: 3.1052048206329346\n",
      "Epoch: 2, Loss: 2.679541826248169\n",
      "Epoch: 2, Loss: 2.1689374446868896\n",
      "Epoch: 2, Loss: 3.9635541439056396\n",
      "Epoch: 2, Loss: 2.735142469406128\n",
      "Epoch: 2, Loss: 1.8591567277908325\n",
      "Epoch: 2, Loss: 2.7024316787719727\n",
      "Epoch: 2, Loss: 6.306401252746582\n",
      "Epoch: 2, Loss: 2.722273349761963\n",
      "Epoch: 2, Loss: 4.857717037200928\n",
      "Epoch: 2, Loss: 2.0688109397888184\n",
      "Epoch: 2, Loss: 5.46809720993042\n",
      "Epoch: 2, Loss: 2.359295129776001\n",
      "Epoch: 2, Loss: 2.053257703781128\n",
      "Epoch: 2, Loss: 1.3722485303878784\n",
      "Epoch: 2, Loss: 5.93862247467041\n",
      "Epoch: 2, Loss: 2.8756844997406006\n",
      "Epoch: 2, Loss: 6.414844036102295\n",
      "Epoch: 2, Loss: 1.9223604202270508\n",
      "Epoch: 2, Loss: 1.841659665107727\n",
      "Epoch: 2, Loss: 2.715862512588501\n",
      "Epoch: 2, Loss: 1.7308778762817383\n",
      "Epoch: 2, Loss: 2.5717270374298096\n",
      "Epoch: 2, Loss: 4.183212757110596\n",
      "Epoch: 2, Loss: 2.7758259773254395\n",
      "Epoch: 2, Loss: 2.0177435874938965\n",
      "Epoch: 2, Loss: 2.136012554168701\n",
      "Epoch: 2, Loss: 3.9484074115753174\n",
      "Epoch: 2, Loss: 3.215935707092285\n",
      "Epoch: 2, Loss: 3.0063772201538086\n",
      "Epoch: 2, Loss: 2.19665789604187\n",
      "Epoch: 2, Loss: 1.8809974193572998\n",
      "Epoch: 2, Loss: 3.9560129642486572\n",
      "Epoch: 2, Loss: 1.7481095790863037\n",
      "Epoch: 2, Loss: 2.171326160430908\n",
      "Epoch: 2, Loss: 2.28762149810791\n",
      "Epoch: 2, Loss: 1.2506237030029297\n",
      "Epoch: 2, Loss: 2.0953590869903564\n",
      "Epoch: 2, Loss: 5.0478949546813965\n",
      "Epoch: 2, Loss: 2.9788668155670166\n",
      "Epoch: 2, Loss: 4.890354156494141\n",
      "Epoch: 2, Loss: 2.1554298400878906\n",
      "Epoch: 2, Loss: 2.7881760597229004\n",
      "Epoch: 2, Loss: 3.1610958576202393\n",
      "Epoch: 2, Loss: 3.60345458984375\n",
      "Epoch: 2, Loss: 2.342190742492676\n",
      "Epoch: 2, Loss: 2.921417474746704\n",
      "Epoch: 2, Loss: 2.2066338062286377\n",
      "Epoch: 2, Loss: 2.7854881286621094\n",
      "Epoch: 2, Loss: 3.7785251140594482\n",
      "Epoch: 2, Loss: 2.9226951599121094\n",
      "Epoch: 2, Loss: 4.48626184463501\n",
      "Epoch: 2, Loss: 1.9272809028625488\n",
      "Epoch: 2, Loss: 2.321139097213745\n",
      "Epoch: 2, Loss: 1.5353275537490845\n",
      "Epoch: 2, Loss: 2.7465755939483643\n",
      "Epoch: 2, Loss: 2.7754011154174805\n",
      "Epoch: 2, Loss: 2.5597994327545166\n",
      "Epoch: 2, Loss: 1.760075569152832\n",
      "Epoch: 2, Loss: 6.442768573760986\n",
      "Epoch: 2, Loss: 1.6356151103973389\n",
      "Epoch: 2, Loss: 2.7780416011810303\n",
      "Epoch: 2, Loss: 4.348250865936279\n",
      "Epoch: 2, Loss: 3.888936996459961\n",
      "Epoch: 2, Loss: 4.029116630554199\n",
      "Epoch: 2, Loss: 4.011478900909424\n",
      "Epoch: 2, Loss: 1.5103403329849243\n",
      "Epoch: 2, Loss: 3.385390281677246\n",
      "Epoch: 2, Loss: 5.198530197143555\n",
      "Epoch: 2, Loss: 2.8436903953552246\n",
      "Epoch: 2, Loss: 4.4997453689575195\n",
      "Epoch: 2, Loss: 2.1914703845977783\n",
      "Epoch: 2, Loss: 2.0954418182373047\n",
      "Epoch: 2, Loss: 2.1345057487487793\n",
      "Epoch: 2, Loss: 2.259615182876587\n",
      "Epoch: 2, Loss: 1.7678496837615967\n",
      "Epoch: 2, Loss: 1.7646375894546509\n",
      "Epoch: 2, Loss: 5.763948440551758\n",
      "Epoch: 2, Loss: 2.326111316680908\n",
      "Epoch: 2, Loss: 2.638780117034912\n",
      "Epoch: 2, Loss: 1.6775903701782227\n",
      "Epoch: 2, Loss: 2.849966049194336\n",
      "Epoch: 2, Loss: 2.4189770221710205\n",
      "Epoch: 2, Loss: 2.048029661178589\n",
      "Epoch: 2, Loss: 3.7332401275634766\n",
      "Epoch: 2, Loss: 1.6620546579360962\n",
      "Epoch: 2, Loss: 2.895199775695801\n",
      "Epoch: 2, Loss: 4.257142543792725\n",
      "Epoch: 2, Loss: 1.7084007263183594\n",
      "Epoch: 2, Loss: 2.3054556846618652\n",
      "Epoch: 2, Loss: 1.8852126598358154\n",
      "Epoch: 2, Loss: 4.18107795715332\n",
      "Epoch: 2, Loss: 2.6012210845947266\n",
      "Epoch: 2, Loss: 4.646727085113525\n",
      "Epoch: 2, Loss: 1.2987257242202759\n",
      "Epoch: 2, Loss: 4.412621974945068\n",
      "Epoch: 2, Loss: 1.5749036073684692\n",
      "Epoch: 2, Loss: 2.3481178283691406\n",
      "Epoch: 2, Loss: 1.972204327583313\n",
      "Epoch: 2, Loss: 4.02572774887085\n",
      "Epoch: 2, Loss: 2.79158353805542\n",
      "Epoch: 2, Loss: 2.76728892326355\n",
      "Epoch: 2, Loss: 4.133619785308838\n",
      "Epoch: 2, Loss: 2.0637729167938232\n",
      "Epoch: 2, Loss: 2.745227813720703\n",
      "Epoch: 2, Loss: 2.485565185546875\n",
      "Epoch: 2, Loss: 2.2858521938323975\n",
      "Epoch: 2, Loss: 2.2657861709594727\n",
      "Epoch: 2, Loss: 3.722938060760498\n",
      "Epoch: 2, Loss: 2.7713096141815186\n",
      "Epoch: 2, Loss: 1.8495856523513794\n",
      "Epoch: 2, Loss: 5.132845401763916\n",
      "Epoch: 2, Loss: 2.7233049869537354\n",
      "Epoch: 2, Loss: 4.058428764343262\n",
      "Epoch: 2, Loss: 2.323845386505127\n",
      "Epoch: 2, Loss: 2.687993288040161\n",
      "Epoch: 2, Loss: 1.7440024614334106\n",
      "Epoch: 2, Loss: 4.04758882522583\n",
      "Epoch: 2, Loss: 4.24389123916626\n",
      "Epoch: 2, Loss: 0.9784538149833679\n",
      "Epoch: 2, Loss: 3.007225513458252\n",
      "Epoch: 2, Loss: 2.2963714599609375\n",
      "Epoch: 2, Loss: 2.831235647201538\n",
      "Epoch: 2, Loss: 3.4775984287261963\n",
      "Epoch: 2, Loss: 1.7068217992782593\n",
      "Epoch: 2, Loss: 3.0353405475616455\n",
      "Epoch: 2, Loss: 2.8155314922332764\n",
      "Epoch: 2, Loss: 1.2576249837875366\n",
      "Epoch: 2, Loss: 5.07199239730835\n",
      "Epoch: 2, Loss: 5.0144453048706055\n",
      "Epoch: 2, Loss: 2.2967092990875244\n",
      "Epoch: 2, Loss: 1.3885301351547241\n",
      "Epoch: 2, Loss: 2.1415934562683105\n",
      "Epoch: 2, Loss: 3.760718584060669\n",
      "Epoch: 2, Loss: 1.4932036399841309\n",
      "Epoch: 2, Loss: 5.403834342956543\n",
      "Epoch: 2, Loss: 5.794453144073486\n",
      "Epoch: 2, Loss: 1.9428752660751343\n",
      "Epoch: 2, Loss: 2.130491018295288\n",
      "Epoch: 2, Loss: 1.7392793893814087\n",
      "Epoch: 2, Loss: 1.8056584596633911\n",
      "Epoch: 2, Loss: 2.9164700508117676\n",
      "Epoch: 2, Loss: 1.8763234615325928\n",
      "Epoch: 2, Loss: 2.4568092823028564\n",
      "Epoch: 2, Loss: 3.0697834491729736\n",
      "Epoch: 2, Loss: 1.4226957559585571\n",
      "Epoch: 2, Loss: 4.215084552764893\n",
      "Epoch: 2, Loss: 2.608936071395874\n",
      "Epoch: 2, Loss: 3.532956123352051\n",
      "Epoch: 2, Loss: 4.922572135925293\n",
      "Epoch: 2, Loss: 1.3529781103134155\n",
      "Epoch: 2, Loss: 2.3871779441833496\n",
      "Epoch: 2, Loss: 3.0205554962158203\n",
      "Epoch: 2, Loss: 2.935940980911255\n",
      "Epoch: 2, Loss: 1.929825782775879\n",
      "Epoch: 2, Loss: 3.706254005432129\n",
      "Epoch: 2, Loss: 2.3477978706359863\n",
      "Epoch: 2, Loss: 1.9005166292190552\n",
      "Epoch: 2, Loss: 2.3061225414276123\n",
      "Epoch: 2, Loss: 1.2095967531204224\n",
      "Epoch: 2, Loss: 5.036795616149902\n",
      "Epoch: 2, Loss: 2.7333691120147705\n",
      "Epoch: 2, Loss: 4.678382873535156\n",
      "Epoch: 2, Loss: 4.916243553161621\n",
      "Epoch: 2, Loss: 3.0657222270965576\n",
      "Epoch: 2, Loss: 1.3306770324707031\n",
      "Epoch: 2, Loss: 1.837202787399292\n",
      "Epoch: 2, Loss: 2.6729419231414795\n",
      "Epoch: 2, Loss: 1.630903720855713\n",
      "Epoch: 2, Loss: 2.246948003768921\n",
      "Epoch: 2, Loss: 3.2520153522491455\n",
      "Epoch: 2, Loss: 4.26726770401001\n",
      "Epoch: 2, Loss: 3.370145559310913\n",
      "Epoch: 2, Loss: 1.9577103853225708\n",
      "Epoch: 2, Loss: 3.310964345932007\n",
      "Epoch: 2, Loss: 5.374399662017822\n",
      "Epoch: 2, Loss: 0.9867343306541443\n",
      "Epoch: 2, Loss: 2.595227003097534\n",
      "Epoch: 2, Loss: 3.9472880363464355\n",
      "Epoch: 2, Loss: 3.437525510787964\n",
      "Epoch: 2, Loss: 3.050590753555298\n",
      "Epoch: 2, Loss: 2.3117074966430664\n",
      "Epoch: 2, Loss: 1.7999520301818848\n",
      "Epoch: 2, Loss: 2.531505823135376\n",
      "Epoch: 2, Loss: 5.061453819274902\n",
      "Epoch: 2, Loss: 3.2592077255249023\n",
      "Epoch: 2, Loss: 3.388705015182495\n",
      "Epoch: 2, Loss: 1.8293942213058472\n",
      "Epoch: 2, Loss: 1.9521342515945435\n",
      "Epoch: 2, Loss: 6.12523078918457\n",
      "Epoch: 2, Loss: 3.0659773349761963\n",
      "Epoch: 2, Loss: 3.910130500793457\n",
      "Epoch: 2, Loss: 2.915010929107666\n",
      "Epoch: 2, Loss: 2.274336814880371\n",
      "Epoch: 2, Loss: 1.368943452835083\n",
      "Epoch: 2, Loss: 2.474858045578003\n",
      "Epoch: 2, Loss: 2.1811764240264893\n",
      "Epoch: 2, Loss: 2.405756711959839\n",
      "Epoch: 2, Loss: 2.436028242111206\n",
      "Epoch: 2, Loss: 4.792807102203369\n",
      "Epoch: 2, Loss: 1.0195776224136353\n",
      "Epoch: 2, Loss: 3.082679033279419\n",
      "Epoch: 2, Loss: 1.9815577268600464\n",
      "Epoch: 2, Loss: 2.023132085800171\n",
      "Epoch: 2, Loss: 1.7878259420394897\n",
      "Epoch: 2, Loss: 4.148381233215332\n",
      "Epoch: 2, Loss: 5.4791579246521\n",
      "Epoch: 2, Loss: 4.1184492111206055\n",
      "Epoch: 2, Loss: 2.168712615966797\n",
      "Epoch: 2, Loss: 1.9688823223114014\n",
      "Epoch: 2, Loss: 1.7291934490203857\n",
      "Epoch: 2, Loss: 2.2129714488983154\n",
      "Epoch: 2, Loss: 1.748091220855713\n",
      "Epoch: 2, Loss: 1.2916730642318726\n",
      "Epoch: 2, Loss: 1.8917813301086426\n",
      "Epoch: 2, Loss: 1.586650013923645\n",
      "Epoch: 2, Loss: 7.163149833679199\n",
      "Epoch: 2, Loss: 4.498142719268799\n",
      "Epoch: 2, Loss: 2.463754653930664\n",
      "Epoch: 2, Loss: 3.277194023132324\n",
      "Epoch: 2, Loss: 2.946686267852783\n",
      "Epoch: 2, Loss: 4.766951560974121\n",
      "Epoch: 2, Loss: 2.4055352210998535\n",
      "Epoch: 2, Loss: 4.345056533813477\n",
      "Epoch: 2, Loss: 3.05944561958313\n",
      "Epoch: 2, Loss: 1.6776549816131592\n",
      "Epoch: 2, Loss: 4.273924827575684\n",
      "Epoch: 2, Loss: 1.7083978652954102\n",
      "Epoch: 2, Loss: 1.7576388120651245\n",
      "Epoch: 2, Loss: 3.6160757541656494\n",
      "Epoch: 2, Loss: 2.4493653774261475\n",
      "Epoch: 2, Loss: 1.882442831993103\n",
      "Epoch: 2, Loss: 5.724147796630859\n",
      "Epoch: 2, Loss: 5.607277870178223\n",
      "Epoch: 2, Loss: 1.1531790494918823\n",
      "Epoch: 2, Loss: 6.539409160614014\n",
      "Epoch: 2, Loss: 3.859783172607422\n",
      "Epoch: 2, Loss: 3.6069953441619873\n",
      "Epoch: 2, Loss: 2.2515766620635986\n",
      "Epoch: 2, Loss: 1.4741590023040771\n",
      "Epoch: 2, Loss: 4.393235683441162\n",
      "Epoch: 2, Loss: 2.339963674545288\n",
      "Epoch: 2, Loss: 2.6345932483673096\n",
      "Epoch: 2, Loss: 1.8306585550308228\n",
      "Epoch: 2, Loss: 1.8748201131820679\n",
      "Epoch: 2, Loss: 1.438201904296875\n",
      "Epoch: 2, Loss: 5.764138221740723\n",
      "Epoch: 2, Loss: 3.8579394817352295\n",
      "Epoch: 2, Loss: 1.3262689113616943\n",
      "Epoch: 2, Loss: 1.3893992900848389\n",
      "Epoch: 2, Loss: 1.1132522821426392\n",
      "Epoch: 2, Loss: 3.2282726764678955\n",
      "Epoch: 2, Loss: 1.4625405073165894\n",
      "Epoch: 2, Loss: 2.668897867202759\n",
      "Epoch: 2, Loss: 1.9035309553146362\n",
      "Epoch: 2, Loss: 5.8555426597595215\n",
      "Epoch: 2, Loss: 1.4271024465560913\n",
      "Epoch: 2, Loss: 0.8022057414054871\n",
      "Epoch: 2, Loss: 5.868048667907715\n",
      "Epoch: 2, Loss: 1.0294861793518066\n",
      "Epoch: 2, Loss: 3.857043504714966\n",
      "Epoch: 2, Loss: 4.353208541870117\n",
      "Epoch: 2, Loss: 1.1158912181854248\n",
      "Epoch: 2, Loss: 1.942063331604004\n",
      "Epoch: 2, Loss: 1.6870946884155273\n",
      "Epoch: 2, Loss: 3.2780210971832275\n",
      "Epoch: 2, Loss: 1.5490596294403076\n",
      "Epoch: 2, Loss: 4.753962516784668\n",
      "Epoch: 2, Loss: 2.078234910964966\n",
      "Epoch: 2, Loss: 2.0401978492736816\n",
      "Epoch: 2, Loss: 4.068913459777832\n",
      "Epoch: 2, Loss: 7.661813259124756\n",
      "Epoch: 2, Loss: 1.9064230918884277\n",
      "Epoch: 2, Loss: 2.407921314239502\n",
      "Epoch: 2, Loss: 3.722399950027466\n",
      "Epoch: 2, Loss: 1.9034583568572998\n",
      "Epoch: 2, Loss: 2.6965670585632324\n",
      "Epoch: 2, Loss: 3.996802568435669\n",
      "Epoch: 2, Loss: 5.543422222137451\n",
      "Epoch: 2, Loss: 2.5332260131835938\n",
      "Epoch: 2, Loss: 2.4592206478118896\n",
      "Epoch: 2, Loss: 1.8739548921585083\n",
      "Epoch: 2, Loss: 2.190915822982788\n",
      "Epoch: 2, Loss: 4.841269493103027\n",
      "Epoch: 2, Loss: 4.003000736236572\n",
      "Epoch: 2, Loss: 3.3839385509490967\n",
      "Epoch: 2, Loss: 2.856391668319702\n",
      "Epoch: 2, Loss: 4.996105194091797\n",
      "Epoch: 2, Loss: 4.802281856536865\n",
      "Epoch: 2, Loss: 3.9050638675689697\n",
      "Epoch: 2, Loss: 2.1642727851867676\n",
      "Epoch: 2, Loss: 4.199941635131836\n",
      "Epoch: 2, Loss: 2.628411054611206\n",
      "Epoch: 2, Loss: 2.188917636871338\n",
      "Epoch: 2, Loss: 1.9944307804107666\n",
      "Epoch: 2, Loss: 2.0668768882751465\n",
      "Epoch: 2, Loss: 4.395964622497559\n",
      "Epoch: 2, Loss: 4.822507858276367\n",
      "Epoch: 2, Loss: 1.7881900072097778\n",
      "Epoch: 2, Loss: 1.5366255044937134\n",
      "Epoch: 2, Loss: 4.703629016876221\n",
      "Epoch: 2, Loss: 7.789017677307129\n",
      "Epoch: 2, Loss: 4.666762351989746\n",
      "Epoch: 2, Loss: 1.959877610206604\n",
      "Epoch: 2, Loss: 2.609903335571289\n",
      "Epoch: 2, Loss: 2.6165101528167725\n",
      "Epoch: 2, Loss: 3.292080879211426\n",
      "Epoch: 2, Loss: 1.9118503332138062\n",
      "Epoch: 2, Loss: 2.822854995727539\n",
      "Epoch: 2, Loss: 1.7375736236572266\n",
      "Epoch: 2, Loss: 2.289884567260742\n",
      "Epoch: 2, Loss: 1.6281994581222534\n",
      "Epoch: 2, Loss: 3.346684694290161\n",
      "Epoch: 2, Loss: 2.543790340423584\n",
      "Epoch: 2, Loss: 6.971121311187744\n",
      "Epoch: 2, Loss: 1.6064995527267456\n",
      "Epoch: 2, Loss: 4.211023330688477\n",
      "Epoch: 2, Loss: 3.517505407333374\n",
      "Epoch: 2, Loss: 3.736025333404541\n",
      "Epoch: 2, Loss: 3.786308526992798\n",
      "Epoch: 2, Loss: 5.398017406463623\n",
      "Epoch: 2, Loss: 2.2497332096099854\n",
      "Epoch: 2, Loss: 3.4324686527252197\n",
      "Epoch: 2, Loss: 3.21712589263916\n",
      "Epoch: 2, Loss: 3.582213878631592\n",
      "Epoch: 2, Loss: 1.7198783159255981\n",
      "Epoch: 2, Loss: 1.8092801570892334\n",
      "Epoch: 2, Loss: 4.554924964904785\n",
      "Epoch: 2, Loss: 2.9638023376464844\n",
      "Epoch: 2, Loss: 2.6837167739868164\n",
      "Epoch: 2, Loss: 4.673244953155518\n",
      "Epoch: 2, Loss: 5.354803562164307\n",
      "Epoch: 2, Loss: 2.373215913772583\n",
      "Epoch: 2, Loss: 3.0505218505859375\n",
      "Epoch: 2, Loss: 2.3552143573760986\n",
      "Epoch: 2, Loss: 5.817989349365234\n",
      "Epoch: 2, Loss: 1.9862444400787354\n",
      "Epoch: 2, Loss: 2.6709790229797363\n",
      "Epoch: 2, Loss: 2.847877264022827\n",
      "Epoch: 2, Loss: 2.068840503692627\n",
      "Epoch: 2, Loss: 2.433934211730957\n",
      "Epoch: 2, Loss: 3.3283212184906006\n",
      "Epoch: 2, Loss: 4.555156230926514\n",
      "Epoch: 2, Loss: 1.702538251876831\n",
      "Epoch: 2, Loss: 3.371729850769043\n",
      "Epoch: 2, Loss: 4.445065975189209\n",
      "Epoch: 2, Loss: 4.653456211090088\n",
      "Epoch: 2, Loss: 1.3106003999710083\n",
      "Epoch: 2, Loss: 1.4819345474243164\n",
      "Epoch: 2, Loss: 3.6098711490631104\n",
      "Epoch: 2, Loss: 1.8650676012039185\n",
      "Epoch: 2, Loss: 2.6927242279052734\n",
      "Epoch: 2, Loss: 2.9882054328918457\n",
      "Epoch: 2, Loss: 3.262345314025879\n",
      "Epoch: 2, Loss: 3.9311444759368896\n",
      "Epoch: 2, Loss: 1.7798428535461426\n",
      "Epoch: 2, Loss: 2.0151796340942383\n",
      "Epoch: 2, Loss: 2.822134256362915\n",
      "Epoch: 2, Loss: 1.473968744277954\n",
      "Epoch: 2, Loss: 1.376409888267517\n",
      "Epoch: 2, Loss: 1.647322416305542\n",
      "Epoch: 2, Loss: 1.920186996459961\n",
      "Epoch: 2, Loss: 1.3768126964569092\n",
      "Epoch: 2, Loss: 1.2637898921966553\n",
      "Epoch: 2, Loss: 3.7029006481170654\n",
      "Epoch: 2, Loss: 2.2072038650512695\n",
      "Epoch: 2, Loss: 4.581214904785156\n",
      "Epoch: 2, Loss: 3.1593093872070312\n",
      "Epoch: 2, Loss: 1.8751280307769775\n",
      "Epoch: 2, Loss: 4.777805328369141\n",
      "Epoch: 2, Loss: 1.8773384094238281\n",
      "Epoch: 2, Loss: 1.643852949142456\n",
      "Epoch: 2, Loss: 6.205892562866211\n",
      "Epoch: 2, Loss: 2.14819598197937\n",
      "Epoch: 2, Loss: 4.231982231140137\n",
      "Epoch: 2, Loss: 2.6334118843078613\n",
      "Epoch: 2, Loss: 2.411940574645996\n",
      "Epoch: 2, Loss: 2.6049296855926514\n",
      "Epoch: 2, Loss: 2.9002034664154053\n",
      "Epoch: 2, Loss: 2.572075128555298\n",
      "Epoch: 2, Loss: 4.18497371673584\n",
      "Epoch: 2, Loss: 4.446759223937988\n",
      "Epoch: 2, Loss: 8.52634334564209\n",
      "Epoch: 2, Loss: 2.680910587310791\n",
      "Epoch: 2, Loss: 2.4258248805999756\n",
      "Epoch: 2, Loss: 2.7471487522125244\n",
      "Epoch: 2, Loss: 3.909245014190674\n",
      "Epoch: 2, Loss: 2.302623987197876\n",
      "Epoch: 2, Loss: 3.719102382659912\n",
      "Epoch: 2, Loss: 2.635556221008301\n",
      "Epoch: 2, Loss: 1.9212900400161743\n",
      "Epoch: 2, Loss: 3.3331143856048584\n",
      "Epoch: 2, Loss: 2.2950491905212402\n",
      "Epoch: 2, Loss: 1.9981045722961426\n",
      "Epoch: 2, Loss: 2.4524385929107666\n",
      "Epoch: 2, Loss: 4.771115303039551\n",
      "Epoch: 2, Loss: 1.953020453453064\n",
      "Epoch: 2, Loss: 1.5014138221740723\n",
      "Epoch: 2, Loss: 1.6451606750488281\n",
      "Epoch: 2, Loss: 1.6762681007385254\n",
      "Epoch: 2, Loss: 1.98171067237854\n",
      "Epoch: 2, Loss: 8.40556526184082\n",
      "Epoch: 2, Loss: 7.719991207122803\n",
      "Epoch: 2, Loss: 9.227256774902344\n",
      "Model and tokenizer saved in 'trained_models/02_consel_data_with_turning_model'\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer, AdamW\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "def train_and_save_model_with_turning(train_data_path, save_directory, model_name=\"gpt2\", max_length_inputs=558, max_length_outputs=558, batch_size=2, epochs=3, learning_rate=5e-5):\n",
    "    \"\"\"\n",
    "    Train a GPT-2 model using a given dataset and save the model and tokenizer.\n",
    "\n",
    "    Args:\n",
    "        train_data_path (str): Path to the training data file (CSV format).\n",
    "        save_directory (str): Directory to save the trained model and tokenizer.\n",
    "        model_name (str): Name of the pre-trained model to use. Defaults to 'gpt2'.\n",
    "        max_length_inputs (int): Maximum length for input texts. Defaults to 185.\n",
    "        max_length_outputs (int): Maximum length for target texts. Defaults to 558.\n",
    "        batch_size (int): Batch size for training. Defaults to 2.\n",
    "        epochs (int): Number of training epochs. Defaults to 3.\n",
    "        learning_rate (float): Learning rate for the optimizer. Defaults to 5e-5.\n",
    "    \"\"\"\n",
    "\n",
    "    # Load and preprocess data\n",
    "    train_df = pd.read_csv(train_data_path)\n",
    "    train_df[\"input_text\"] = \"Question: \" + train_df[\"questionText\"] + \" Diagnosis: \" + train_df[\"re_diagnosis\"]\n",
    "    train_df[\"target_text\"] = train_df[\"clean_answer_text\"]\n",
    "\n",
    "    # Initialize the model and tokenizer\n",
    "    tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "    model = GPT2LMHeadModel.from_pretrained(model_name)\n",
    "\n",
    "    # Set padding token for GPT-2\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    model.config.pad_token_id = tokenizer.pad_token_id\n",
    "\n",
    "    # Create a custom dataset class\n",
    "    class ChatBotDataSet(Dataset):\n",
    "        def __init__(self, tokenizer, input_texts, target_texts):\n",
    "            self.tokenizer = tokenizer\n",
    "            self.input_texts = input_texts\n",
    "            self.target_texts = target_texts\n",
    "\n",
    "        def __len__(self):\n",
    "            return len(self.input_texts)\n",
    "\n",
    "        def __getitem__(self, index):\n",
    "            # Encode input and target texts with truncation and padding\n",
    "            input_encodings = self.tokenizer(\n",
    "                self.input_texts[index],\n",
    "                max_length=max_length_inputs,\n",
    "                padding=\"max_length\",\n",
    "                truncation=True,\n",
    "                return_tensors=\"pt\"\n",
    "            )\n",
    "            target_encodings = self.tokenizer(\n",
    "                self.target_texts[index],\n",
    "                max_length=max_length_outputs,\n",
    "                padding=\"max_length\",\n",
    "                truncation=True,\n",
    "                return_tensors=\"pt\"\n",
    "            )\n",
    "\n",
    "            return {\n",
    "                \"input_ids\": input_encodings[\"input_ids\"].squeeze(),\n",
    "                \"attention_mask\": input_encodings[\"attention_mask\"].squeeze(),\n",
    "                \"labels\": target_encodings[\"input_ids\"].squeeze(),\n",
    "            }\n",
    "\n",
    "    # Prepare dataset and dataloader\n",
    "    input_texts = train_df[\"input_text\"].tolist()\n",
    "    target_texts = train_df[\"target_text\"].tolist()\n",
    "    dataset = ChatBotDataSet(tokenizer, input_texts, target_texts)\n",
    "    train_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    # Model training setup\n",
    "    optimizer = AdamW(model.parameters(), lr=learning_rate)\n",
    "    model.train()\n",
    "\n",
    "    # Training loop\n",
    "    for epoch in range(epochs):\n",
    "        for batch in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(\n",
    "                input_ids=batch[\"input_ids\"],\n",
    "                attention_mask=batch[\"attention_mask\"],\n",
    "                labels=batch[\"labels\"]\n",
    "            )\n",
    "            loss = outputs.loss\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            print(f\"Epoch: {epoch}, Loss: {loss.item()}\")\n",
    "\n",
    "    # Save the model and tokenizer\n",
    "    model.save_pretrained(save_directory)\n",
    "    tokenizer.save_pretrained(save_directory)\n",
    "    print(f\"Model and tokenizer saved in '{save_directory}'\")\n",
    "\n",
    "# Example usage\n",
    "cleaned_data_path = \"cleaned_data\"\n",
    "train_data_path = f\"{cleaned_data_path}/cleaned_predicted_consel_data.csv\"\n",
    "save_directory = \"trained_models/02_consel_data_with_turning_model\"\n",
    "# train_and_save_model_with_turning(train_data_path, save_directory)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'training_data.jsonl'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert to JSONL format with prompt and completion structure\n",
    "import json\n",
    "train_data_path = f\"{cleaned_data_path}/cleaned_predicted_consel_data.csv\"\n",
    "train_df = pd.read_csv(train_data_path)\n",
    "train_df[\"input_text\"] = \"Please respond politely and empathetically to this query: \" + train_df[\"questionText\"] + \" Diagnosis: \" + train_df[\"re_diagnosis\"]\n",
    "train_df[\"target_text\"] = train_df[\"clean_answer_text\"]\n",
    "\n",
    "\n",
    "jsonl_data = train_df.apply(lambda row: {\n",
    "    \"prompt\": row[\"input_text\"],\n",
    "    \"completion\": row[\"target_text\"]\n",
    "}, axis=1).tolist()\n",
    "\n",
    "# Save to a JSONL file\n",
    "output_jsonl_path = \"training_data.jsonl\"\n",
    "with open(output_jsonl_path, 'w') as outfile:\n",
    "    for entry in jsonl_data:\n",
    "        json.dump(entry, outfile)\n",
    "        outfile.write('\\n')\n",
    "\n",
    "output_jsonl_path  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total tokens: 421654\n"
     ]
    }
   ],
   "source": [
    "import tiktoken\n",
    "\n",
    "tokenizer = tiktoken.encoding_for_model(\"gpt-3.5-turbo\")\n",
    "with open('training_data.jsonl', 'r') as file:\n",
    "    total_tokens = 0\n",
    "    for line in file:\n",
    "        data = json.loads(line)\n",
    "        prompt = data.get(\"prompt\", \"\")\n",
    "        completion = data.get(\"completion\", \"\")\n",
    "        tokens = tokenizer.encode(prompt + completion)\n",
    "        total_tokens += len(tokens)\n",
    "\n",
    "print(f\"Total tokens: {total_tokens}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
