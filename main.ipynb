{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Mental Health Counsel Chatbot\n",
        "\n",
        "Kaggle Notebook: Mental Health Counsel Chatbot\n",
        "Description: Provides mental health counseling data, which we used to supplement information from the primary dataset and align topics for consistent categorization.\n",
        "Mental Health Synthetic Dataset\n",
        "\n",
        "Kaggle Dataset: Mental Health Synthetic Dataset\n",
        "Description: This primary dataset contains synthetic data on mental health symptoms, demographics, and treatment, forming the basis for model training and recommendation generation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m_BfAu_XPWub",
        "outputId": "693f18da-464b-47dc-8e40-b70491dae910"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Index(['User ID', 'Age', 'Gender', 'Symptoms', 'Duration (weeks)',\n",
              "       'Previous Diagnosis', 'Therapy History', 'Medication',\n",
              "       'Diagnosis / Condition', 'Suggested Therapy', 'Self-care Advice',\n",
              "       'Urgency Level', 'Mood', 'Stress Level'],\n",
              "      dtype='object')"
            ]
          },
          "execution_count": 64,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "mental_df = pd.read_csv(\"mental_health.csv\")\n",
        "mental_df.columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Index(['User ID', 'Age', 'Gender', 'Symptoms', 'Duration', 'Prev_Diagnosis',\n",
              "       'Therapy_History', 'Medication', 'Diagnosis', 'Suggested_Therapy',\n",
              "       'Self_Care_Advice', 'Urgency_Level', 'Mood', 'Stress_Level'],\n",
              "      dtype='object')"
            ]
          },
          "execution_count": 65,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Renmae columns: easy to follow up.\n",
        "mental_df = mental_df.rename(columns= {\n",
        "    'Diagnosis / Condition' : 'Diagnosis',\n",
        "    'Self-care Advice': 'Self_Care_Advice',\n",
        "    'Therapy History' : 'Therapy_History',\n",
        "    'Stress Level' : 'Stress_Level',\n",
        "    'Urgency Level' : 'Urgency_Level',\n",
        "    'Suggested Therapy': 'Suggested_Therapy',\n",
        "    'Duration (weeks)': 'Duration',\n",
        "    'Previous Diagnosis': 'Prev_Diagnosis'\n",
        "})\n",
        "\n",
        "# Check renamed columns\n",
        "mental_df.columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {
        "id": "1rl9DKaPRJ4-"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Symtoms unique values \n",
            " ['feeling anxious' 'excessive worry' 'trouble sleeping'\n",
            " 'loss of interest in activities' 'panic attacks' 'lack of concentration'\n",
            " 'feeling irritable' 'feeling sad' 'feeling overwhelmed']\n",
            "therapy_unique_values \n",
            " ['Support Groups' 'Cognitive Behavioral Therapy' 'Psychotherapy'\n",
            " 'Mindfulness-Based Therapy' 'No Therapy Needed']\n",
            "diagnosis_unique_values \n",
            " ['Panic Disorder' 'Depression' 'Anxiety' 'Burnout' 'Stress']\n"
          ]
        }
      ],
      "source": [
        "symptoms_unique_values = mental_df['Symptoms'].unique()\n",
        "therapy_unique_values = mental_df['Suggested_Therapy'].unique()\n",
        "diagnosis_unique_values = mental_df['Diagnosis'].unique()\n",
        "\n",
        "print(\"Symtoms unique values \\n\", symptoms_unique_values)\n",
        "print(\"therapy_unique_values \\n\", therapy_unique_values)\n",
        "print(\"diagnosis_unique_values \\n\", diagnosis_unique_values)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "A label encoder is a technique that converts non-numerical data into numerical values, \n",
        "which is useful for machine learning and data analysis. <br>\n",
        "It's often used when working with categorical data, such as ordinal data, \n",
        "where there's a hierarchy among the values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import classification_report\n",
        "# Target supposed to be Target Supposed.\n",
        "#  th\n",
        "def build_self_test_self_care_advice():\n",
        "    le_diagnosis = LabelEncoder()\n",
        "    le_symtoms = LabelEncoder()\n",
        "    le_self_care = LabelEncoder()\n",
        "    le_therapy = LabelEncoder()\n",
        "    # mental_df['Duration'] is already Int style, we don't have to encode it\n",
        "    \n",
        "    mental_df['Diagnosis_encoded'] = le_diagnosis.fit_transform(mental_df['Diagnosis'])\n",
        "    mental_df['Symptoms_encoded'] = le_symtoms.fit_transform(mental_df['Symptoms'])\n",
        "    mental_df['Self_Care_Advice_encoded'] = le_self_care.fit_transform(mental_df['Self_Care_Advice'])\n",
        "    mental_df['Suggested_Therapy_encoded'] = le_therapy.fit_transform(mental_df['Suggested_Therapy'])\n",
        "\n",
        "    # Training Data\n",
        "    X = mental_df[['Diagnosis_encoded', 'Symptoms_encoded']]\n",
        "    y_self_care = mental_df['Self_Care_Advice_encoded']\n",
        "    y_therapy = mental_df['Suggested_Therapy_encoded']\n",
        "\n",
        "    X_train, X_test, y_self_care_train, y_self_care_test, y_therapy_train, y_therapy_test = train_test_split(X, y_self_care, y_therapy, test_size=0.2, random_state=42)\n",
        "    # Train models for Self Care Advice and Suggested Therapy\n",
        "    model_self_care = RandomForestClassifier()\n",
        "    model_therapy = RandomForestClassifier()\n",
        "\n",
        "    model_self_care.fit(X_train, y_self_care_train)\n",
        "    model_therapy.fit(X_train, y_therapy_train)\n",
        "\n",
        "    # Make predictions\n",
        "    self_care_pred = model_self_care.predict(X_test)\n",
        "    therapy_pred = model_therapy.predict(X_test)\n",
        "\n",
        "    # Display classification reports\n",
        "    print(\"Self Care Advice Classification Report:\")\n",
        "    self_care_report = classification_report(y_self_care_test, self_care_pred, target_names=le_self_care.classes_)\n",
        "    print(self_care_report)\n",
        "    print(\"\\nSuggested Therapy Classification Report:\")\n",
        "    self_therapy_report = classification_report(y_therapy_test, therapy_pred, target_names=le_therapy.classes_)\n",
        "    print(self_therapy_report)\n",
        "    return model_self_care , model_therapy, le_diagnosis, le_symtoms, le_self_care, le_therapy, self_care_report, self_therapy_report"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Initial Reports\n",
        "\n",
        "### Self Care Advice Classification Report\n",
        "\n",
        "| Self Care Advice       | Precision | Recall | F1-Score | Support |\n",
        "|------------------------|-----------|--------|----------|---------|\n",
        "| Breathing Exercises    | 0.20      | 0.17   | 0.18     | 166     |\n",
        "| Exercise               | 0.17      | 0.24   | 0.20     | 181     |\n",
        "| Journaling             | 0.32      | 0.14   | 0.20     | 191     |\n",
        "| Meditation             | 0.11      | 0.03   | 0.04     | 116     |\n",
        "| Take Breaks            | 0.17      | 0.33   | 0.23     | 164     |\n",
        "| Talk to a Friend       | 0.22      | 0.21   | 0.21     | 182     |\n",
        "| **Accuracy**           |           |        | 0.19     | 1000    |\n",
        "| **Macro Avg**          | 0.20      | 0.19   | 0.18     | 1000    |\n",
        "| **Weighted Avg**       | 0.21      | 0.19   | 0.18     | 1000    |\n",
        "\n",
        "### Suggested Therapy Classification Report\n",
        "\n",
        "| Suggested Therapy               | Precision | Recall | F1-Score | Support |\n",
        "|---------------------------------|-----------|--------|----------|---------|\n",
        "| Cognitive Behavioral Therapy    | 0.16      | 0.13   | 0.14     | 200     |\n",
        "| Mindfulness-Based Therapy       | 0.14      | 0.03   | 0.05     | 189     |\n",
        "| No Therapy Needed               | 0.17      | 0.09   | 0.11     | 187     |\n",
        "| Psychotherapy                   | 0.17      | 0.31   | 0.22     | 202     |\n",
        "| Support Groups                  | 0.24      | 0.34   | 0.28     | 222     |\n",
        "| **Accuracy**                    |           |        | 0.19     | 1000    |\n",
        "| **Macro Avg**                   | 0.17      | 0.18   | 0.16     | 1000    |\n",
        "| **Weighted Avg**                | 0.18      | 0.19   | 0.17     | 1000    |\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {},
      "outputs": [],
      "source": [
        "# model_self_care , model_therapy, le_diagnosis, le_symtoms, le_self_care, le_therapy, self_care_report, self_therapy_report = build_model()\n",
        "# Save model for purpose\n",
        "import joblib\n",
        "def save_model(model_self_care , model_therapy, le_diagnosis, le_symtoms, le_self_care, le_therapy ):\n",
        "    joblib.dump(model_self_care, \"model_self_care.pkl\")\n",
        "    joblib.dump(model_therapy, \"model_therapy.pkl\")\n",
        "    joblib.dump(le_diagnosis, \"le_diagnosis.pkl\")\n",
        "    joblib.dump(le_symtoms, \"le_symtoms.pkl\")\n",
        "    joblib.dump(le_self_care, \"le_self_care.pkl\")\n",
        "    joblib.dump(le_therapy, \"le_therapy.pkl\")\n",
        "\n",
        "# save_model(model_self_care , model_therapy, le_diagnosis, le_symtoms, le_self_care, le_therapy)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Current accuracy 0.18 precison also,\n",
        "To imporve this models. <br>\n",
        "There are serveral ways to imporve ways.<br>\n",
        "Add more parameters (which contains demographic infomrationm which user can simply input them), also re mapping based Diagnosis.<br>\n",
        "I created three value includes 2 informaiton.<br>\n",
        "It wil help to organize better modeling. <br>\n",
        "Current features has <br>\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['Other' 'Female' 'Non-binary' 'Male']\n",
            "[29 37 47 35 22  8 31 20 21  9 38 43 30  2  3 34 36 19 33 23 46 49 48 41\n",
            " 11 16 10 45 13  4 39 12 51 24 17 32  1 14 44 15 26  5 40 27 42 18  6 50\n",
            " 28 25  7]\n",
            "['Moderate' 'High' 'Low' 'Critical']\n",
            "[ 1  4  5  2  6  9 10  8  7  3]\n",
            "['OCD' 'None' 'PTSD' 'Bipolar Disorder' 'Anxiety' 'Depression']\n"
          ]
        }
      ],
      "source": [
        "print(mental_df['Gender'].unique())\n",
        "print(mental_df['Duration'].unique())\n",
        "print(mental_df['Urgency_Level'].unique())\n",
        "print(mental_df['Stress_Level'].unique())\n",
        "print(mental_df['Prev_Diagnosis'].unique())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Diagnosis Group Classification Report:\n",
            "                          precision    recall  f1-score   support\n",
            "\n",
            "       Anxiety Disorders       0.47      0.61      0.53       422\n",
            "          Mood Disorders       0.49      0.45      0.47       440\n",
            "Stress-Related Disorders       0.27      0.09      0.13       138\n",
            "\n",
            "                accuracy                           0.47      1000\n",
            "               macro avg       0.41      0.38      0.38      1000\n",
            "            weighted avg       0.45      0.47      0.45      1000\n",
            "\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "(RandomForestClassifier(),\n",
              " LabelEncoder(),\n",
              " '                          precision    recall  f1-score   support\\n\\n       Anxiety Disorders       0.47      0.61      0.53       422\\n          Mood Disorders       0.49      0.45      0.47       440\\nStress-Related Disorders       0.27      0.09      0.13       138\\n\\n                accuracy                           0.47      1000\\n               macro avg       0.41      0.38      0.38      1000\\n            weighted avg       0.45      0.47      0.45      1000\\n')"
            ]
          },
          "execution_count": 70,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Diagnosis Group\n",
        "\n",
        "def group_diagnosis(row):\n",
        "    if row['Diagnosis'] in ['Panic Disorder', 'Anxiety']:\n",
        "        return 'Anxiety Disorders'\n",
        "    elif row['Diagnosis'] in ['Depression', 'Burnout']:\n",
        "        return 'Mood Disorders'\n",
        "    elif row['Diagnosis'] == 'Stress':\n",
        "        return 'Stress-Related Disorders'\n",
        "def group_prev_diagnosis(row):\n",
        "    if row['Prev_Diagnosis'] in ['Panic Disorder', 'Anxiety', 'OCD']:\n",
        "        return 'Anxiety Disorders'\n",
        "    elif row['Prev_Diagnosis'] in ['Depression', 'Bipolar Disorder']:\n",
        "        return 'Mood Disorders'\n",
        "    elif row['Prev_Diagnosis'] in ['Stress', 'PTSD']:\n",
        "        return 'Stress-Related Disorders'\n",
        "    else:\n",
        "        return ''\n",
        "\n",
        "def re_map_gender(row):\n",
        "    if row['Gender'] == 'Male':\n",
        "        return 1\n",
        "    elif row['Gender'] == 'Female':\n",
        "        return 2\n",
        "    else:\n",
        "        return 3\n",
        "def re_map_urgency_level(row):\n",
        "    if row[\"Urgency_Level\"] == \"Low\":\n",
        "        return 1\n",
        "    elif row[\"Urgency_Level\"] == \"Moderate\":\n",
        "        return 2\n",
        "    elif row[\"Urgency_Level\"] == \"High\":\n",
        "        return 3\n",
        "    else:\n",
        "        return 4\n",
        "    \n",
        "# Bascially\n",
        "def improved_test_reports_diagnosis(mental_df):\n",
        "    # Diagnosis Group\n",
        "    mental_df[\"Diagnosis_Group\"] = mental_df.apply(group_diagnosis, axis=1)\n",
        "    mental_df['Prev_Diagnosis_Group'] = mental_df.apply(group_prev_diagnosis, axis=1)\n",
        "    mental_df[\"Re_Gender\"] = mental_df.apply(re_map_gender, axis=1)\n",
        "    mental_df[\"Urgency_Level\"] = mental_df.apply(re_map_urgency_level, axis=1)\n",
        "    \n",
        "    le_diagnosis_group = LabelEncoder()\n",
        "    le_prev_Diagnosis_group = LabelEncoder()\n",
        "    le_symtoms = LabelEncoder()\n",
        "\n",
        "    mental_df['Diagnosis_Group_encoded'] = le_diagnosis_group.fit_transform(mental_df['Diagnosis_Group'])\n",
        "    mental_df['Prev_Diagnosis_Group_encoded'] =  le_prev_Diagnosis_group.fit_transform(mental_df['Prev_Diagnosis'])\n",
        "    mental_df['Symptoms_encoded'] = le_symtoms.fit_transform(mental_df['Symptoms'])\n",
        "\n",
        "    # Training Data with duration\n",
        "    X = mental_df[['Age', 'Symptoms_encoded', \"Re_Gender\", \"Prev_Diagnosis_Group_encoded\", \"Duration\", \"Stress_Level\", \"Urgency_Level\"]] \n",
        "    y_diagnosis = mental_df['Diagnosis_Group_encoded']\n",
        "\n",
        "    X_train, X_test, y_diagnosis_train, y_diagnosis_test = train_test_split(X, y_diagnosis, test_size=0.2, random_state=42)\n",
        "    \n",
        "    # Train model for Diagnosis\n",
        "    model_diagnosis = RandomForestClassifier()\n",
        "    model_diagnosis.fit(X_train, y_diagnosis_train)\n",
        "\n",
        "    # Make predictions\n",
        "    diagnos_pred = model_diagnosis.predict(X_test)\n",
        "\n",
        "    # Display classification reports\n",
        "    print(\"Diagnosis Group Classification Report:\")\n",
        "    diagnosis_report = classification_report(y_diagnosis_test, diagnos_pred, target_names=le_diagnosis_group.classes_)\n",
        "    print(diagnosis_report)\n",
        "    \n",
        "    return model_diagnosis, le_diagnosis_group, diagnosis_report\n",
        "improved_test_reports_diagnosis(mental_df)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {},
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'y_d' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[71], line 36\u001b[0m\n\u001b[0;32m     32\u001b[0m     \u001b[38;5;28mprint\u001b[39m(diagnosis_report)\n\u001b[0;32m     34\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m model_diagnosis, le_diagnosis_group, diagnosis_report\n\u001b[1;32m---> 36\u001b[0m improved_test_reports_diagnosis_svm(mental_df)\n",
            "Cell \u001b[1;32mIn[71], line 20\u001b[0m, in \u001b[0;36mimproved_test_reports_diagnosis_svm\u001b[1;34m(mental_df)\u001b[0m\n\u001b[0;32m     17\u001b[0m X \u001b[38;5;241m=\u001b[39m mental_df[[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAge\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSymptoms_encoded\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRe_Gender\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPrev_Diagnosis_Group_encoded\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDuration\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStress_Level\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUrgency_Level\u001b[39m\u001b[38;5;124m\"\u001b[39m]] \n\u001b[0;32m     18\u001b[0m y_diagnosis \u001b[38;5;241m=\u001b[39m mental_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDiagnosis_Group_encoded\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m---> 20\u001b[0m X_train, X_test, y_diagnosis_train, y_diagnosis_test \u001b[38;5;241m=\u001b[39m train_test_split(X, y_d\u001b[38;5;241m|\u001b[39miagnosis, test_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.2\u001b[39m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m)\n\u001b[0;32m     22\u001b[0m \u001b[38;5;66;03m# Train model using Support Vector Machine\u001b[39;00m\n\u001b[0;32m     23\u001b[0m model_diagnosis \u001b[38;5;241m=\u001b[39m SVC(kernel\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpoly\u001b[39m\u001b[38;5;124m'\u001b[39m)  \u001b[38;5;66;03m# You can also try 'rbf' or 'poly'\u001b[39;00m\n",
            "\u001b[1;31mNameError\u001b[0m: name 'y_d' is not defined"
          ]
        }
      ],
      "source": [
        "from sklearn.svm import SVC\n",
        "def improved_test_reports_diagnosis_svm(mental_df):\n",
        "    # Same preprocessing steps\n",
        "    mental_df[\"Diagnosis_Group\"] = mental_df.apply(group_diagnosis, axis=1)\n",
        "    mental_df['Prev_Diagnosis_Group'] = mental_df.apply(group_prev_diagnosis, axis=1)\n",
        "    mental_df[\"Re_Gender\"] = mental_df.apply(re_map_gender, axis=1)\n",
        "    mental_df[\"Urgency_Level\"] = mental_df.apply(re_map_urgency_level, axis=1)\n",
        "\n",
        "    le_diagnosis_group = LabelEncoder()\n",
        "    le_prev_Diagnosis_group = LabelEncoder()\n",
        "    le_symtoms = LabelEncoder()\n",
        "\n",
        "    mental_df['Diagnosis_Group_encoded'] = le_diagnosis_group.fit_transform(mental_df['Diagnosis_Group'])\n",
        "    mental_df['Prev_Diagnosis_Group_encoded'] = le_prev_Diagnosis_group.fit_transform(mental_df['Prev_Diagnosis'])\n",
        "    mental_df['Symptoms_encoded'] = le_symtoms.fit_transform(mental_df['Symptoms'])\n",
        "\n",
        "    X = mental_df[['Age', 'Symptoms_encoded', \"Re_Gender\", \"Prev_Diagnosis_Group_encoded\", \"Duration\", \"Stress_Level\", \"Urgency_Level\"]] \n",
        "    y_diagnosis = mental_df['Diagnosis_Group_encoded']\n",
        "\n",
        "    X_train, X_test, y_diagnosis_train, y_diagnosis_test = train_test_split(X, y_d|iagnosis, test_size=0.2, random_state=42)\n",
        "\n",
        "    # Train model using Support Vector Machine\n",
        "    model_diagnosis = SVC(kernel='poly')  # You can also try 'rbf' or 'poly'\n",
        "    model_diagnosis.fit(X_train, y_diagnosis_train)\n",
        "\n",
        "    # Make predictions\n",
        "    diagnos_pred = model_diagnosis.predict(X_test)\n",
        "\n",
        "    # Display classification report\n",
        "    print(\"Diagnosis Group Classification Report (SVM):\")\n",
        "    diagnosis_report = classification_report(y_diagnosis_test, diagnos_pred, target_names=le_diagnosis_group.classes_)\n",
        "    print(diagnosis_report)\n",
        "\n",
        "    return model_diagnosis, le_diagnosis_group, diagnosis_report\n",
        "\n",
        "improved_test_reports_diagnosis_svm(mental_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'Panic': ['terror', 'affright', 'panic', 'scare'], 'Disorder': ['perturb', 'disorderliness', 'distract', 'disarray', 'disquiet', 'trouble', 'cark', 'disorder', 'unhinge', 'upset'], 'Depression': ['depression', 'slump', 'natural_depression', 'impression', 'Great_Depression', 'depressive_disorder', 'clinical_depression', 'economic_crisis', 'Depression', 'imprint', 'low'], 'Burnout': [], 'Stress': ['accentuate', 'accent', 'punctuate', 'stress', 'tension', 'try', 'tenseness', 'emphasis', 'emphasize', 'emphasise', 'strain', 'focus']}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to\n",
            "[nltk_data]     C:\\Users\\ykim\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "from nltk.corpus import wordnet\n",
        "from itertools import chain\n",
        "\n",
        "# Based on questions and topics, Create chains then finds vlaues.\n",
        "def find_related_words():\n",
        "    nltk.download('wordnet')\n",
        "\n",
        "    words = {\n",
        "        'Panic': 'panic',\n",
        "        'Disorder': 'disorder',\n",
        "        'Depression': 'depression',\n",
        "        'Burnout': 'burnout',\n",
        "        'Stress': 'stress'\n",
        "    }\n",
        "\n",
        "    # Initialize a dictionary to store the results\n",
        "    related_words = {}\n",
        "\n",
        "    for key, word in words.items():\n",
        "        word_synsets = wordnet.synsets(word)\n",
        "        similar_words = set(chain(*[synset.lemma_names() for synset in word_synsets]))\n",
        "        related_words[key] = list(similar_words)\n",
        "\n",
        "    return related_words\n",
        "\n",
        "# Get the related words and print the dictionary\n",
        "similar_words_dict = find_related_words()\n",
        "print(similar_words_dict)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Result for related words\n",
        "# {'Panic': ['panic', 'scare', 'terror', 'affright'], \n",
        "# 'Disorder': ['distract', 'upset', 'trouble', 'disorderliness', 'cark', 'disarray', 'disorder', 'perturb', 'disquiet', 'unhinge'], \n",
        "# 'Depression': ['impression', 'clinical_depression', 'slump', 'Great_Depression', 'Depression', 'low', 'depressive_disorder', 'natural_depression', 'imprint', 'economic_crisis', 'depression'], \n",
        "# 'Burnout': [], \n",
        "# 'Stress': ['strain', 'emphasis', 'emphasize', 'tension', 'punctuate', 'focus', 'accentuate', 'try', 'tenseness', 'emphasise', 'stress', 'accent']}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Counsel data training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Orignal length1482\n",
            "\n",
            "Word frequencies in the 'topics' column:\n",
            "Family :  119\n",
            "Conflict :  91\n",
            "Substance :  14\n",
            "AbuseAddiction :  9\n",
            "Behavioral :  49\n",
            "ChangeSocial :  3\n",
            "Relationships :  214\n",
            "Relationship :  45\n",
            "Dissolution :  68\n",
            "Anger :  32\n",
            "Management :  25\n",
            "Sleep :  10\n",
            "Improvement :  17\n",
            "Professional :  34\n",
            "EthicsLegal :  6\n",
            "& :  45\n",
            "Regulatory :  20\n",
            "Social :  41\n",
            "RelationshipsMarriage :  11\n",
            "MarriageIntimacy :  26\n",
            "Domestic :  9\n",
            "ViolenceAnger :  2\n",
            "ManagementFamily :  3\n",
            "Human :  50\n",
            "Sexuality :  33\n",
            "ManagementSleep :  2\n",
            "Military :  3\n",
            "Issues :  10\n",
            "RelationshipsDomestic :  3\n",
            "Violence :  10\n",
            "ViolenceRelationship :  1\n",
            "Marriage :  25\n",
            "Grief :  20\n",
            "and :  23\n",
            "Loss :  9\n",
            "ConflictChildren :  1\n",
            "Adolescents :  9\n",
            "MarriageRelationship :  4\n",
            "TraumaHuman :  1\n",
            "RelationshipsIntimacy :  42\n",
            "ManagementParenting :  1\n",
            "Intimacy :  17\n",
            "Workplace :  9\n",
            "SexualityMarriage :  3\n",
            "LGBTQ :  29\n",
            "SpiritualityFamily :  2\n",
            "Ethics :  23\n",
            "ViolenceRelationships :  1\n",
            "ConflictRelationships :  5\n",
            "Self-esteem :  29\n",
            "Self-esteemRelationships :  12\n",
            "Parenting :  35\n",
            "ConflictMarriage :  6\n",
            "ConflictSelf-esteem :  3\n",
            "ParentingRelationships :  1\n",
            "nan :  10\n",
            "Counseling :  53\n",
            "RelationshipsSelf-esteem :  9\n",
            "Eating :  7\n",
            "RelationshipsProfessional :  2\n",
            "ParentingSubstance :  1\n",
            "AbuseSpirituality :  1\n",
            "Self-esteemRelationship :  1\n",
            "ConflictAnger :  2\n",
            "ParentingAnger :  1\n",
            "MarriageFamily :  3\n",
            "ConflictProfessional :  2\n",
            "RelationshipsHuman :  4\n",
            "SexualityLGBTQ :  10\n",
            "RelationshipsParentingFamily :  1\n",
            "Legal :  16\n",
            "LGBTQIntimacy :  1\n",
            "ManagementRelationships :  3\n",
            "AbuseFamily :  1\n",
            "Self-esteemMarriageTraumaIntimacy :  3\n",
            "MarriageAddiction :  1\n",
            "RelationshipsLegal :  1\n",
            "SexualityRelationships :  4\n",
            "ConflictRelationshipsMarriage :  1\n",
            "MarriageAnger :  1\n",
            "RelationshipsFamily :  8\n",
            "Change :  56\n",
            "SexualitySocial :  1\n",
            "Self-esteemEating :  1\n",
            "Career :  2\n",
            "CounselingProfessional :  2\n",
            "MarriageGrief :  1\n",
            "Self-esteemSocial :  5\n",
            "AddictionSubstance :  11\n",
            "Abuse :  23\n",
            "Spirituality :  8\n",
            "RelationshipsSocial :  22\n",
            "SexualityAddiction :  2\n",
            "IntimacyRelationships :  11\n",
            "RelationshipsSelf-esteemHuman :  1\n",
            "Trauma :  28\n",
            "SexualityIntimacyMarriage :  9\n",
            "RelationshipsParenting :  1\n",
            "ConflictParenting :  16\n",
            "RegulatoryProfessional :  3\n",
            "ManagementDomestic :  2\n",
            "ParentingFamily :  6\n",
            "ConflictLegal :  6\n",
            "ViolenceLegal :  1\n",
            "IntimacyHuman :  7\n",
            "IntimacySocial :  1\n",
            "TraumaMilitary :  3\n",
            "ManagementRelationshipsSocial :  3\n",
            "TraumaFamily :  4\n",
            "TraumaSelf-esteemRelationship :  1\n",
            "RelationshipsRelationship :  2\n",
            "RelationshipsBehavioral :  6\n",
            "MarriageDomestic :  1\n",
            "ConflictParentingMarriage :  1\n",
            "MarriageIntimacyHuman :  9\n",
            "MarriageRelationshipsIntimacy :  9\n",
            "SexualityFamily :  1\n",
            "ConflictSpirituality :  1\n",
            "RelationshipsIntimacyHuman :  1\n",
            "ParentingRelationship :  9\n",
            "RelationshipsTrauma :  1\n",
            "AddictionMarriageIntimacy :  3\n",
            "ConflictTrauma :  1\n",
            "ConflictSocial :  1\n",
            "RelationshipsRelationshipsIntimacy :  1\n",
            "LGBTQRelationshipsIntimacy :  1\n",
            "MarriageSocial :  1\n",
            "ManagementSocial :  1\n",
            "RelationshipsRelationships :  1\n",
            "ChangeMarriage :  1\n",
            "ManagementBehavioral :  1\n",
            "ViolenceMarriage :  1\n",
            "LGBTQFamily :  4\n",
            "IntimacyMarriage :  1\n",
            "ConflictRelationshipsIntimacy :  1\n",
            "ConflictLGBTQ :  6\n",
            "SpiritualityRelationships :  2\n",
            "RelationshipsWorkplace :  5\n",
            "SexualityIntimacyRelationships :  14\n",
            "SexualityIntimacy :  8\n",
            "RegulatoryAddiction :  2\n",
            "RelationshipsSubstance :  8\n",
            "Self-esteemSleep :  1\n",
            "RelationshipsChildren :  1\n",
            "ChangeLGBTQ :  1\n",
            "LossFamily :  2\n",
            "Self-esteemBehavioral :  2\n",
            "RelationshipsRelationshipsAddiction :  1\n",
            "DiagnosisCounseling :  4\n",
            "Fundamentals :  87\n",
            "RelationshipsLGBTQ :  2\n",
            "Self-esteemLGBTQ :  1\n",
            "EthicsParentingLegal :  1\n",
            "TraumaRelationships :  8\n",
            "LGBTQHuman :  4\n",
            "IntimacyTrauma :  2\n",
            "ViolenceSleep :  2\n",
            "LossSubstance :  7\n",
            "AbuseTrauma :  7\n",
            "IntimacyRelationshipsHuman :  11\n",
            "RelationshipsCareer :  2\n",
            "ChangeRelationships :  1\n",
            "Addiction :  3\n",
            "ConflictDomestic :  1\n",
            "Alzheimer'sFamily :  2\n",
            "SexualityRelationshipsIntimacy :  2\n",
            "ChangeSleep :  1\n",
            "SpiritualitySocial :  6\n",
            "RelationshipsIntimacyLGBTQ :  5\n",
            "MarriageHuman :  2\n",
            "MarriageIntimacyAddictionBehavioral :  1\n",
            "LossRelationships :  2\n",
            "ConflictRelationship :  1\n",
            "IntimacyRelationshipsDomestic :  1\n",
            "AddictionProfessional :  2\n",
            "ConflictParentingRelationship :  1\n",
            "Diagnosis :  8\n",
            "RegulatoryParentingFamily :  5\n",
            "AbuseSocial :  4\n",
            "RelationshipsAddictionSubstance :  2\n",
            "AbuseSelf-esteem :  1\n",
            "Self-esteemSpirituality :  1\n",
            "SexualityTrauma :  1\n",
            "SexualityTraumaIntimacyRelationships :  1\n",
            "LossTrauma :  1\n",
            "Self-esteemSubstance :  1\n",
            "RelationshipsMarriageWorkplace :  3\n",
            "RelationshipsMilitary :  4\n",
            "RegulatorySubstance :  5\n",
            "ConflictRelationshipsParenting :  2\n",
            "ManagementRelationshipsSubstance :  1\n",
            "RelationshipsIntimacySpirituality :  5\n",
            "SexualityLGBTQIntimacy :  3\n",
            "ParentingChildren :  4\n",
            "Children :  3\n",
            "EthicsCounseling :  15\n",
            "ConflictRelationshipsRelationship :  1\n",
            "AdolescentsBehavioral :  1\n",
            "RelationshipsAddiction :  2\n",
            "ConflictAddictionSubstance :  1\n",
            "ParentingAddiction :  1\n",
            "ManagementSelf-esteemMarriageFamily :  1\n",
            "ConflictParentingChildren :  1\n",
            "total counts1482\n",
            "stress_count 54\n",
            "depression_count 196\n",
            "disorder_count 12\n",
            "anxiety_count 180\n",
            "burn_out_count 0\n",
            "Found selcted count 442\n"
          ]
        }
      ],
      "source": [
        "counsel_df = pd.read_csv(\"counselchat-data.csv\")\n",
        "\n",
        "from collections import Counter\n",
        "# Ensure you've downloaded the WordNet corpus\n",
        "# Display the word frequencies\n",
        "def get_word_frequencies(counsel_df):\n",
        "    print(\"Orignal length\" + str(len(counsel_df)))\n",
        "    counsel_df = counsel_df[['questionText', 'topics','answerText']]\n",
        "    all_words = ' '.join(counsel_df['topics'].astype(str)).replace(',', '').split()\n",
        "    # Count the frequency of each word\n",
        "    word_count = Counter(all_words)\n",
        "    print(\"\\nWord frequencies in the 'topics' column:\")\n",
        "    found_selected_count = 0\n",
        "    stress_count = 0\n",
        "    depression_count = 0\n",
        "    disorder_count = 0\n",
        "    anxiety_count = 0\n",
        "    burn_out_count = 0\n",
        "    for word, count in word_count.items():\n",
        "        if word.__contains__(\"Stress\"):\n",
        "            stress_count +=count\n",
        "        elif word.__contains__(\"Depression\"):\n",
        "            depression_count += count\n",
        "        elif word.__contains__(\"Disorder\"):\n",
        "            disorder_count += count\n",
        "        elif word.__contains__(\"Anxiety\"):\n",
        "            anxiety_count += count\n",
        "        elif word.__contains__(\"Burnout\"):\n",
        "            burn_out_count += count\n",
        "        else:\n",
        "            print(f\"{word} :  {count}\")\n",
        "    found_selected_count = stress_count + depression_count + disorder_count + anxiety_count + burn_out_count\n",
        "    print(\"total counts\" + str(len(counsel_df)))\n",
        "    print(\"stress_count\", stress_count)\n",
        "    print(\"depression_count\", depression_count)\n",
        "    print(\"disorder_count\", disorder_count)\n",
        "    print(\"anxiety_count\", anxiety_count)\n",
        "    print(\"burn_out_count\", burn_out_count)\n",
        "    print(\"Found selcted count\", found_selected_count)\n",
        "\n",
        "get_word_frequencies(counsel_df)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training Accuracy: 0.9976133651551312\n",
            "Number of rows in test set: 1063\n"
          ]
        }
      ],
      "source": [
        "# https://my.clevelandclinic.org/health/diseases/22295-mental-health-disorders\n",
        "\n",
        "import nltk\n",
        "from nltk.corpus import wordnet\n",
        "\n",
        "counsel_df = pd.read_csv(\"counselchat-data.csv\")\n",
        "target_Keywords = {\n",
        "    'Anxiety Disorders': ['panic disorder', 'anxiety'],\n",
        "    'Mood Disorders': ['depression', 'burnout'],\n",
        "    'Stress-Related Disorders': ['stress', 'PTSD']\n",
        "}\n",
        "\n",
        "similar_words_dict = {\n",
        "    'panic': ['panic', 'scare', 'terror', 'affright'],\n",
        "    'disorder': ['distract', 'upset', 'trouble', 'disorderliness', 'cark', 'disarray', 'disorder', 'perturb', 'disquiet', 'unhinge'],\n",
        "    'depression': ['impression', 'clinical_depression', 'slump', 'Great_Depression', 'depression', 'low', 'depressive_disorder', 'natural_depression', 'imprint', 'economic_crisis', 'depression'],\n",
        "    'burnout': [],\n",
        "    'stress': ['strain', 'emphasis', 'emphasize', 'tension', 'punctuate', 'focus', 'accentuate', 'try', 'tenseness', 'emphasise', 'stress', 'accent']\n",
        "}\n",
        "\n",
        "def group_diagnosis(row):\n",
        "    all_words = str(row['topics']).lower().split()\n",
        "\n",
        "    for disorder, keywords in target_Keywords.items():\n",
        "        for keyword in keywords:\n",
        "            if any(keyword in word for word in all_words):\n",
        "                return disorder\n",
        "            if keyword in similar_words_dict:\n",
        "                # Check if any of the similar words match\n",
        "                if any(sim_word in all_words for sim_word in similar_words_dict[keyword]):\n",
        "                    return disorder\n",
        "    return None\n",
        "# Original 120.\n",
        "# print(similar_words_dict)\n",
        "# Adding new counsel_df based on items\n",
        "counsel_df[\"re_diagnosis\"] = counsel_df.apply(group_diagnosis, axis=1)\n",
        "# print(counsel_df['re_diagnosis'])\n",
        "# print(len(counsel_df['re_diagnosis']))\n",
        "\n",
        "# cd remapping diagnosis.\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Function to create and train the model\n",
        "# TfidfVectorizer\n",
        "# LogicRegression\n",
        "def create_diagnosis_model(train_data, target_column='re_diagnosis'):\n",
        "    # Extract text and target columns\n",
        "    X_train = train_data['topics']\n",
        "    y_train = train_data[target_column]\n",
        "\n",
        "    # TF-IDF vectorization\n",
        "    tfidf = TfidfVectorizer(max_features=5000)\n",
        "    X_train_tfidf = tfidf.fit_transform(X_train)\n",
        "\n",
        "    # Train a logistic regression model\n",
        "    model = LogisticRegression(max_iter=1000)\n",
        "    model.fit(X_train_tfidf, y_train)\n",
        "\n",
        "    # Evaluate model accuracy on the training set\n",
        "    X_train_pred = model.predict(X_train_tfidf)\n",
        "    accuracy = accuracy_score(y_train, X_train_pred)\n",
        "    print(f\"Training Accuracy: {accuracy}\")\n",
        "\n",
        "    return model, tfidf\n",
        "\n",
        "# Function to make predictions using the trained model\n",
        "def predict_missing_diagnoses(df, model, tfidf, target_column='re_diagnosis'):\n",
        "    # Extarct out df re_diagnosis is None\n",
        "    df_test = df[df[target_column].isna()]\n",
        "\n",
        "    X_test = df_test['topics']\n",
        "    print(f\"Number of rows in test set: {len(X_test)}\")\n",
        "    X_test = df_test['topics'].fillna('')  # Replace NaN values with an empty string\n",
        "    non_empty_mask = X_test.str.strip() != ''\n",
        "\n",
        "    # 한번더 걸러준다\n",
        "    df_test = df_test[non_empty_mask]\n",
        "    X_test =  X_test[non_empty_mask]\n",
        "\n",
        "    # \n",
        "    X_test_tfidf = tfidf.transform(X_test)\n",
        "\n",
        "    # Predict missing diagnoses\n",
        "    predictions = model.predict(X_test_tfidf)\n",
        "\n",
        "    # Assign predictions back to the DataFrame\n",
        "    df.loc[df[target_column].isna() & non_empty_mask, target_column] = predictions\n",
        "\n",
        "    return df\n",
        "\n",
        "# Example usage\n",
        "# Step 1: Create model\n",
        "# Training and prediction process\n",
        "df_train = counsel_df[counsel_df['re_diagnosis'].notna()]\n",
        "model, tfidf = create_diagnosis_model(df_train)\n",
        "remapped_consel_df = predict_missing_diagnoses(counsel_df, model, tfidf)\n",
        "\n",
        "\n",
        "\n",
        "# remapped_consel_df.to_csv(\"remapped_consel_df\", index=False)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1482\n",
            "Index(['questionText', 'topics', 're_diagnosis', 'answerText'], dtype='object')\n"
          ]
        }
      ],
      "source": [
        "print(len(remapped_consel_df))\n",
        "print(remapped_consel_df.columns)\n",
        "remapped_consel_df = remapped_consel_df[['questionText', 'topics', 're_diagnosis', 'answerText']]\n",
        "remapped_consel_df.to_csv(\"remapped_consel_df\", index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 76,
      "metadata": {},
      "outputs": [
        {
          "ename": "TypeError",
          "evalue": "GPT2Tokenizer._tokenize() got an unexpected keyword argument 'truncation'",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[76], line 52\u001b[0m\n\u001b[0;32m     49\u001b[0m model\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[0;32m     51\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m3\u001b[39m):  \u001b[38;5;66;03m# Increase epochs for better performance\u001b[39;00m\n\u001b[1;32m---> 52\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m train_loader:\n\u001b[0;32m     53\u001b[0m         optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m     54\u001b[0m         outputs \u001b[38;5;241m=\u001b[39m model(\n\u001b[0;32m     55\u001b[0m             input_ids\u001b[38;5;241m=\u001b[39mbatch[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m     56\u001b[0m             attention_mask\u001b[38;5;241m=\u001b[39mbatch[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mattention_mask\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m     57\u001b[0m             labels\u001b[38;5;241m=\u001b[39mbatch[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m     58\u001b[0m         )\n",
            "File \u001b[1;32mc:\\Users\\ykim\\AppData\\Local\\anaconda3\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:631\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    628\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    629\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    630\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 631\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_data()\n\u001b[0;32m    632\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    633\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    635\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
            "File \u001b[1;32mc:\\Users\\ykim\\AppData\\Local\\anaconda3\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:675\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    673\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    674\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 675\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_fetcher\u001b[38;5;241m.\u001b[39mfetch(index)  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    676\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    677\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
            "File \u001b[1;32mc:\\Users\\ykim\\AppData\\Local\\anaconda3\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:51\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
            "File \u001b[1;32mc:\\Users\\ykim\\AppData\\Local\\anaconda3\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:51\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
            "Cell \u001b[1;32mIn[76], line 28\u001b[0m, in \u001b[0;36mChatBotDataSet.__getitem__\u001b[1;34m(self, index)\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, index):\n\u001b[1;32m---> 28\u001b[0m     input_ids \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokenizer\u001b[38;5;241m.\u001b[39mencode(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_texts[index], max_length\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_len, truncation\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     29\u001b[0m     target_ids \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokenizer\u001b[38;5;241m.\u001b[39mencode(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_texts[index], max_length\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_len, truncation\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     31\u001b[0m     \u001b[38;5;66;03m# Manually apply padding to reach max_len\u001b[39;00m\n",
            "File \u001b[1;32mc:\\Users\\ykim\\AppData\\Local\\anaconda3\\Lib\\site-packages\\transformers\\tokenization_utils.py:728\u001b[0m, in \u001b[0;36mPreTrainedTokenizer.encode\u001b[1;34m(self, text, text_pair, add_special_tokens, max_length, stride, truncation_strategy, return_tensors, **kwargs)\u001b[0m\n\u001b[0;32m    691\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mencode\u001b[39m(\u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    692\u001b[0m             text,\n\u001b[0;32m    693\u001b[0m             text_pair\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    698\u001b[0m             return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    699\u001b[0m             \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    700\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    701\u001b[0m \u001b[38;5;124;03m    Converts a string in a sequence of ids (integer), using the tokenizer and vocabulary.\u001b[39;00m\n\u001b[0;32m    702\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    726\u001b[0m \u001b[38;5;124;03m        **kwargs: passed to the `self.tokenize()` method\u001b[39;00m\n\u001b[0;32m    727\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 728\u001b[0m     encoded_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencode_plus(text,\n\u001b[0;32m    729\u001b[0m                                       text_pair\u001b[38;5;241m=\u001b[39mtext_pair,\n\u001b[0;32m    730\u001b[0m                                       max_length\u001b[38;5;241m=\u001b[39mmax_length,\n\u001b[0;32m    731\u001b[0m                                       add_special_tokens\u001b[38;5;241m=\u001b[39madd_special_tokens,\n\u001b[0;32m    732\u001b[0m                                       stride\u001b[38;5;241m=\u001b[39mstride,\n\u001b[0;32m    733\u001b[0m                                       truncation_strategy\u001b[38;5;241m=\u001b[39mtruncation_strategy,\n\u001b[0;32m    734\u001b[0m                                       return_tensors\u001b[38;5;241m=\u001b[39mreturn_tensors,\n\u001b[0;32m    735\u001b[0m                                       \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m encoded_inputs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
            "File \u001b[1;32mc:\\Users\\ykim\\AppData\\Local\\anaconda3\\Lib\\site-packages\\transformers\\tokenization_utils.py:786\u001b[0m, in \u001b[0;36mPreTrainedTokenizer.encode_plus\u001b[1;34m(self, text, text_pair, add_special_tokens, max_length, stride, truncation_strategy, return_tensors, **kwargs)\u001b[0m\n\u001b[0;32m    783\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    784\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInput is not valid. Should be a string, a list/tuple of strings or a list/tuple of integers.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 786\u001b[0m first_ids \u001b[38;5;241m=\u001b[39m get_input_ids(text)\n\u001b[0;32m    787\u001b[0m second_ids \u001b[38;5;241m=\u001b[39m get_input_ids(text_pair) \u001b[38;5;28;01mif\u001b[39;00m text_pair \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    789\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprepare_for_model(first_ids,\n\u001b[0;32m    790\u001b[0m                               pair_ids\u001b[38;5;241m=\u001b[39msecond_ids,\n\u001b[0;32m    791\u001b[0m                               max_length\u001b[38;5;241m=\u001b[39mmax_length,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    794\u001b[0m                               truncation_strategy\u001b[38;5;241m=\u001b[39mtruncation_strategy,\n\u001b[0;32m    795\u001b[0m                               return_tensors\u001b[38;5;241m=\u001b[39mreturn_tensors)\n",
            "File \u001b[1;32mc:\\Users\\ykim\\AppData\\Local\\anaconda3\\Lib\\site-packages\\transformers\\tokenization_utils.py:778\u001b[0m, in \u001b[0;36mPreTrainedTokenizer.encode_plus.<locals>.get_input_ids\u001b[1;34m(text)\u001b[0m\n\u001b[0;32m    776\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_input_ids\u001b[39m(text):\n\u001b[0;32m    777\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(text, six\u001b[38;5;241m.\u001b[39mstring_types):\n\u001b[1;32m--> 778\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconvert_tokens_to_ids(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokenize(text, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs))\n\u001b[0;32m    779\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(text, (\u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mtuple\u001b[39m)) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(text) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(text[\u001b[38;5;241m0\u001b[39m], six\u001b[38;5;241m.\u001b[39mstring_types):\n\u001b[0;32m    780\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconvert_tokens_to_ids(text)\n",
            "File \u001b[1;32mc:\\Users\\ykim\\AppData\\Local\\anaconda3\\Lib\\site-packages\\transformers\\tokenization_utils.py:649\u001b[0m, in \u001b[0;36mPreTrainedTokenizer.tokenize\u001b[1;34m(self, text, **kwargs)\u001b[0m\n\u001b[0;32m    644\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msum\u001b[39m((\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tokenize(token, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs) \u001b[38;5;28;01mif\u001b[39;00m token \u001b[38;5;129;01mnot\u001b[39;00m \\\n\u001b[0;32m    645\u001b[0m             \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madded_tokens_encoder \u001b[38;5;129;01mand\u001b[39;00m token \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mall_special_tokens \\\n\u001b[0;32m    646\u001b[0m             \u001b[38;5;28;01melse\u001b[39;00m [token] \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m tokenized_text), [])\n\u001b[0;32m    648\u001b[0m added_tokens \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madded_tokens_encoder\u001b[38;5;241m.\u001b[39mkeys()) \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mall_special_tokens\n\u001b[1;32m--> 649\u001b[0m tokenized_text \u001b[38;5;241m=\u001b[39m split_on_tokens(added_tokens, text)\n\u001b[0;32m    650\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m tokenized_text\n",
            "File \u001b[1;32mc:\\Users\\ykim\\AppData\\Local\\anaconda3\\Lib\\site-packages\\transformers\\tokenization_utils.py:644\u001b[0m, in \u001b[0;36mPreTrainedTokenizer.tokenize.<locals>.split_on_tokens\u001b[1;34m(tok_list, text)\u001b[0m\n\u001b[0;32m    641\u001b[0m             tokenized_text \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m [sub_text]\n\u001b[0;32m    642\u001b[0m     text_list \u001b[38;5;241m=\u001b[39m tokenized_text\n\u001b[1;32m--> 644\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msum\u001b[39m((\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tokenize(token, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs) \u001b[38;5;28;01mif\u001b[39;00m token \u001b[38;5;129;01mnot\u001b[39;00m \\\n\u001b[0;32m    645\u001b[0m         \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madded_tokens_encoder \u001b[38;5;129;01mand\u001b[39;00m token \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mall_special_tokens \\\n\u001b[0;32m    646\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m [token] \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m tokenized_text), [])\n",
            "File \u001b[1;32mc:\\Users\\ykim\\AppData\\Local\\anaconda3\\Lib\\site-packages\\transformers\\tokenization_utils.py:644\u001b[0m, in \u001b[0;36m<genexpr>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    641\u001b[0m             tokenized_text \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m [sub_text]\n\u001b[0;32m    642\u001b[0m     text_list \u001b[38;5;241m=\u001b[39m tokenized_text\n\u001b[1;32m--> 644\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msum\u001b[39m((\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tokenize(token, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs) \u001b[38;5;28;01mif\u001b[39;00m token \u001b[38;5;129;01mnot\u001b[39;00m \\\n\u001b[0;32m    645\u001b[0m         \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madded_tokens_encoder \u001b[38;5;129;01mand\u001b[39;00m token \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mall_special_tokens \\\n\u001b[0;32m    646\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m [token] \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m tokenized_text), [])\n",
            "\u001b[1;31mTypeError\u001b[0m: GPT2Tokenizer._tokenize() got an unexpected keyword argument 'truncation'"
          ]
        }
      ],
      "source": [
        "# Finally, train NLP gpt3.\n",
        "# Starting from \n",
        "train_df = pd.read_csv('remapped_consel_df')\n",
        "\n",
        "train_df[\"input_text\"] = \"Question: \" + train_df[\"questionText\"] + \" Topics: \" + train_df[\"topics\"] + \" Diagnosis: \" + train_df[\"re_diagnosis\"] + \" Response:\"\n",
        "train_df[\"target_text\"] = train_df[\"answerText\"]\n",
        "\n",
        "import torch\n",
        "from transformers import GPT2LMHeadModel, GPT2Tokenizer, AdamW\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "\n",
        "model_name = \"gpt2\"  # Replace with 'gpt3' if using OpenAI API\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
        "model = GPT2LMHeadModel.from_pretrained(model_name)\n",
        "\n",
        "class ChatBotDataSet(Dataset):\n",
        "    def __init__(self, tokenizer, input_texts, target_texts, max_len=128):\n",
        "        self.tokenizer = tokenizer\n",
        "        self.input_texts = input_texts\n",
        "        self.target_texts = target_texts\n",
        "        self.max_len = max_len\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.input_texts)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        input_ids = self.tokenizer.encode(self.input_texts[index], max_length=self.max_len, truncation=True)\n",
        "        target_ids = self.tokenizer.encode(self.target_texts[index], max_length=self.max_len, truncation=True)\n",
        "\n",
        "        # Manually apply padding to reach max_len\n",
        "        input_ids += [self.tokenizer.pad_token_id] * (self.max_len - len(input_ids))\n",
        "        target_ids += [self.tokenizer.pad_token_id] * (self.max_len - len(target_ids))\n",
        "\n",
        "        return {\n",
        "            \"input_ids\": torch.tensor(input_ids),\n",
        "            \"attention_mask\": torch.tensor([1] * len(input_ids)),  # Attention mask set to 1 for non-padded tokens\n",
        "            \"labels\": torch.tensor(target_ids),\n",
        "        }\n",
        "\n",
        "\n",
        "input_texts = train_df[\"input_text\"].tolist()\n",
        "target_texts = train_df[\"target_text\"].tolist()\n",
        "dataset = ChatBotDataSet(tokenizer, input_texts, target_texts)\n",
        "\n",
        "train_loader = DataLoader(dataset, batch_size=2, shuffle=True)\n",
        "\n",
        "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
        "model.train()\n",
        "\n",
        "for epoch in range(3):  # Increase epochs for better performance\n",
        "    for batch in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(\n",
        "            input_ids=batch[\"input_ids\"],\n",
        "            attention_mask=batch[\"attention_mask\"],\n",
        "            labels=batch[\"labels\"]\n",
        "        )\n",
        "        loss = outputs.loss\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        print(f\"Epoch: {epoch}, Loss: {loss.item()}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
