{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Mental Health Counsel Chatbot\n",
        "\n",
        "Kaggle Notebook: Mental Health Counsel Chatbot\n",
        "Description: Provides mental health counseling data, which we used to supplement information from the primary dataset and align topics for consistent categorization.\n",
        "Mental Health Synthetic Dataset\n",
        "\n",
        "Kaggle Dataset: Mental Health Synthetic Dataset\n",
        "Description: This primary dataset contains synthetic data on mental health symptoms, demographics, and treatment, forming the basis for model training and recommendation generation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m_BfAu_XPWub",
        "outputId": "693f18da-464b-47dc-8e40-b70491dae910"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Index(['User ID', 'Age', 'Gender', 'Symptoms', 'Duration (weeks)',\n",
              "       'Previous Diagnosis', 'Therapy History', 'Medication',\n",
              "       'Diagnosis / Condition', 'Suggested Therapy', 'Self-care Advice',\n",
              "       'Urgency Level', 'Mood', 'Stress Level'],\n",
              "      dtype='object')"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "mental_df = pd.read_csv(\"mental_health.csv\")\n",
        "mental_df.columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Index(['User ID', 'Age', 'Gender', 'Symptoms', 'Duration', 'Prev_Diagnosis',\n",
              "       'Therapy_History', 'Medication', 'Diagnosis', 'Suggested_Therapy',\n",
              "       'Self_Care_Advice', 'Urgency_Level', 'Mood', 'Stress_Level'],\n",
              "      dtype='object')"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Renmae columns: easy to follow up.\n",
        "mental_df = mental_df.rename(columns= {\n",
        "    'Diagnosis / Condition' : 'Diagnosis',\n",
        "    'Self-care Advice': 'Self_Care_Advice',\n",
        "    'Therapy History' : 'Therapy_History',\n",
        "    'Stress Level' : 'Stress_Level',\n",
        "    'Urgency Level' : 'Urgency_Level',\n",
        "    'Suggested Therapy': 'Suggested_Therapy',\n",
        "    'Duration (weeks)': 'Duration',\n",
        "    'Previous Diagnosis': 'Prev_Diagnosis'\n",
        "})\n",
        "\n",
        "# Check renamed columns\n",
        "mental_df.columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "1rl9DKaPRJ4-"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Symtoms unique values \n",
            " ['feeling anxious' 'excessive worry' 'trouble sleeping'\n",
            " 'loss of interest in activities' 'panic attacks' 'lack of concentration'\n",
            " 'feeling irritable' 'feeling sad' 'feeling overwhelmed']\n",
            "therapy_unique_values \n",
            " ['Support Groups' 'Cognitive Behavioral Therapy' 'Psychotherapy'\n",
            " 'Mindfulness-Based Therapy' 'No Therapy Needed']\n",
            "diagnosis_unique_values \n",
            " ['Panic Disorder' 'Depression' 'Anxiety' 'Burnout' 'Stress']\n"
          ]
        }
      ],
      "source": [
        "symptoms_unique_values = mental_df['Symptoms'].unique()\n",
        "therapy_unique_values = mental_df['Suggested_Therapy'].unique()\n",
        "diagnosis_unique_values = mental_df['Diagnosis'].unique()\n",
        "\n",
        "print(\"Symtoms unique values \\n\", symptoms_unique_values)\n",
        "print(\"therapy_unique_values \\n\", therapy_unique_values)\n",
        "print(\"diagnosis_unique_values \\n\", diagnosis_unique_values)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "A label encoder is a technique that converts non-numerical data into numerical values, \n",
        "which is useful for machine learning and data analysis. <br>\n",
        "It's often used when working with categorical data, such as ordinal data, \n",
        "where there's a hierarchy among the values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import classification_report\n",
        "# Target supposed to be Target Supposed.\n",
        "#  th\n",
        "def build_self_test_self_care_advice():\n",
        "    le_diagnosis = LabelEncoder()\n",
        "    le_symtoms = LabelEncoder()\n",
        "    le_self_care = LabelEncoder()\n",
        "    le_therapy = LabelEncoder()\n",
        "    # mental_df['Duration'] is already Int style, we don't have to encode it\n",
        "    \n",
        "    mental_df['Diagnosis_encoded'] = le_diagnosis.fit_transform(mental_df['Diagnosis'])\n",
        "    mental_df['Symptoms_encoded'] = le_symtoms.fit_transform(mental_df['Symptoms'])\n",
        "    mental_df['Self_Care_Advice_encoded'] = le_self_care.fit_transform(mental_df['Self_Care_Advice'])\n",
        "    mental_df['Suggested_Therapy_encoded'] = le_therapy.fit_transform(mental_df['Suggested_Therapy'])\n",
        "\n",
        "    # Training Data\n",
        "    X = mental_df[['Diagnosis_encoded', 'Symptoms_encoded']]\n",
        "    y_self_care = mental_df['Self_Care_Advice_encoded']\n",
        "    y_therapy = mental_df['Suggested_Therapy_encoded']\n",
        "\n",
        "    X_train, X_test, y_self_care_train, y_self_care_test, y_therapy_train, y_therapy_test = train_test_split(X, y_self_care, y_therapy, test_size=0.2, random_state=42)\n",
        "    # Train models for Self Care Advice and Suggested Therapy\n",
        "    model_self_care = RandomForestClassifier()\n",
        "    model_therapy = RandomForestClassifier()\n",
        "\n",
        "    model_self_care.fit(X_train, y_self_care_train)\n",
        "    model_therapy.fit(X_train, y_therapy_train)\n",
        "\n",
        "    # Make predictions\n",
        "    self_care_pred = model_self_care.predict(X_test)\n",
        "    therapy_pred = model_therapy.predict(X_test)\n",
        "\n",
        "    # Display classification reports\n",
        "    print(\"Self Care Advice Classification Report:\")\n",
        "    self_care_report = classification_report(y_self_care_test, self_care_pred, target_names=le_self_care.classes_)\n",
        "    print(self_care_report)\n",
        "    print(\"\\nSuggested Therapy Classification Report:\")\n",
        "    self_therapy_report = classification_report(y_therapy_test, therapy_pred, target_names=le_therapy.classes_)\n",
        "    print(self_therapy_report)\n",
        "    return model_self_care , model_therapy, le_diagnosis, le_symtoms, le_self_care, le_therapy, self_care_report, self_therapy_report"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Initial Reports\n",
        "\n",
        "### Self Care Advice Classification Report\n",
        "\n",
        "| Self Care Advice       | Precision | Recall | F1-Score | Support |\n",
        "|------------------------|-----------|--------|----------|---------|\n",
        "| Breathing Exercises    | 0.20      | 0.17   | 0.18     | 166     |\n",
        "| Exercise               | 0.17      | 0.24   | 0.20     | 181     |\n",
        "| Journaling             | 0.32      | 0.14   | 0.20     | 191     |\n",
        "| Meditation             | 0.11      | 0.03   | 0.04     | 116     |\n",
        "| Take Breaks            | 0.17      | 0.33   | 0.23     | 164     |\n",
        "| Talk to a Friend       | 0.22      | 0.21   | 0.21     | 182     |\n",
        "| **Accuracy**           |           |        | 0.19     | 1000    |\n",
        "| **Macro Avg**          | 0.20      | 0.19   | 0.18     | 1000    |\n",
        "| **Weighted Avg**       | 0.21      | 0.19   | 0.18     | 1000    |\n",
        "\n",
        "### Suggested Therapy Classification Report\n",
        "\n",
        "| Suggested Therapy               | Precision | Recall | F1-Score | Support |\n",
        "|---------------------------------|-----------|--------|----------|---------|\n",
        "| Cognitive Behavioral Therapy    | 0.16      | 0.13   | 0.14     | 200     |\n",
        "| Mindfulness-Based Therapy       | 0.14      | 0.03   | 0.05     | 189     |\n",
        "| No Therapy Needed               | 0.17      | 0.09   | 0.11     | 187     |\n",
        "| Psychotherapy                   | 0.17      | 0.31   | 0.22     | 202     |\n",
        "| Support Groups                  | 0.24      | 0.34   | 0.28     | 222     |\n",
        "| **Accuracy**                    |           |        | 0.19     | 1000    |\n",
        "| **Macro Avg**                   | 0.17      | 0.18   | 0.16     | 1000    |\n",
        "| **Weighted Avg**                | 0.18      | 0.19   | 0.17     | 1000    |\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [],
      "source": [
        "# model_self_care , model_therapy, le_diagnosis, le_symtoms, le_self_care, le_therapy, self_care_report, self_therapy_report = build_model()\n",
        "# Save model for purpose\n",
        "import joblib\n",
        "def save_model(model_self_care , model_therapy, le_diagnosis, le_symtoms, le_self_care, le_therapy ):\n",
        "    joblib.dump(model_self_care, \"model_self_care.pkl\")\n",
        "    joblib.dump(model_therapy, \"model_therapy.pkl\")\n",
        "    joblib.dump(le_diagnosis, \"le_diagnosis.pkl\")\n",
        "    joblib.dump(le_symtoms, \"le_symtoms.pkl\")\n",
        "    joblib.dump(le_self_care, \"le_self_care.pkl\")\n",
        "    joblib.dump(le_therapy, \"le_therapy.pkl\")\n",
        "\n",
        "# save_model(model_self_care , model_therapy, le_diagnosis, le_symtoms, le_self_care, le_therapy)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Current accuracy 0.18 precison also,\n",
        "To imporve this models. <br>\n",
        "There are serveral ways to imporve ways.<br>\n",
        "Add more parameters (which contains demographic infomrationm which user can simply input them), also re mapping based Diagnosis.<br>\n",
        "I created three value includes 2 informaiton.<br>\n",
        "It wil help to organize better modeling. <br>\n",
        "Current features has <br>\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['Other' 'Female' 'Non-binary' 'Male']\n",
            "[29 37 47 35 22  8 31 20 21  9 38 43 30  2  3 34 36 19 33 23 46 49 48 41\n",
            " 11 16 10 45 13  4 39 12 51 24 17 32  1 14 44 15 26  5 40 27 42 18  6 50\n",
            " 28 25  7]\n",
            "['Moderate' 'High' 'Low' 'Critical']\n",
            "[ 1  4  5  2  6  9 10  8  7  3]\n",
            "['OCD' 'None' 'PTSD' 'Bipolar Disorder' 'Anxiety' 'Depression']\n"
          ]
        }
      ],
      "source": [
        "print(mental_df['Gender'].unique())\n",
        "print(mental_df['Duration'].unique())\n",
        "print(mental_df['Urgency_Level'].unique())\n",
        "print(mental_df['Stress_Level'].unique())\n",
        "print(mental_df['Prev_Diagnosis'].unique())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Diagnosis Group Classification Report:\n",
            "                          precision    recall  f1-score   support\n",
            "\n",
            "       Anxiety Disorders       0.46      0.57      0.51       422\n",
            "          Mood Disorders       0.46      0.45      0.45       440\n",
            "Stress-Related Disorders       0.10      0.04      0.05       138\n",
            "\n",
            "                accuracy                           0.44      1000\n",
            "               macro avg       0.34      0.35      0.34      1000\n",
            "            weighted avg       0.41      0.44      0.42      1000\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Diagnosis Group\n",
        "\n",
        "def group_diagnosis(row):\n",
        "    if row['Diagnosis'] in ['Panic Disorder', 'Anxiety']:\n",
        "        return 'Anxiety Disorders'\n",
        "    elif row['Diagnosis'] in ['Depression', 'Burnout']:\n",
        "        return 'Mood Disorders'\n",
        "    elif row['Diagnosis'] == 'Stress':\n",
        "        return 'Stress-Related Disorders'\n",
        "def group_prev_diagnosis(row):\n",
        "    if row['Prev_Diagnosis'] in ['Panic Disorder', 'Anxiety', 'OCD']:\n",
        "        return 'Anxiety Disorders'\n",
        "    elif row['Prev_Diagnosis'] in ['Depression', 'Bipolar Disorder']:\n",
        "        return 'Mood Disorders'\n",
        "    elif row['Prev_Diagnosis'] in ['Stress', 'PTSD']:\n",
        "        return 'Stress-Related Disorders'\n",
        "    else:\n",
        "        return ''\n",
        "\n",
        "def re_map_gender(row):\n",
        "    if row['Gender'] == 'Male':\n",
        "        return 1\n",
        "    elif row['Gender'] == 'Female':\n",
        "        return 2\n",
        "    else:\n",
        "        return 3\n",
        "def re_map_urgency_level(row):\n",
        "    if row[\"Urgency_Level\"] == \"Low\":\n",
        "        return 1\n",
        "    elif row[\"Urgency_Level\"] == \"Moderate\":\n",
        "        return 2\n",
        "    elif row[\"Urgency_Level\"] == \"High\":\n",
        "        return 3\n",
        "    else:\n",
        "        return 4\n",
        "    \n",
        "# Inital diagnosis model\n",
        "def improved_test_reports_diagnosis(mental_df):\n",
        "    # Diagnosis Group\n",
        "    mental_df[\"Diagnosis_Group\"] = mental_df.apply(group_diagnosis, axis=1)\n",
        "    mental_df['Prev_Diagnosis_Group'] = mental_df.apply(group_prev_diagnosis, axis=1)\n",
        "    mental_df[\"Re_Gender\"] = mental_df.apply(re_map_gender, axis=1)\n",
        "    mental_df[\"Urgency_Level\"] = mental_df.apply(re_map_urgency_level, axis=1)\n",
        "    \n",
        "    le_diagnosis_group = LabelEncoder()\n",
        "    le_prev_Diagnosis_group = LabelEncoder()\n",
        "    le_symtoms = LabelEncoder()\n",
        "\n",
        "    mental_df['Diagnosis_Group_encoded'] = le_diagnosis_group.fit_transform(mental_df['Diagnosis_Group'])\n",
        "    mental_df['Prev_Diagnosis_Group_encoded'] =  le_prev_Diagnosis_group.fit_transform(mental_df['Prev_Diagnosis'])\n",
        "    mental_df['Symptoms_encoded'] = le_symtoms.fit_transform(mental_df['Symptoms'])\n",
        "\n",
        "    # Training Data with duration\n",
        "    X = mental_df[['Age', 'Symptoms_encoded', \"Re_Gender\", \"Prev_Diagnosis_Group_encoded\", \"Duration\", \"Stress_Level\", \"Urgency_Level\"]] \n",
        "    y_diagnosis = mental_df['Diagnosis_Group_encoded']\n",
        "\n",
        "    X_train, X_test, y_diagnosis_train, y_diagnosis_test = train_test_split(X, y_diagnosis, test_size=0.2, random_state=42)\n",
        "    \n",
        "    # Train model for Diagnosis\n",
        "    model_diagnosis = RandomForestClassifier()\n",
        "    model_diagnosis.fit(X_train, y_diagnosis_train)\n",
        "\n",
        "    # Make predictions\n",
        "    diagnos_pred = model_diagnosis.predict(X_test)\n",
        "\n",
        "    # Display classification reports\n",
        "    print(\"Diagnosis Group Classification Report:\")\n",
        "    diagnosis_report = classification_report(y_diagnosis_test, diagnos_pred, target_names=le_diagnosis_group.classes_)\n",
        "    print(diagnosis_report)\n",
        "    \n",
        "    return model_diagnosis, le_diagnosis_group, le_prev_Diagnosis_group, le_symtoms, diagnosis_report\n",
        "\n",
        "model_diagnosis, le_diagnosis_group, le_prev_Diagnosis_group, le_symtoms, diagnosis_report = improved_test_reports_diagnosis(mental_df)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model saved to diagnosisModel\n"
          ]
        }
      ],
      "source": [
        "import joblib\n",
        "import os\n",
        "\n",
        "# Download Model\n",
        "def createModel(model_diagnosis, le_diagnosis_group, le_prev_Diagnosis_group, le_symtoms):\n",
        "      # Save the model to the diagnosisModel folder\n",
        "    os.makedirs('diagnosisModel', exist_ok=True)\n",
        "    model_path = 'diagnosisModel'\n",
        "    joblib.dump(model_diagnosis, 'diagnosisModel/diagnosis_model.pkl')\n",
        "    joblib.dump(le_diagnosis_group, 'diagnosisModel/le_diagnosis_group.pkl')\n",
        "    joblib.dump(le_prev_Diagnosis_group, 'diagnosisModel/le_prev_Diagnosis_group.pkl')\n",
        "    joblib.dump(le_symtoms, 'diagnosisModel/le_symptoms.pkl')\n",
        "    print(f\"Model saved to {model_path}\")\n",
        "    \n",
        "createModel(model_diagnosis, le_diagnosis_group, le_prev_Diagnosis_group, le_symtoms)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Diagnosis Group Classification Report (SVM):\n",
            "                          precision    recall  f1-score   support\n",
            "\n",
            "       Anxiety Disorders       0.42      1.00      0.59       422\n",
            "          Mood Disorders       0.00      0.00      0.00       440\n",
            "Stress-Related Disorders       0.00      0.00      0.00       138\n",
            "\n",
            "                accuracy                           0.42      1000\n",
            "               macro avg       0.14      0.33      0.20      1000\n",
            "            weighted avg       0.18      0.42      0.25      1000\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\ykim\\AppData\\Local\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "c:\\Users\\ykim\\AppData\\Local\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "c:\\Users\\ykim\\AppData\\Local\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "(SVC(kernel='poly'),\n",
              " LabelEncoder(),\n",
              " '                          precision    recall  f1-score   support\\n\\n       Anxiety Disorders       0.42      1.00      0.59       422\\n          Mood Disorders       0.00      0.00      0.00       440\\nStress-Related Disorders       0.00      0.00      0.00       138\\n\\n                accuracy                           0.42      1000\\n               macro avg       0.14      0.33      0.20      1000\\n            weighted avg       0.18      0.42      0.25      1000\\n')"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from sklearn.svm import SVC\n",
        "def improved_test_reports_diagnosis_svm(mental_df):\n",
        "    # Same preprocessing steps\n",
        "    mental_df[\"Diagnosis_Group\"] = mental_df.apply(group_diagnosis, axis=1)\n",
        "    mental_df['Prev_Diagnosis_Group'] = mental_df.apply(group_prev_diagnosis, axis=1)\n",
        "    mental_df[\"Re_Gender\"] = mental_df.apply(re_map_gender, axis=1)\n",
        "    mental_df[\"Urgency_Level\"] = mental_df.apply(re_map_urgency_level, axis=1)\n",
        "\n",
        "    le_diagnosis_group = LabelEncoder()\n",
        "    le_prev_Diagnosis_group = LabelEncoder()\n",
        "    le_symtoms = LabelEncoder()\n",
        "\n",
        "    mental_df['Diagnosis_Group_encoded'] = le_diagnosis_group.fit_transform(mental_df['Diagnosis_Group'])\n",
        "    mental_df['Prev_Diagnosis_Group_encoded'] = le_prev_Diagnosis_group.fit_transform(mental_df['Prev_Diagnosis'])\n",
        "    mental_df['Symptoms_encoded'] = le_symtoms.fit_transform(mental_df['Symptoms'])\n",
        "\n",
        "    X = mental_df[['Age', 'Symptoms_encoded', \"Re_Gender\", \"Prev_Diagnosis_Group_encoded\", \"Duration\", \"Stress_Level\", \"Urgency_Level\"]] \n",
        "    y_diagnosis = mental_df['Diagnosis_Group_encoded']\n",
        "\n",
        "    X_train, X_test, y_diagnosis_train, y_diagnosis_test = train_test_split(X, y_diagnosis, test_size=0.2, random_state=42)\n",
        "\n",
        "    # Train model using Support Vector Machine\n",
        "    model_diagnosis = SVC(kernel='poly')  # You can also try 'rbf' or 'poly'\n",
        "    model_diagnosis.fit(X_train, y_diagnosis_train)\n",
        "\n",
        "    # Make predictions\n",
        "    diagnos_pred = model_diagnosis.predict(X_test)\n",
        "\n",
        "    # Display classification report\n",
        "    print(\"Diagnosis Group Classification Report (SVM):\")\n",
        "    diagnosis_report = classification_report(y_diagnosis_test, diagnos_pred, target_names=le_diagnosis_group.classes_)\n",
        "    print(diagnosis_report)\n",
        "\n",
        "    return model_diagnosis, le_diagnosis_group, diagnosis_report\n",
        "\n",
        "improved_test_reports_diagnosis_svm(mental_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to\n",
            "[nltk_data]     C:\\Users\\ykim\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'Panic': ['panic', 'affright', 'terror', 'scare'], 'Disorder': ['disquiet', 'perturb', 'unhinge', 'disorderliness', 'upset', 'disarray', 'disorder', 'trouble', 'cark', 'distract'], 'Depression': ['Depression', 'slump', 'depressive_disorder', 'economic_crisis', 'natural_depression', 'clinical_depression', 'impression', 'Great_Depression', 'low', 'imprint', 'depression'], 'Burnout': [], 'Stress': ['emphasis', 'accent', 'emphasize', 'tension', 'emphasise', 'accentuate', 'punctuate', 'tenseness', 'try', 'stress', 'strain', 'focus']}\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "from nltk.corpus import wordnet\n",
        "from itertools import chain\n",
        "\n",
        "# Based on questions and topics, Create chains then finds vlaues.\n",
        "def find_related_words():\n",
        "    nltk.download('wordnet')\n",
        "\n",
        "    words = {\n",
        "        'Panic': 'panic',\n",
        "        'Disorder': 'disorder',\n",
        "        'Depression': 'depression',\n",
        "        'Burnout': 'burnout',\n",
        "        'Stress': 'stress'\n",
        "    }\n",
        "\n",
        "    # Initialize a dictionary to store the results\n",
        "    related_words = {}\n",
        "\n",
        "    for key, word in words.items():\n",
        "        word_synsets = wordnet.synsets(word)\n",
        "        similar_words = set(chain(*[synset.lemma_names() for synset in word_synsets]))\n",
        "        related_words[key] = list(similar_words)\n",
        "\n",
        "    return related_words\n",
        "\n",
        "# Get the related words and print the dictionary\n",
        "similar_words_dict = find_related_words()\n",
        "print(similar_words_dict)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Result for related words\n",
        "# {'Panic': ['panic', 'scare', 'terror', 'affright'], \n",
        "# 'Disorder': ['distract', 'upset', 'trouble', 'disorderliness', 'cark', 'disarray', 'disorder', 'perturb', 'disquiet', 'unhinge'], \n",
        "# 'Depression': ['impression', 'clinical_depression', 'slump', 'Great_Depression', 'Depression', 'low', 'depressive_disorder', 'natural_depression', 'imprint', 'economic_crisis', 'depression'], \n",
        "# 'Burnout': [], \n",
        "# 'Stress': ['strain', 'emphasis', 'emphasize', 'tension', 'punctuate', 'focus', 'accentuate', 'try', 'tenseness', 'emphasise', 'stress', 'accent']}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Counsel data training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Orignal length1482\n",
            "\n",
            "Word frequencies in the 'topics' column:\n",
            "Family :  119\n",
            "Conflict :  91\n",
            "Substance :  14\n",
            "AbuseAddiction :  9\n",
            "Behavioral :  49\n",
            "ChangeSocial :  3\n",
            "Relationships :  214\n",
            "Relationship :  45\n",
            "Dissolution :  68\n",
            "Anger :  32\n",
            "Management :  25\n",
            "Sleep :  10\n",
            "Improvement :  17\n",
            "Professional :  34\n",
            "EthicsLegal :  6\n",
            "& :  45\n",
            "Regulatory :  20\n",
            "Social :  41\n",
            "RelationshipsMarriage :  11\n",
            "MarriageIntimacy :  26\n",
            "Domestic :  9\n",
            "ViolenceAnger :  2\n",
            "ManagementFamily :  3\n",
            "Human :  50\n",
            "Sexuality :  33\n",
            "ManagementSleep :  2\n",
            "Military :  3\n",
            "Issues :  10\n",
            "RelationshipsDomestic :  3\n",
            "Violence :  10\n",
            "ViolenceRelationship :  1\n",
            "Marriage :  25\n",
            "Grief :  20\n",
            "and :  23\n",
            "Loss :  9\n",
            "ConflictChildren :  1\n",
            "Adolescents :  9\n",
            "MarriageRelationship :  4\n",
            "TraumaHuman :  1\n",
            "RelationshipsIntimacy :  42\n",
            "ManagementParenting :  1\n",
            "Intimacy :  17\n",
            "Workplace :  9\n",
            "SexualityMarriage :  3\n",
            "LGBTQ :  29\n",
            "SpiritualityFamily :  2\n",
            "Ethics :  23\n",
            "ViolenceRelationships :  1\n",
            "ConflictRelationships :  5\n",
            "Self-esteem :  29\n",
            "Self-esteemRelationships :  12\n",
            "Parenting :  35\n",
            "ConflictMarriage :  6\n",
            "ConflictSelf-esteem :  3\n",
            "ParentingRelationships :  1\n",
            "nan :  10\n",
            "Counseling :  53\n",
            "RelationshipsSelf-esteem :  9\n",
            "Eating :  7\n",
            "RelationshipsProfessional :  2\n",
            "ParentingSubstance :  1\n",
            "AbuseSpirituality :  1\n",
            "Self-esteemRelationship :  1\n",
            "ConflictAnger :  2\n",
            "ParentingAnger :  1\n",
            "MarriageFamily :  3\n",
            "ConflictProfessional :  2\n",
            "RelationshipsHuman :  4\n",
            "SexualityLGBTQ :  10\n",
            "RelationshipsParentingFamily :  1\n",
            "Legal :  16\n",
            "LGBTQIntimacy :  1\n",
            "ManagementRelationships :  3\n",
            "AbuseFamily :  1\n",
            "Self-esteemMarriageTraumaIntimacy :  3\n",
            "MarriageAddiction :  1\n",
            "RelationshipsLegal :  1\n",
            "SexualityRelationships :  4\n",
            "ConflictRelationshipsMarriage :  1\n",
            "MarriageAnger :  1\n",
            "RelationshipsFamily :  8\n",
            "Change :  56\n",
            "SexualitySocial :  1\n",
            "Self-esteemEating :  1\n",
            "Career :  2\n",
            "CounselingProfessional :  2\n",
            "MarriageGrief :  1\n",
            "Self-esteemSocial :  5\n",
            "AddictionSubstance :  11\n",
            "Abuse :  23\n",
            "Spirituality :  8\n",
            "RelationshipsSocial :  22\n",
            "SexualityAddiction :  2\n",
            "IntimacyRelationships :  11\n",
            "RelationshipsSelf-esteemHuman :  1\n",
            "Trauma :  28\n",
            "SexualityIntimacyMarriage :  9\n",
            "RelationshipsParenting :  1\n",
            "ConflictParenting :  16\n",
            "RegulatoryProfessional :  3\n",
            "ManagementDomestic :  2\n",
            "ParentingFamily :  6\n",
            "ConflictLegal :  6\n",
            "ViolenceLegal :  1\n",
            "IntimacyHuman :  7\n",
            "IntimacySocial :  1\n",
            "TraumaMilitary :  3\n",
            "ManagementRelationshipsSocial :  3\n",
            "TraumaFamily :  4\n",
            "TraumaSelf-esteemRelationship :  1\n",
            "RelationshipsRelationship :  2\n",
            "RelationshipsBehavioral :  6\n",
            "MarriageDomestic :  1\n",
            "ConflictParentingMarriage :  1\n",
            "MarriageIntimacyHuman :  9\n",
            "MarriageRelationshipsIntimacy :  9\n",
            "SexualityFamily :  1\n",
            "ConflictSpirituality :  1\n",
            "RelationshipsIntimacyHuman :  1\n",
            "ParentingRelationship :  9\n",
            "RelationshipsTrauma :  1\n",
            "AddictionMarriageIntimacy :  3\n",
            "ConflictTrauma :  1\n",
            "ConflictSocial :  1\n",
            "RelationshipsRelationshipsIntimacy :  1\n",
            "LGBTQRelationshipsIntimacy :  1\n",
            "MarriageSocial :  1\n",
            "ManagementSocial :  1\n",
            "RelationshipsRelationships :  1\n",
            "ChangeMarriage :  1\n",
            "ManagementBehavioral :  1\n",
            "ViolenceMarriage :  1\n",
            "LGBTQFamily :  4\n",
            "IntimacyMarriage :  1\n",
            "ConflictRelationshipsIntimacy :  1\n",
            "ConflictLGBTQ :  6\n",
            "SpiritualityRelationships :  2\n",
            "RelationshipsWorkplace :  5\n",
            "SexualityIntimacyRelationships :  14\n",
            "SexualityIntimacy :  8\n",
            "RegulatoryAddiction :  2\n",
            "RelationshipsSubstance :  8\n",
            "Self-esteemSleep :  1\n",
            "RelationshipsChildren :  1\n",
            "ChangeLGBTQ :  1\n",
            "LossFamily :  2\n",
            "Self-esteemBehavioral :  2\n",
            "RelationshipsRelationshipsAddiction :  1\n",
            "DiagnosisCounseling :  4\n",
            "Fundamentals :  87\n",
            "RelationshipsLGBTQ :  2\n",
            "Self-esteemLGBTQ :  1\n",
            "EthicsParentingLegal :  1\n",
            "TraumaRelationships :  8\n",
            "LGBTQHuman :  4\n",
            "IntimacyTrauma :  2\n",
            "ViolenceSleep :  2\n",
            "LossSubstance :  7\n",
            "AbuseTrauma :  7\n",
            "IntimacyRelationshipsHuman :  11\n",
            "RelationshipsCareer :  2\n",
            "ChangeRelationships :  1\n",
            "Addiction :  3\n",
            "ConflictDomestic :  1\n",
            "Alzheimer'sFamily :  2\n",
            "SexualityRelationshipsIntimacy :  2\n",
            "ChangeSleep :  1\n",
            "SpiritualitySocial :  6\n",
            "RelationshipsIntimacyLGBTQ :  5\n",
            "MarriageHuman :  2\n",
            "MarriageIntimacyAddictionBehavioral :  1\n",
            "LossRelationships :  2\n",
            "ConflictRelationship :  1\n",
            "IntimacyRelationshipsDomestic :  1\n",
            "AddictionProfessional :  2\n",
            "ConflictParentingRelationship :  1\n",
            "Diagnosis :  8\n",
            "RegulatoryParentingFamily :  5\n",
            "AbuseSocial :  4\n",
            "RelationshipsAddictionSubstance :  2\n",
            "AbuseSelf-esteem :  1\n",
            "Self-esteemSpirituality :  1\n",
            "SexualityTrauma :  1\n",
            "SexualityTraumaIntimacyRelationships :  1\n",
            "LossTrauma :  1\n",
            "Self-esteemSubstance :  1\n",
            "RelationshipsMarriageWorkplace :  3\n",
            "RelationshipsMilitary :  4\n",
            "RegulatorySubstance :  5\n",
            "ConflictRelationshipsParenting :  2\n",
            "ManagementRelationshipsSubstance :  1\n",
            "RelationshipsIntimacySpirituality :  5\n",
            "SexualityLGBTQIntimacy :  3\n",
            "ParentingChildren :  4\n",
            "Children :  3\n",
            "EthicsCounseling :  15\n",
            "ConflictRelationshipsRelationship :  1\n",
            "AdolescentsBehavioral :  1\n",
            "RelationshipsAddiction :  2\n",
            "ConflictAddictionSubstance :  1\n",
            "ParentingAddiction :  1\n",
            "ManagementSelf-esteemMarriageFamily :  1\n",
            "ConflictParentingChildren :  1\n",
            "total counts1482\n",
            "stress_count 54\n",
            "depression_count 196\n",
            "disorder_count 12\n",
            "anxiety_count 180\n",
            "burn_out_count 0\n",
            "Found selcted count 442\n"
          ]
        }
      ],
      "source": [
        "counsel_df = pd.read_csv(\"counselchat-data.csv\")\n",
        "\n",
        "from collections import Counter\n",
        "# Ensure you've downloaded the WordNet corpus\n",
        "# Display the word frequencies\n",
        "def get_word_frequencies(counsel_df):\n",
        "    print(\"Orignal length\" + str(len(counsel_df)))\n",
        "    counsel_df = counsel_df[['questionText', 'topics','answerText']]\n",
        "    all_words = ' '.join(counsel_df['topics'].astype(str)).replace(',', '').split()\n",
        "    # Count the frequency of each word\n",
        "    word_count = Counter(all_words)\n",
        "    print(\"\\nWord frequencies in the 'topics' column:\")\n",
        "    found_selected_count = 0\n",
        "    stress_count = 0\n",
        "    depression_count = 0\n",
        "    disorder_count = 0\n",
        "    anxiety_count = 0\n",
        "    burn_out_count = 0\n",
        "    for word, count in word_count.items():\n",
        "        if word.__contains__(\"Stress\"):\n",
        "            stress_count +=count\n",
        "        elif word.__contains__(\"Depression\"):\n",
        "            depression_count += count\n",
        "        elif word.__contains__(\"Disorder\"):\n",
        "            disorder_count += count\n",
        "        elif word.__contains__(\"Anxiety\"):\n",
        "            anxiety_count += count\n",
        "        elif word.__contains__(\"Burnout\"):\n",
        "            burn_out_count += count\n",
        "        else:\n",
        "            print(f\"{word} :  {count}\")\n",
        "    found_selected_count = stress_count + depression_count + disorder_count + anxiety_count + burn_out_count\n",
        "    print(\"total counts\" + str(len(counsel_df)))\n",
        "    print(\"stress_count\", stress_count)\n",
        "    print(\"depression_count\", depression_count)\n",
        "    print(\"disorder_count\", disorder_count)\n",
        "    print(\"anxiety_count\", anxiety_count)\n",
        "    print(\"burn_out_count\", burn_out_count)\n",
        "    print(\"Found selcted count\", found_selected_count)\n",
        "\n",
        "get_word_frequencies(counsel_df)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training Accuracy: 0.9976133651551312\n",
            "Number of rows in test set: 1063\n"
          ]
        }
      ],
      "source": [
        "# https://my.clevelandclinic.org/health/diseases/22295-mental-health-disorders\n",
        "\n",
        "import nltk\n",
        "from nltk.corpus import wordnet\n",
        "\n",
        "counsel_df = pd.read_csv(\"counselchat-data.csv\")\n",
        "target_Keywords = {\n",
        "    'Anxiety Disorders': ['panic disorder', 'anxiety'],\n",
        "    'Mood Disorders': ['depression', 'burnout'],\n",
        "    'Stress-Related Disorders': ['stress', 'PTSD']\n",
        "}\n",
        "\n",
        "similar_words_dict = {\n",
        "    'panic': ['panic', 'scare', 'terror', 'affright'],\n",
        "    'disorder': ['distract', 'upset', 'trouble', 'disorderliness', 'cark', 'disarray', 'disorder', 'perturb', 'disquiet', 'unhinge'],\n",
        "    'depression': ['impression', 'clinical_depression', 'slump', 'Great_Depression', 'depression', 'low', 'depressive_disorder', 'natural_depression', 'imprint', 'economic_crisis', 'depression'],\n",
        "    'burnout': [],\n",
        "    'stress': ['strain', 'emphasis', 'emphasize', 'tension', 'punctuate', 'focus', 'accentuate', 'try', 'tenseness', 'emphasise', 'stress', 'accent']\n",
        "}\n",
        "\n",
        "def group_diagnosis(row):\n",
        "    all_words = str(row['topics']).lower().split()\n",
        "\n",
        "    for disorder, keywords in target_Keywords.items():\n",
        "        for keyword in keywords:\n",
        "            if any(keyword in word for word in all_words):\n",
        "                return disorder\n",
        "            if keyword in similar_words_dict:\n",
        "                # Check if any of the similar words match\n",
        "                if any(sim_word in all_words for sim_word in similar_words_dict[keyword]):\n",
        "                    return disorder\n",
        "    return None\n",
        "# Original 120.\n",
        "# print(similar_words_dict)\n",
        "# Adding new counsel_df based on items\n",
        "counsel_df[\"re_diagnosis\"] = counsel_df.apply(group_diagnosis, axis=1)\n",
        "# print(counsel_df['re_diagnosis'])\n",
        "# print(len(counsel_df['re_diagnosis']))\n",
        "\n",
        "# cd remapping diagnosis.\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Function to create and train the model\n",
        "# TfidfVectorizer\n",
        "# LogicRegression\n",
        "def create_diagnosis_model(train_data, target_column='re_diagnosis'):\n",
        "    # Extract text and target columns\n",
        "    X_train = train_data['topics']\n",
        "    y_train = train_data[target_column]\n",
        "\n",
        "    # TF-IDF vectorization\n",
        "    tfidf = TfidfVectorizer(max_features=5000)\n",
        "    X_train_tfidf = tfidf.fit_transform(X_train)\n",
        "\n",
        "    # Train a logistic regression model\n",
        "    model = LogisticRegression(max_iter=1000)\n",
        "    model.fit(X_train_tfidf, y_train)\n",
        "\n",
        "    # Evaluate model accuracy on the training set\n",
        "    X_train_pred = model.predict(X_train_tfidf)\n",
        "    accuracy = accuracy_score(y_train, X_train_pred)\n",
        "    print(f\"Training Accuracy: {accuracy}\")\n",
        "\n",
        "    return model, tfidf\n",
        "\n",
        "# Function to make predictions using the trained model\n",
        "def predict_missing_diagnoses(df, model, tfidf, target_column='re_diagnosis'):\n",
        "    # Extarct out df re_diagnosis is None\n",
        "    df_test = df[df[target_column].isna()]\n",
        "\n",
        "    X_test = df_test['topics']\n",
        "    print(f\"Number of rows in test set: {len(X_test)}\")\n",
        "    X_test = df_test['topics'].fillna('')  # Replace NaN values with an empty string\n",
        "    non_empty_mask = X_test.str.strip() != ''\n",
        "\n",
        "    # 한번더 걸러준다\n",
        "    df_test = df_test[non_empty_mask]\n",
        "    X_test =  X_test[non_empty_mask]\n",
        "\n",
        "    # \n",
        "    X_test_tfidf = tfidf.transform(X_test)\n",
        "\n",
        "    # Predict missing diagnoses\n",
        "    predictions = model.predict(X_test_tfidf)\n",
        "\n",
        "    # Assign predictions back to the DataFrame\n",
        "    df.loc[df[target_column].isna() & non_empty_mask, target_column] = predictions\n",
        "\n",
        "    return df\n",
        "\n",
        "# Example usage\n",
        "# Step 1: Create model\n",
        "# Training and prediction process\n",
        "df_train = counsel_df[counsel_df['re_diagnosis'].notna()]\n",
        "model, tfidf = create_diagnosis_model(df_train)\n",
        "remapped_consel_df = predict_missing_diagnoses(counsel_df, model, tfidf)\n",
        "\n",
        "\n",
        "\n",
        "# remapped_consel_df.to_csv(\"remapped_consel_df\", index=False)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Environment path: c:\\Users\\ykim\\AppData\\Local\\anaconda3\\python.exe\n"
          ]
        }
      ],
      "source": [
        "import sys\n",
        "print(\"Environment path:\", sys.executable)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1482\n",
            "Index(['questionID', 'questionTitle', 'questionText', 'questionUrl', 'topics',\n",
            "       'therapistName', 'therapistUrl', 'answerText', 'upvotes',\n",
            "       're_diagnosis'],\n",
            "      dtype='object')\n"
          ]
        }
      ],
      "source": [
        "print(len(remapped_consel_df))\n",
        "print(remapped_consel_df.columns)\n",
        "remapped_consel_df = remapped_consel_df[['questionText', 'topics', 're_diagnosis', 'answerText']]\n",
        "remapped_consel_df.to_csv(\"remapped_consel_df\", index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [
        {
          "ename": "RuntimeError",
          "evalue": "Failed to import transformers.models.gpt2.modeling_gpt2 because of the following error (look up to see its traceback):\noperator torchvision::nms does not exist",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\utils\\import_utils.py:1778\u001b[0m, in \u001b[0;36m_LazyModule._get_module\u001b[1;34m(self, module_name)\u001b[0m\n\u001b[0;32m   1777\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1778\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m importlib\u001b[38;5;241m.\u001b[39mimport_module(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m module_name, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m)\n\u001b[0;32m   1779\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
            "File \u001b[1;32mc:\\Users\\ykim\\AppData\\Local\\anaconda3\\Lib\\importlib\\__init__.py:126\u001b[0m, in \u001b[0;36mimport_module\u001b[1;34m(name, package)\u001b[0m\n\u001b[0;32m    125\u001b[0m         level \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m--> 126\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _bootstrap\u001b[38;5;241m.\u001b[39m_gcd_import(name[level:], package, level)\n",
            "File \u001b[1;32m<frozen importlib._bootstrap>:1204\u001b[0m, in \u001b[0;36m_gcd_import\u001b[1;34m(name, package, level)\u001b[0m\n",
            "File \u001b[1;32m<frozen importlib._bootstrap>:1176\u001b[0m, in \u001b[0;36m_find_and_load\u001b[1;34m(name, import_)\u001b[0m\n",
            "File \u001b[1;32m<frozen importlib._bootstrap>:1147\u001b[0m, in \u001b[0;36m_find_and_load_unlocked\u001b[1;34m(name, import_)\u001b[0m\n",
            "File \u001b[1;32m<frozen importlib._bootstrap>:690\u001b[0m, in \u001b[0;36m_load_unlocked\u001b[1;34m(spec)\u001b[0m\n",
            "File \u001b[1;32m<frozen importlib._bootstrap_external>:940\u001b[0m, in \u001b[0;36mexec_module\u001b[1;34m(self, module)\u001b[0m\n",
            "File \u001b[1;32m<frozen importlib._bootstrap>:241\u001b[0m, in \u001b[0;36m_call_with_frames_removed\u001b[1;34m(f, *args, **kwds)\u001b[0m\n",
            "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\models\\gpt2\\modeling_gpt2.py:40\u001b[0m\n\u001b[0;32m     33\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodeling_outputs\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     34\u001b[0m     BaseModelOutputWithPastAndCrossAttentions,\n\u001b[0;32m     35\u001b[0m     CausalLMOutputWithCrossAttentions,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     38\u001b[0m     TokenClassifierOutput,\n\u001b[0;32m     39\u001b[0m )\n\u001b[1;32m---> 40\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodeling_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m PreTrainedModel, SequenceSummary\n\u001b[0;32m     41\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpytorch_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Conv1D, find_pruneable_heads_and_indices, prune_conv1d_layer\n",
            "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:48\u001b[0m\n\u001b[0;32m     47\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mintegrations\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m PeftAdapterMixin, deepspeed_config, is_deepspeed_zero3_enabled\n\u001b[1;32m---> 48\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mloss\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mloss_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LOSS_MAPPING\n\u001b[0;32m     49\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpytorch_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (  \u001b[38;5;66;03m# noqa: F401\u001b[39;00m\n\u001b[0;32m     50\u001b[0m     Conv1D,\n\u001b[0;32m     51\u001b[0m     apply_chunking_to_forward,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     57\u001b[0m     prune_linear_layer,\n\u001b[0;32m     58\u001b[0m )\n",
            "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\loss\\loss_utils.py:19\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BCEWithLogitsLoss, MSELoss\n\u001b[1;32m---> 19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mloss_deformable_detr\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DeformableDetrForObjectDetectionLoss, DeformableDetrForSegmentationLoss\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mloss_for_object_detection\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ForObjectDetectionLoss, ForSegmentationLoss\n",
            "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\loss\\loss_deformable_detr.py:4\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnn\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mimage_transforms\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m center_to_corners_format\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m is_scipy_available\n",
            "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\image_transforms.py:22\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m---> 22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mimage_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     23\u001b[0m     ChannelDimension,\n\u001b[0;32m     24\u001b[0m     ImageInput,\n\u001b[0;32m     25\u001b[0m     get_channel_dimension_axis,\n\u001b[0;32m     26\u001b[0m     get_image_size,\n\u001b[0;32m     27\u001b[0m     infer_channel_dimension_format,\n\u001b[0;32m     28\u001b[0m )\n\u001b[0;32m     29\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ExplicitEnum, TensorType, is_jax_tensor, is_tf_tensor, is_torch_tensor\n",
            "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\image_utils.py:58\u001b[0m\n\u001b[0;32m     57\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_torchvision_available():\n\u001b[1;32m---> 58\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorchvision\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtransforms\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m InterpolationMode\n\u001b[0;32m     60\u001b[0m     pil_torch_interpolation_mapping \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m     61\u001b[0m         PILImageResampling\u001b[38;5;241m.\u001b[39mNEAREST: InterpolationMode\u001b[38;5;241m.\u001b[39mNEAREST,\n\u001b[0;32m     62\u001b[0m         PILImageResampling\u001b[38;5;241m.\u001b[39mBOX: InterpolationMode\u001b[38;5;241m.\u001b[39mBOX,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     66\u001b[0m         PILImageResampling\u001b[38;5;241m.\u001b[39mLANCZOS: InterpolationMode\u001b[38;5;241m.\u001b[39mLANCZOS,\n\u001b[0;32m     67\u001b[0m     }\n",
            "File \u001b[1;32mc:\\Users\\ykim\\AppData\\Local\\anaconda3\\Lib\\site-packages\\torchvision\\__init__.py:6\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorchvision\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _meta_registrations, datasets, io, models, ops, transforms, utils\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mextension\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _HAS_OPS\n",
            "File \u001b[1;32mc:\\Users\\ykim\\AppData\\Local\\anaconda3\\Lib\\site-packages\\torchvision\\_meta_registrations.py:163\u001b[0m\n\u001b[0;32m    160\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m grad\u001b[38;5;241m.\u001b[39mnew_empty((batch_size, channels, height, width))\n\u001b[1;32m--> 163\u001b[0m \u001b[38;5;129m@torch\u001b[39m\u001b[38;5;241m.\u001b[39m_custom_ops\u001b[38;5;241m.\u001b[39mimpl_abstract(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtorchvision::nms\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    164\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmeta_nms\u001b[39m(dets, scores, iou_threshold):\n\u001b[0;32m    165\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_check(dets\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m, \u001b[38;5;28;01mlambda\u001b[39;00m: \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mboxes should be a 2d tensor, got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdets\u001b[38;5;241m.\u001b[39mdim()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124mD\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
            "File \u001b[1;32mc:\\Users\\ykim\\AppData\\Local\\anaconda3\\Lib\\site-packages\\torch\\library.py:795\u001b[0m, in \u001b[0;36mregister_fake.<locals>.register\u001b[1;34m(func)\u001b[0m\n\u001b[0;32m    794\u001b[0m     use_lib \u001b[38;5;241m=\u001b[39m lib\n\u001b[1;32m--> 795\u001b[0m use_lib\u001b[38;5;241m.\u001b[39m_register_fake(op_name, func, _stacklevel\u001b[38;5;241m=\u001b[39mstacklevel \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m    796\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func\n",
            "File \u001b[1;32mc:\\Users\\ykim\\AppData\\Local\\anaconda3\\Lib\\site-packages\\torch\\library.py:184\u001b[0m, in \u001b[0;36mLibrary._register_fake\u001b[1;34m(self, op_name, fn, _stacklevel)\u001b[0m\n\u001b[0;32m    182\u001b[0m     func_to_register \u001b[38;5;241m=\u001b[39m fn\n\u001b[1;32m--> 184\u001b[0m handle \u001b[38;5;241m=\u001b[39m entry\u001b[38;5;241m.\u001b[39mfake_impl\u001b[38;5;241m.\u001b[39mregister(func_to_register, source)\n\u001b[0;32m    185\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_registration_handles\u001b[38;5;241m.\u001b[39mappend(handle)\n",
            "File \u001b[1;32mc:\\Users\\ykim\\AppData\\Local\\anaconda3\\Lib\\site-packages\\torch\\_library\\fake_impl.py:31\u001b[0m, in \u001b[0;36mFakeImplHolder.register\u001b[1;34m(self, func, source)\u001b[0m\n\u001b[0;32m     26\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m     27\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mregister_fake(...): the operator \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mqualname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     28\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124malready has an fake impl registered at \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     29\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkernel\u001b[38;5;241m.\u001b[39msource\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     30\u001b[0m     )\n\u001b[1;32m---> 31\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_dispatch_has_kernel_for_dispatch_key(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mqualname, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMeta\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m     32\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m     33\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mregister_fake(...): the operator \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mqualname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     34\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124malready has an DispatchKey::Meta implementation via a \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     37\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mregister_fake.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     38\u001b[0m     )\n",
            "\u001b[1;31mRuntimeError\u001b[0m: operator torchvision::nms does not exist",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[25], line 8\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m----> 8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m GPT2LMHeadModel, GPT2Tokenizer, AdamW\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Dataset, DataLoader\n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m# Load and preprocess data\u001b[39;00m\n",
            "File \u001b[1;32m<frozen importlib._bootstrap>:1229\u001b[0m, in \u001b[0;36m_handle_fromlist\u001b[1;34m(module, fromlist, import_, recursive)\u001b[0m\n",
            "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\utils\\import_utils.py:1767\u001b[0m, in \u001b[0;36m_LazyModule.__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   1765\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_class_to_module\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[0;32m   1766\u001b[0m     module \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_module(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_class_to_module[name])\n\u001b[1;32m-> 1767\u001b[0m     value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(module, name)\n\u001b[0;32m   1768\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_modules:\n\u001b[0;32m   1769\u001b[0m     value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_module(name)\n",
            "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\utils\\import_utils.py:1766\u001b[0m, in \u001b[0;36m_LazyModule.__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   1764\u001b[0m     value \u001b[38;5;241m=\u001b[39m Placeholder\n\u001b[0;32m   1765\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_class_to_module\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[1;32m-> 1766\u001b[0m     module \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_module(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_class_to_module[name])\n\u001b[0;32m   1767\u001b[0m     value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(module, name)\n\u001b[0;32m   1768\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_modules:\n",
            "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\utils\\import_utils.py:1780\u001b[0m, in \u001b[0;36m_LazyModule._get_module\u001b[1;34m(self, module_name)\u001b[0m\n\u001b[0;32m   1778\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m importlib\u001b[38;5;241m.\u001b[39mimport_module(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m module_name, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m)\n\u001b[0;32m   1779\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m-> 1780\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m   1781\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFailed to import \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodule_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m because of the following error (look up to see its\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1782\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m traceback):\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1783\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n",
            "\u001b[1;31mRuntimeError\u001b[0m: Failed to import transformers.models.gpt2.modeling_gpt2 because of the following error (look up to see its traceback):\noperator torchvision::nms does not exist"
          ]
        }
      ],
      "source": [
        "# Finally, train NLP gpt3.\n",
        "# Starting from \n",
        "train_df = pd.read_csv('remapped_consel_df')\n",
        "\n",
        "\n",
        "import pandas as pd\n",
        "import torch\n",
        "from transformers import GPT2LMHeadModel, GPT2Tokenizer, AdamW\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "# Load and preprocess data\n",
        "train_df = pd.read_csv('remapped_consel_df')\n",
        "train_df[\"input_text\"] = \"Question: \" + train_df[\"questionText\"] + \" Topics: \" + train_df[\"topics\"] + \" Diagnosis: \" + train_df[\"re_diagnosis\"] + \" Response:\"\n",
        "train_df[\"target_text\"] = train_df[\"answerText\"]\n",
        "\n",
        "# Initialize the model and tokenizer\n",
        "model_name = \"gpt2\"\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
        "model = GPT2LMHeadModel.from_pretrained(model_name)\n",
        "\n",
        "# Set padding token for GPT2, which does not have one by default\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "model.config.pad_token_id = tokenizer.pad_token_id\n",
        "\n",
        "class ChatBotDataSet(Dataset):\n",
        "    def __init__(self, tokenizer, input_texts, target_texts, max_len=128):\n",
        "        self.tokenizer = tokenizer\n",
        "        self.input_texts = input_texts\n",
        "        self.target_texts = target_texts\n",
        "        self.max_len = max_len\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.input_texts)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        # Encode input and target texts with truncation and padding\n",
        "        input_encodings = self.tokenizer(\n",
        "            self.input_texts[index],\n",
        "            max_length=self.max_len,\n",
        "            padding=\"max_length\",\n",
        "            truncation=True,\n",
        "            return_tensors=\"pt\"\n",
        "        )\n",
        "        target_encodings = self.tokenizer(\n",
        "            self.target_texts[index],\n",
        "            max_length=self.max_len,\n",
        "            padding=\"max_length\",\n",
        "            truncation=True,\n",
        "            return_tensors=\"pt\"\n",
        "        )\n",
        "\n",
        "        return {\n",
        "            \"input_ids\": input_encodings[\"input_ids\"].squeeze(),\n",
        "            \"attention_mask\": input_encodings[\"attention_mask\"].squeeze(),\n",
        "            \"labels\": target_encodings[\"input_ids\"].squeeze(),\n",
        "        }\n",
        "\n",
        "# Prepare dataset and dataloader\n",
        "input_texts = train_df[\"input_text\"].tolist()\n",
        "target_texts = train_df[\"target_text\"].tolist()\n",
        "# dataset = ChatBotDataSet(tokenizer, input_texts, target_texts)\n",
        "# train_loader = DataLoader(dataset, batch_size=2, shuffle=True)\n",
        "\n",
        "# # Model training setup\n",
        "# optimizer = AdamW(model.parameters(), lr=5e-5)\n",
        "# model.train()\n",
        "\n",
        "# for epoch in range(3):  # Adjust epochs as needed\n",
        "#     for batch in train_loader:\n",
        "#         optimizer.zero_grad()\n",
        "#         outputs = model(\n",
        "#             input_ids=batch[\"input_ids\"],\n",
        "#             attention_mask=batch[\"attention_mask\"],\n",
        "#             labels=batch[\"labels\"]\n",
        "#         )\n",
        "#         loss = outputs.loss\n",
        "#         loss.backward()\n",
        "#         optimizer.step()\n",
        "#         print(f\"Epoch: {epoch}, Loss: {loss.item()}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {},
      "outputs": [
        {
          "ename": "RuntimeError",
          "evalue": "Failed to import transformers.models.gpt2.modeling_gpt2 because of the following error (look up to see its traceback):\npartially initialized module 'torchvision' has no attribute 'extension' (most likely due to a circular import)",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\utils\\import_utils.py:1778\u001b[0m, in \u001b[0;36m_LazyModule._get_module\u001b[1;34m(self, module_name)\u001b[0m\n\u001b[0;32m   1777\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1778\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m importlib\u001b[38;5;241m.\u001b[39mimport_module(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m module_name, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m)\n\u001b[0;32m   1779\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
            "File \u001b[1;32mc:\\Users\\ykim\\AppData\\Local\\anaconda3\\Lib\\importlib\\__init__.py:126\u001b[0m, in \u001b[0;36mimport_module\u001b[1;34m(name, package)\u001b[0m\n\u001b[0;32m    125\u001b[0m         level \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m--> 126\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _bootstrap\u001b[38;5;241m.\u001b[39m_gcd_import(name[level:], package, level)\n",
            "File \u001b[1;32m<frozen importlib._bootstrap>:1204\u001b[0m, in \u001b[0;36m_gcd_import\u001b[1;34m(name, package, level)\u001b[0m\n",
            "File \u001b[1;32m<frozen importlib._bootstrap>:1176\u001b[0m, in \u001b[0;36m_find_and_load\u001b[1;34m(name, import_)\u001b[0m\n",
            "File \u001b[1;32m<frozen importlib._bootstrap>:1147\u001b[0m, in \u001b[0;36m_find_and_load_unlocked\u001b[1;34m(name, import_)\u001b[0m\n",
            "File \u001b[1;32m<frozen importlib._bootstrap>:690\u001b[0m, in \u001b[0;36m_load_unlocked\u001b[1;34m(spec)\u001b[0m\n",
            "File \u001b[1;32m<frozen importlib._bootstrap_external>:940\u001b[0m, in \u001b[0;36mexec_module\u001b[1;34m(self, module)\u001b[0m\n",
            "File \u001b[1;32m<frozen importlib._bootstrap>:241\u001b[0m, in \u001b[0;36m_call_with_frames_removed\u001b[1;34m(f, *args, **kwds)\u001b[0m\n",
            "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\models\\gpt2\\modeling_gpt2.py:40\u001b[0m\n\u001b[0;32m     33\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodeling_outputs\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     34\u001b[0m     BaseModelOutputWithPastAndCrossAttentions,\n\u001b[0;32m     35\u001b[0m     CausalLMOutputWithCrossAttentions,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     38\u001b[0m     TokenClassifierOutput,\n\u001b[0;32m     39\u001b[0m )\n\u001b[1;32m---> 40\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodeling_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m PreTrainedModel, SequenceSummary\n\u001b[0;32m     41\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpytorch_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Conv1D, find_pruneable_heads_and_indices, prune_conv1d_layer\n",
            "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:48\u001b[0m\n\u001b[0;32m     47\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mintegrations\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m PeftAdapterMixin, deepspeed_config, is_deepspeed_zero3_enabled\n\u001b[1;32m---> 48\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mloss\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mloss_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LOSS_MAPPING\n\u001b[0;32m     49\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpytorch_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (  \u001b[38;5;66;03m# noqa: F401\u001b[39;00m\n\u001b[0;32m     50\u001b[0m     Conv1D,\n\u001b[0;32m     51\u001b[0m     apply_chunking_to_forward,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     57\u001b[0m     prune_linear_layer,\n\u001b[0;32m     58\u001b[0m )\n",
            "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\loss\\loss_utils.py:19\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BCEWithLogitsLoss, MSELoss\n\u001b[1;32m---> 19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mloss_deformable_detr\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DeformableDetrForObjectDetectionLoss, DeformableDetrForSegmentationLoss\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mloss_for_object_detection\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ForObjectDetectionLoss, ForSegmentationLoss\n",
            "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\loss\\loss_deformable_detr.py:4\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnn\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mimage_transforms\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m center_to_corners_format\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m is_scipy_available\n",
            "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\image_transforms.py:22\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m---> 22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mimage_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     23\u001b[0m     ChannelDimension,\n\u001b[0;32m     24\u001b[0m     ImageInput,\n\u001b[0;32m     25\u001b[0m     get_channel_dimension_axis,\n\u001b[0;32m     26\u001b[0m     get_image_size,\n\u001b[0;32m     27\u001b[0m     infer_channel_dimension_format,\n\u001b[0;32m     28\u001b[0m )\n\u001b[0;32m     29\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ExplicitEnum, TensorType, is_jax_tensor, is_tf_tensor, is_torch_tensor\n",
            "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\image_utils.py:58\u001b[0m\n\u001b[0;32m     57\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_torchvision_available():\n\u001b[1;32m---> 58\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorchvision\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtransforms\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m InterpolationMode\n\u001b[0;32m     60\u001b[0m     pil_torch_interpolation_mapping \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m     61\u001b[0m         PILImageResampling\u001b[38;5;241m.\u001b[39mNEAREST: InterpolationMode\u001b[38;5;241m.\u001b[39mNEAREST,\n\u001b[0;32m     62\u001b[0m         PILImageResampling\u001b[38;5;241m.\u001b[39mBOX: InterpolationMode\u001b[38;5;241m.\u001b[39mBOX,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     66\u001b[0m         PILImageResampling\u001b[38;5;241m.\u001b[39mLANCZOS: InterpolationMode\u001b[38;5;241m.\u001b[39mLANCZOS,\n\u001b[0;32m     67\u001b[0m     }\n",
            "File \u001b[1;32mc:\\Users\\ykim\\AppData\\Local\\anaconda3\\Lib\\site-packages\\torchvision\\__init__.py:10\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mextension\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _HAS_OPS\n\u001b[1;32m---> 10\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     11\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mversion\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m __version__  \u001b[38;5;66;03m# noqa: F401\u001b[39;00m\n",
            "File \u001b[1;32mc:\\Users\\ykim\\AppData\\Local\\anaconda3\\Lib\\site-packages\\torchvision\\_meta_registrations.py:25\u001b[0m\n\u001b[0;32m     22\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m wrapper\n\u001b[1;32m---> 25\u001b[0m \u001b[38;5;129m@register_meta\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mroi_align\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmeta_roi_align\u001b[39m(\u001b[38;5;28minput\u001b[39m, rois, spatial_scale, pooled_height, pooled_width, sampling_ratio, aligned):\n\u001b[0;32m     27\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_check(rois\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m5\u001b[39m, \u001b[38;5;28;01mlambda\u001b[39;00m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrois must have shape as Tensor[K, 5]\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
            "File \u001b[1;32mc:\\Users\\ykim\\AppData\\Local\\anaconda3\\Lib\\site-packages\\torchvision\\_meta_registrations.py:18\u001b[0m, in \u001b[0;36mregister_meta.<locals>.wrapper\u001b[1;34m(fn)\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapper\u001b[39m(fn):\n\u001b[1;32m---> 18\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torchvision\u001b[38;5;241m.\u001b[39mextension\u001b[38;5;241m.\u001b[39m_has_ops():\n\u001b[0;32m     19\u001b[0m         get_meta_lib()\u001b[38;5;241m.\u001b[39mimpl(\u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mgetattr\u001b[39m(torch\u001b[38;5;241m.\u001b[39mops\u001b[38;5;241m.\u001b[39mtorchvision, op_name), overload_name), fn)\n",
            "\u001b[1;31mAttributeError\u001b[0m: partially initialized module 'torchvision' has no attribute 'extension' (most likely due to a circular import)",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[34], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m GPT2LMHeadModel, GPT2Tokenizer\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorchvision\u001b[39;00m\n",
            "File \u001b[1;32m<frozen importlib._bootstrap>:1229\u001b[0m, in \u001b[0;36m_handle_fromlist\u001b[1;34m(module, fromlist, import_, recursive)\u001b[0m\n",
            "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\utils\\import_utils.py:1767\u001b[0m, in \u001b[0;36m_LazyModule.__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   1765\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_class_to_module\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[0;32m   1766\u001b[0m     module \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_module(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_class_to_module[name])\n\u001b[1;32m-> 1767\u001b[0m     value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(module, name)\n\u001b[0;32m   1768\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_modules:\n\u001b[0;32m   1769\u001b[0m     value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_module(name)\n",
            "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\utils\\import_utils.py:1766\u001b[0m, in \u001b[0;36m_LazyModule.__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   1764\u001b[0m     value \u001b[38;5;241m=\u001b[39m Placeholder\n\u001b[0;32m   1765\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_class_to_module\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[1;32m-> 1766\u001b[0m     module \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_module(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_class_to_module[name])\n\u001b[0;32m   1767\u001b[0m     value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(module, name)\n\u001b[0;32m   1768\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_modules:\n",
            "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\utils\\import_utils.py:1780\u001b[0m, in \u001b[0;36m_LazyModule._get_module\u001b[1;34m(self, module_name)\u001b[0m\n\u001b[0;32m   1778\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m importlib\u001b[38;5;241m.\u001b[39mimport_module(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m module_name, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m)\n\u001b[0;32m   1779\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m-> 1780\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m   1781\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFailed to import \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodule_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m because of the following error (look up to see its\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1782\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m traceback):\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1783\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n",
            "\u001b[1;31mRuntimeError\u001b[0m: Failed to import transformers.models.gpt2.modeling_gpt2 because of the following error (look up to see its traceback):\npartially initialized module 'torchvision' has no attribute 'extension' (most likely due to a circular import)"
          ]
        }
      ],
      "source": [
        "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
        "import torch\n",
        "import torchvision\n",
        "\n",
        "\n",
        "# Load the tokenizer\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(\"advice_model\")\n",
        "\n",
        "# Load the model\n",
        "model = GPT2LMHeadModel.from_pretrained(\"advice_model\")\n",
        "\n",
        "# Example usage\n",
        "input_text = \"Question: How can I reduce stress? Topics: Anxiety Diagnosis: Stress Response:\"\n",
        "inputs = tokenizer.encode(input_text, return_tensors=\"pt\")\n",
        "outputs = model.generate(inputs, max_length=100, num_return_sequences=1)\n",
        "response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
